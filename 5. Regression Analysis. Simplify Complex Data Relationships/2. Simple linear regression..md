You will learn how to use models to describe complex data relationships. You'll focus on relationships of correlation. Then, you'll build a simple linear regression model in Python and interpret your results.
### Foundations of linear regression
##### Welcome to module 2
Here, we'll go over how to set up, build, evaluate, and interpret our first regression model. We'll also review model assumptions, construction, evaluation, and interpretation concepts. If you need to be reorganized, remember the PACE framework, Plan, Analyze, Construct, and Execute. The use of the framework is iterative at times.
Linear regression is a technique that estimates the linear relationship between a continuous dependent variable and one or more independent variables. An independent variable is the variable whose trends are associated with the dependent variable. Commonly represented with the letter X. The dependent variable is the variable that a given model estimates, also known as the outcome variable. The dependent variable is commonly represented by the letter Y. 
Simple linear regression is a technique that estimates the linear relationship between one independent variable, X, and one continuous dependent variable, Y. The model assumptions, code, evaluation metrics, and interpretation skills extends directly into more complex models like multiple linear regression. By solidifying your foundation through simple linear regression, you'll be prepared to tackle advanced models that can answer more complex questions in different industries and business context.
Skills to use in simple linear regression are Python programming, exploratory data analysis or EDA, and statistics. These tools will allows you to construct a linear regression model that can help you influence strategy and decision making in any company or organization. 
What we will learn in regression models using the PACE stages. As for the Plan stage;
- review the linear regression equation
- learn about estimating parameters using ordinary least squares (OLS)
- define each of the four key assumptions of linear regression.
As for the Analyze stage;
- we'll use Python and EDA to verify if our data meets these assumptions.
In the Construct stage;
- We'll build our first regression model together,
- and we will also have the chance to practice both Python and EDA as well
- we'll also learn several evaluation techniques to help you quantify how good your model is
Lastly, aligned with the execute stage;
- we'll use our metrics to practice interpreting our results for stakeholders and non technical audiences.
##### Ordinary least squares estimation
 Simple linear regression is a technique that estimates the linear relationship between one independent variable, X, and one continuous dependent variable, Y. The linear in linear regression indicates what data looks like when plotted on an x, y coordinate plane, a line. In simple linear regression, we're only interested in two variables, one X and one Y variable.
 The equation for a linear regression line is y equals intercept plus slope times X, which is represented by $B_0 + B_1  X$. Since we'll have a number of data points for any given problem, there are many different lines we could draw that might fit the data. However, we are looking for the best fit line. The line that fits the data best by minimizing a loss function or error. In order to find the best fit line, we need to measure error.
 We can consider error as some difference between the data we have, the observed values, and the predicted values generated by a given model. The predicted values are the estimated Y values for each X calculated by a model. The difference between observed or actual values and the predicted values of the regression line are what's known as a residual. The equation for residual value is residual equals observed  value minus predicted value:$${\text{residual}} = {\text{observed value (or the actual value)}} - {\text{predicted value}}$$Each data point has one residual. 
 We can calculate individual residual for each data point. An important thing to note is that the sum of the residuals is always equals to zero for the OLS estimators. For some estimators, the sum of the residuals is not always equal to zero. 
 In order to capture a summary of total error in the model, we square each residual and then add up the residuals for every data point. This is called the sum of squared residuals, or SSR. SSR is the sum of the squared differences between each observed value and the associated predicted value. 
 For linear regression, we'll be using a technique called ordinary least squares to get our best-fit-line.
 Ordinary least squares, also known as OLS, is a method that minimizes the sum of squared residuals to estimate parameters in a linear regression model. Using OLS, we can estimate $B_0$ hat and $B_1$ hat using the properties of the sample data. 
 The hat symbol means it is an estimate of a parameter for we will never know the exact parameter. Remember that parameters or Betas are characteristics of a population. Since we are used to sample data, our goal is to get a reasonable estimate of the parameter.
 Let's say that we have a certain sample of data and we now want to determine a line that fits the data well. For our first attempt at a best-fit line, the slope is 1 and the intercept is 2.5. To calculate the sum of squared residuals or SSR;
 - we calculate the predicted values for each X
 - next, we can find the residual for every observed value of x. The residuals are the difference between each observed value and what the line predicted. 
 We continue to draw other lines to get a little bit closer to the points. Let's try another line. This time, the slope is 1.25 and the intercept is 3. Again we have to plot the residuals and calculate the sum of the squared residuals, or SSR. We could just keep trying different lines and from trial and error pick a line that we think is closest to the data points.
 This is very time consuming. The good news is that in Python the computer will use OLS, or the Ordinary least squares estimation technique to test out many lines and identify which ones is the best fit line.
 With OLS estimation, we find that $B_0$ hat equals 1.5 and $B_1$ hat equals 3.2. The lines we're estimating represent the best fit of the model to the data.
 ##### Explore ordinary least squares
 One way for finding the best fit line in regression modelling is to try different models until you find the best one. But for simple linear regression, the formulas for the best Beta coefficients have been derived. Here, we'll go through an example to gain a better understanding of how the sum of squared residuals can change as $B_0$ hat and $B_1$ hat change.
 1. Formula and notation review
 Simple linear regression is a method for estimating the linear relationship between a continuous dependent variable and one independent variable. An estimate based on simple linear regression can be represented mathematically as y hat equals $B_0$ hat + $B_1$ hat multiplied by $X$. The hat symbol indicates that the Beta coefficients are just estimates. As a result, the y-values derived from the regression models are also just estimates.
 A common technique for calculating the coefficients of a linear regression model is called Ordinary least squares, or OLS. OLS estimates the Beta coefficients in a linear regression model by minimizing the error called the sum of squared residuals, or SSR. The sum of squared residuals is the sum of the squared differences between the observed values and the values predicted by the regression model.
 1. Minimizing the sum of squared residuals (SSR)
 If you have a dataset with observations, you can plot them on a 2-dimensional X, Y coordinate plane. This can be visualized on a scatterplot. Use the equation for Y to calculate the predicted values by plugging in each value of X. Then, we calculate the sum of the square of the residual. We repeat this process with a different line and calculate its sum of the square of the residual. 
 3. Estimating Beta coefficients
 You could keep adjusting the slope and intercept, and then calculating the predicted values, residuals, and squared residuals. But there is really no way to be sure you've found the best fit line. Through advanced math, some formulas have been derived to find the Beta coefficients that minimize error. You won't be asked to calculate Beta coefficients without the help of a computer, but it can be interesting to explore if you desire.
 Key Takeaways
 Given a sample of data, you can try out different lines that could fit your data. You could calculate the sum of squared residuals for each line to determine which fits your data best. As a data professional, it's important to understand what the sum of squared residuals represents, and how to calculate it on your own. Today we have computers, programming languages like Python that can calculate the sum of squared residuals and perform OLS for us. You can explore the deeper math behind OLS and SSR on your own if you wish.
##### Correlation and the intuition behind simple linear regression
 Simple linear regression is a technique that estimates the linear relationship between one independent variable, X, and one continuous dependent variable, Y. Ordinary least squares estimation, or OLS, is a common way to determine the coefficients of the regression line-- the line of "best fit" through the data. Here, we'll learn the following:
 - the meaning of correlation
 - learn about r or the correlation coefficient,
 - discover how to determine the regression equation.
 1. Correlation
 Correlation is a measurement of the way two variables move together. If there is a strong correlation between the variables, then knowing one will be helpful to predict the other. However, if there is a weak correlation between two variables, then knowing the value of one will not tell you much about the value of the other. In the context of linear regression, correlation refers to linear correlation: as one variable changes, so does the other at a constant rate.
 In statistics, a continuous variable can be summarized using some basic numbers. Two of these summary statistics are:
 - Average: A measurement of central tendency (mean, median, or mode)
 - Standard deviation: A measurement of spread
 When two variables are summarized together, there is another relevant statistic called r, Pearson's correlation coefficient (named after the person who helped develop it), or simply the linear correlation coefficient. The correlation coefficient quantifies the strength of the linear relationship between two variables. It always falls in the range of -1 to 1. 
 - When r is negative, there is a negative correlation between the variables: as one increases, the other decreases. 
 - When r is positive, there is a positive correlation between the variables: as one increases, so too does the other.
 - When $r=0$, there is no linear correlation between the variables.
 Note that there are cases where one variable might be precisely determined by another-- like $y=x^2$, or $y=sin(x)$-- but the value of linear correlation between X and Y would nonetheless be low or zero because their relationship is non-linear.
The closer to -1 or 1 r is, the more linear the data appears. When r is exactly 1 or -1, then the variables are perfectly correlated, and their graph is a line. When r is zero, there is no correlation between the variables, and the data appears to be a shapeless cloud of points.
However, r only tells you the strength of the linear correlation between the variables; it does not tell you anything about the magnitude of the slope of the relationship between the variables aside from its sign. For example, variables with r=1 wouldn't tell you if increasing X by one would lead to Y increasing by 10, 100, 0.1, or something else. It would only tell you that you can be sure it would increase. 
The correlation coefficient works as an indicator of of association because it uses the product of each variable's deviation from its mean. When the product is positive, it means both the X and the Y values are either below their respective means (negative standard units) or above their respective means (positive standard units). They vary together. However, when this product is negative, it means one of the values is above its mean and the other is below it. They vary in opposing directions relative to their respective means.
2. Regression
In the absence of any other information, if you had to guess a randomly selected student's exam score, the best way for you to minimize your error would be to guess the average of all the student's scores. But what if you also knew how many hours that student studied? Now, your best guess might be the average score of only the students who studied for that many hours.
Linear regression expands on this concept. A regression line represents the estimated average value of Y (dependent variable) for every value of X (the independent variable), given the assumptions and limitations of a linear model. 
In other words, the actual average Y (dependent variable) values for each X (independent variable) might not lie exactly on the regression line if the relationship between X and Y is not perfectly linear or if there are other factors influencing Y that are not included in the model. The regression line tries to balance out these influences to find a straight-line relationship that best fits the data as a whole. It's an estimation of the central tendency of Y, given X.
3. The regression equation
Now that you know about r and you better understand the concept of regression, you're ready to put everything together to find the line of best fit through the data. The formula for this is known as the regression equation. There are two steps to this. 
The first is:
- The mean value of X and the mean value of Y (i.e. point(x, y)) will always fall on the regression line.
The second is to understand what r means:
- For each increase of one standard deviation in X, there is an expected increase of r standard deviations in Y, on average over X.
In other words, the slope of the regression line is:$${\text{m}} = {\text{r(SD y)}} / {\text{SD x}}$$where m is the formula for the line: $${\text{y}} = {\text{mx + b}}$$The intercept represented by b, is therefore:$${\text{b}} = {\text{y - mx}}$$Because you know point (x, y) is on the regression line, you can plug in the x and y values from this point to calculate the intercept. 
 Key Takeaways
 Linear regression is one of the most important tools that data professionals use to analyze data. Understanding the fundamental building blocks of simple linear regression will help you as you continue learning about more complex methods of regression analysis. Here are some key points to keep in mind:
 - Correlation is a measurement of the way two variables move together.
 - r (Pearson's correlation coefficient or correlation coefficient) quantifies the strength of the linear relationship between two variables.
	 - It always falls in the range of - 1 to 1.
	 - Variables that tend to vary together from their means are positively correlated, variables that tend to vary in the opposite ways to their respective means are negatively correlated.
 - The regression line estimates the average y value for each x value. It minimizes the error when estimating y, given x.
 - The slope of the regression line is:$${\text{r(SD y) / SD x}}$$The point (x, y) is always on the regression line.
### Assumptions and construction in Python
##### Make linear regression assumptions
This happens in the Analyze stage of the PACE framework. The tasks involved in simple linear regression analysis are:
- Checking the technical needs of the model, that is the assumptions of the model. 
- Consider the business context of the problem you're working on. This happens in the Plan stage of the PACE framework.
Model assumptions are statements about the data that must be true in order to justify the use of a particular modelling technique. This means that the characteristics of the data you're using helps one in choosing a model to work with. Ensuring that we're using the right model given the data that we have allows us to be confident in the results those models produce. Model assumptions are the bridge between the Analyze and the Construct stage of the PACE framework. This means that you examine the assumptions before the Construct phase when possible. 
Certain assumptions can only be checked after model construction. Make sure you check those assumptions after you apply the model to confirm if the model is valid or not. Data visualization can be used as a tool to determine if model assumptions are met. Here, Python programming language will be helpful in doing that. 
There are four key assumptions of a simple linear regression:
1. Linearity
	The linear in linear regression comes from the way data looks when plotted on an x, y coordinate plane: a line. The assumption states that each predictor variable X is linearly related to the outcome variable Y. To detect if this assumption is met, you just have to make sure that the points on the plot appear to fall along a straight line.
	If the visualization (a scatterplot) looks like a random cloud or resembles a curve rather than a line, then the assumption is considered invalidated, meaning that this model does not fit the data well. You might need a different or a more complicated model for this dataset. 
	If the visualization (a scatterplot) shows the data points clustering around a line, it indicates the linear regression would be an appropriate model to represent the relationship between x and y.
2. Normality
	This assumes that the residual values or errors are normally distributed. Since this assumption is about residuals, you cannot check the assumption until after the model is built. But once the model is built, you can create a specific plot called a quantile-quantile or QQ plot of residuals. If the points on the plot appear to form a diagonal line, then you can assume normality and check this assumption off the list.   
3. Independent observations
	It states that each observation in the dataset is independent. Here, it is useful to use contextual information about data collection and the variables used to determine if this is true. If the assumption is met, we would expect a scatterplot of the fitted values versus residuals to resemble a random cloud of data points. If there are any patterns, we might need to re-examine the data. 
4. Homoscedasticity
	Homoscedasticity means having the same scatter. In a scatterplot of fitted values versus residuals, there should be constant variance along the values of the dependent variable. This assumption is true if you notice no clear pattern in a scatterplot. Sometimes this is described as random cloud of data points. But for example, if you observe a cone-shaped pattern, then the assumption is invalid.
Linearity, normality, independent observation, and homoscedasticity are four assumptions of a simple linear regression. You don't have to memorize this. Remember, data analysis is an iterative process. You can go back to these concepts, check to see how the assumptions aligned with your data, and then move forward with the regression process.
##### The four main assumptions of simple linear regression
To recap, there are four assumptions of simple linear regression:
1. Linearity: Each predictor variable (Xi) is linearly related to the outcome variable (Y).
2. Normality: The errors are normally distributed.
3. Independent Observations: Each observation in the dataset is independent.
4. Homoscedasticity: The variance of the errors is constant or similar across the model.
Here, the term 'errors' and 'residuals' interchangeably in connection. You may see this on other online material but actually there is a difference:
- Residuals are the difference between the predicted and observed values. You can calculate residuals after you build a model by subtracting the predicted values from the observed values.
- Errors are the natural noise assumed to be in the model.
- Residuals are used to estimate errors when checking the normality and homoscedasticity assumptions of linear regression.
###### How to check the validity of the assumptions
Many of the simple linear regression assumptions can be checked through data visualizations. Some assumptions can be checked before a model is built, and others can only be checked after the model is constructed, and predicted values are calculated.
###### What to do if an assumption is violated
Now that you've reviewed the four assumptions and how to test for their violations, it's time to discuss some common next steps you can take once an assumption is violated. Keep in mind that once you transform the data, this might change how you interpret the results. Additionally, if these potential solutions don't work for your data, you have to consider trying a different kind of model.
For now, focus on a few essential approaches to get you started!
###### Linearity
Transform one or both of the variables, such as taking the logarithm. 
- For example, if you are measuring the relationship between years of education and income, you can take the logarithm of the income variable and check if that helps the linear relationship.
###### Normality
Transform one or both variables. Most commonly, this would involve taking the logarithm of the outcome variable.
- When the outcome variable is right skewed, such as income, the normality of the residuals can be affected. So, taking the logarithm of the outcome variable can sometimes help with this assumption.
- If you transform a variable, you will need to reconstruct the model and then recheck the normality assumption to be sure. If the assumption is still not satisfied, you'll have to continue troubleshooting the issue.
###### Independent observations
Take just a subset of the available data.
- If, for example, you are conducting a survey and get responses from people in the same household, their responses may be correlated. You can correct this by just keeping the data of one person in each household.
- Another example is if you are collecting data over a time period. Let's say you are researching data on bike rentals. If you collect your data every 15 minutes, the number of bikes rented out at 8:00 a.m. might correlate with the number of bikes rented out at 8:15 a.m. But perhaps the number of bikes rented out is independent if the data is taken once every 2 hours, instead of once every 15 minutes.
###### Homoscedasticity
Define a different outcome variable.
- If you are interested in understanding how a city's population correlates with the number of restaurants in a city, you know that some cities are much more populous than others. You can then redefine the outcome variable as the ratio of population to restaurants.
Transform the Y variable.
- As with the above assumptions, sometimes taking the logarithm or transforming the Y variable in another way can potentially fix inconsistencies with the homoscedasticity assumption.

Key Takeaways
- There are 4 key assumptions for simple linear regression: linearity, normality, independent observations, and homoscedasticity.
- There are different ways to check the validity of each assumption. Some assumptions can be checked before the model is built, while some can be checked after the model is built.
- There are ways to work with the data that can correct for violations of model assumptions.
- Changing the variables will change the interpretation.
- If the assumptions are violated, even after data transformations, you should consider other models for your data.
### Evaluate a linear regression model
##### Evaluate uncertainty in regression analysis
We use PACE to think like a data professional. We Plan by thinking through the problem and sub setting the available data appropriately. In our problem or example, we asked, how can we better understand the relationship between penguin anatomy and their body mass? 
In the Analyze stage, we perform EDA and check model assumptions. Then moving on to the Construct stage. As a reminder, there are two parts. We built our model and we're able to pull out some parameter estimates. Now we're going to focus on the next step of Construct phase: Model evaluation.
Model evaluation is an important practice in data analysis. Careful evaluation and interpretation of your regression model helps you understand its performance and accuracy. Randomness and unpredictability are characteristics of every regression model that make it difficult to predict outcomes with 100% certainty. After all, there is still a difference between our observed and predicted values. You've just found the model that you are most certain about. 
Confidence intervals or CI are a range of values that describe the uncertainty surrounding an estimate. In the case of linear regression, we are estimating parameters. A parameter is a characteristic of a population. So a 95% CI means that interval has a 95% chance of containing the true parameter value of the slope. What if the slope and intercept were slightly different?
When we draw out a few different lines on our plot with slightly different slopes and intercepts all within our CI, we get a region around the regression line that is tight around the center and fans out a bit towards either end of the line. You can observe this shape when plotting regression line using Seaborn's regplot function. These lines make up the shaded region that is around the regression line. Essentially the CI around the parameter estimates reveal what we call a confidence band. 
A confidence band is the area surrounding the line that describes the uncertainty around the predicted outcomes at every value of X. Typically expressed as a shaded region around the best fit line on a scatterplot. It reveals the CI for each point on a regression line. Confidence bands are simply another way to report your findings responsibly. 
Simple linear regression is a powerful addition to any data professional's toolbox. Regression analysis can help you make discoveries and understand the relationship behind the data. But we must remember data is noisy and results can be uncertain. When using regression models like simple linear regression, even the best data doesn't tell a complete story.
As a data professional, you should always aim not only to evaluate the performance and accuracy of your models, but also to report uncertainty. Communicating about confidence intervals and confidence bands is part of being a responsible data professional. These metrics will also help you understand how well the models can tell the story behind the data.
##### Interpret measures of uncertainty in regression
Review of concepts
We can represent a simple linear regression line as:
$${\text{y}} = {{B_0 + B_1 X}}$$Since regression analysis utilizes estimation techniques, there is always a level of uncertainty surrounding the predictions made by regression models. To represent the error, we can actually rewrite the equation to include the error term, represented by the letter E (pronounced 'epsilon'):
$${\text{y}} = B_0 + B1_X + {\text{E}}$$There is one residual, also known as the difference between the predicted and actual value, for each data point in the dataset used to construct the model. We can then quantify how uncertain the entire model is through a few measures of uncertainty:
- Confidence intervals around beta coefficients. Its defined as a range of values that describes the uncertainty surrounding an estimate.
- P-values for the beta coefficients. Its defined as the probability of observing results as extreme as those observed when the null hypothesis is true.
- Confidence band around the regression line
Interpreting uncertainty
You may have learnt about p-values and CI within the context of hypothesis testing. Even though it may seem unintuitive, even in regression analysis we are testing hypothesis. 
P-values
When running regression analysis, you want to know if X (independent variable) is really correlated with Y (dependent variable) or not. So we do a hypothesis test on the regression results. In regression analysis, for each Beta coefficient, we are testing the following set of null and alternative hypothesis:
- $H_0$ (null hypothesis): $B_1 = 0$
- $H_1$ (alternative hypothesis): $B_1$ != $0$
When the p-value is less than the CI of 0.05, we can reject the null hypothesis that $B_1$ is equal to 0, and state that the coefficient is statistically significant, which means that a difference in X is truly correlated with a difference in Y.
Confidence Intervals
Each Beta coefficient also has a confidence interval associated with its estimate. A 95% interval means the interval itself has a 95% chance of containing the true parameter value of the coefficient. So there is 5% chance that our CI does not contain the true value of $B_1$. More precisely, this means that if you were to repeat the experiment many times, 95% of the CI would contain the true value of $B_1$.
But, since there is uncertainty in both of the estimated beta coefficients, then the estimated y values also have uncertainty. This is where confidence bands become useful.
Confidence band
The area surrounding the line that describes the uncertainty around the predicted outcome. You can think of the confidence band as representing the confidence interval surrounding each point estimate of y. Since there is uncertainty at every point in the line, we use the confidence band to summarize the confidence intervals across the regression model. The confidence band is always narrowest towards the mean of the sample and widest at the extremities.
##### Model evaluation metrics
Here, we are still in the Construct phase of the PACE framework. We are getting close to the Execute phase, where you share the stories behind the data you are studying. Using a variety of evaluation metrics, supports data professionals confidence in the insights produced by their analysis. These metrics are key to responsible communication of results. If models are inaccurate or imprecise, decisions made based on those insights may also be inaccurate.
Three metrics you might encounter are:
- R-squared
This is the main metric academic researchers and data professionals use when evaluating regression models. Its also called coefficient of determination. 
It measures the proportion of variation in the dependent variable, Y, explained by the independent variable(s), X. At most, R-squared can equal 1, which would mean that X explains 100% of the variance in Y. If R-squared equals 0, then that would mean X explains 0% of the variance in Y. 
For example, if an OLS summary table shows the model has an R-squared of 0.769. This means that bill length explains about 77% of the variance in body mass. There is still 23% of the variance of body mass that is unexplained by the model. This variance might be due to other factors or natural unexplained differences from penguin to penguin. There is no benchmark value that R-squared has to equal. 
But in general, the higher R-squared the better, because it adds validity to any recommendation you make based on your analysis. 
R-squared is a useful metric that can help evaluate your model. But there are also processes that help strengthen the evaluation of a model. 
- Mean Squared Error (MSE)
- Mean Absolute Error (MAE)
##### Evaluation metrics for simple linear regression
Here, we'll discuss more comprehensive overview about evaluation metrics for simple linear regression and introduce a few more that you may encounter throughout your career as a data professional. 
###### R-squared: The coefficient of determination
The main evaluation metric for linear regression is R-squared, or the coefficient of determination. It measures the proportion of variation in the dependent variable, Y, explained by the independent variable(s), X. 
- This is calculated by subtracting the sum of squared residuals divided by the total sum of squared from 1.$${\text{R-squared}} = 1 - {\text{sum of squared residuals / total sum of squares}}$$
R-squared ranges from 0 to 1. So, if a model has an R-squared of 0.85, that means that the X variables explain about 85% of the variation in the Y variable. Although R-squared is a highly interpretable and commonly used metric, you may also encounter mean squared error (MSE) and mean absolute error (MAE) when R-squared is insufficient in evaluating model performance.
###### MSE: Mean squared error
It is the average of the squared difference between the predicted and actual values.
- Because of how MSE is calculated, MSE is very sensitive to large errors.
###### MAE: Mean absolute error
It is the average of the absolute difference between the predicted and actual values.
- If your data has outliers that you want to ignore, you can use MAE, as it is not sensitive to large errors.
 Other evaluation metrics
Beyond the three metrics listed above, you may also encounter AIC (Akaike information criterion) and BIC (Bayesian information criterion). There is also adjusted R-squared which is a variation of R-squared that accounts for having multiple independent variables present in a linear regression model.
Key takeaways
- There are many evaluation metrics to choose from with regard to simple linear regression.
- The most common evaluation metric you'll encounter is probably R-squared. But, there are times when R-squared is insufficient or inappropriate to use.
- Based on your experiences and the particulars of a metric, you can use your best judgement to select appropriate metric to evaluate a regression model.
##### Interpret and present linear regression results
This is where you build a regression model and check for the assumptions for simple linear regression using a few common metrics. Next, its onto the Execute phase of the PACE framework. This is the point of the PACE framework when your ability to communicate is crucial.
Here, you review the results of your regression model, explore ways in which those insights can be translated into formal visualizations, and communicating a meaningful narrative with stakeholders. 
After building out the model, the next step is to present the model, insights, and recommendations to the stakeholders. 
These folks are non-technical business partners so using data-specific terminology won't help them. Using it risks not only interesting the stakeholders in the project but also loosing their buy-in. So by telling a story where you articulate the model results and how that translates to the bottom line, results in the model being implemented immediately. The model is put into production and it is used for the purpose that it was built for. 
Telling the right story to the right people takes time and practice. This is a skill that will continue to work on through out our careers. Its important to know your audience and tailor your delivery and story in order to have the most impact. 
Regression model interpretation depends on coefficients and p-values. Coefficients will determine how changes in the independent variable are associated with changes in the dependent variable. P-values demonstrates whether coefficients are statistically significant. Recall that the slope is used to determine the amount you expect Y to increase or decrease per one unit increase of X. 
The correlation between X and Y doesn't necessarily reflect causation. Causation describes a cause-and-effect relationship where one variable directly causes the other to change in a particular way. Evan though we cannot speak to causation, we can still provide valuable insights about the correlation. 
By providing measures of uncertainty around our estimates, we're responsibly reporting our results. When sharing insights as a data analytics professional, you need to make sure that your findings can be quickly and correctly interpreted. Communicating the context of your data is one way you can report it responsibly. For example, if your results only apply to a specific part of the population, make it clear that people should be cautious about extrapolating to larger or different groups of the same population. Data visualizations are excellent ways of making statistics relatable to others.
Use caution when presenting terms like coefficients and P-values in your visualizations. Consider that not everyone will fully grasp the significance of these terms immediately. There are many useful libraries like Matplotlib and Seaborn that can help create visualizations. Programs like Tableau, PowerPoint, or Google Slides allow for creation of high-quality presentations to help you provide contexts that's relevant to the business problem.
Regardless of your visualization tool of choice, communicating with those around you is critical to your success at all points of the PACE process. As a data analytics professional, clear communication allows you and your work to have impact throughout your team and organization.
##### Correlation versus causation
What is correlation? There are two main kinds of correlation: positive and negative correlation.
- Positive correlation is a relationship between two variables that tend to increase or decrease together.
- Negative correlation is an inverse relationship between two variables, where when one variable increases, the other variable tends to decrease, and vice versa.
To generalize, correlation measures the way two variables tend to change together. There is a metric called the Pearson correlation coefficient that ranges from -1 to 1 that can measure the relationship between two variables.
Its difficult to claim causation.
Causation describes a cause-and-effect relationship where one variable directly causes the other to change in a particular way. To argue causation between variables, in general, you must run a randomized controlled experiment. The following are some key components of a proper randomized controlled experiment:
- You must control for every factor in the experiment.
- You must have a control group under certain conditions.
- You must have at least one treatment group under certain conditions.
- The difference(s) between the control and treatment groups must be observable and measurable.
Setting up a randomized controlled experiment is quite laborious and intensive. There are a number of requirements and factors not included in this reading, but there is a lot of information online and academic research that you can explore on your own. Understanding the basics of causal claims allows you to responsibly report the results of your data analysis.
When working as a data professional, you often do not have complete control of how the data is collected. You and your team might not be able to run a randomized controlled experiment. But, even if you cannot make causal claims, correlational research can still yield interesting results that have meaningful business implications.
Claiming causation requires specific circumstances that are often not within your control.
