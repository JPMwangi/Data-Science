You'll build on your prior knowledge of hypothesis testing to explore two more statistical tests: Chi-squared and analysis of variance (ANOVA). You'll learn how data professionals use these tests to analyze different types of data. Finally, you'll conduct two kinds of Chi-squared tests, as well as one-way and two-way ANOVA tests.
### The chi-squared test
##### Welcome to module 4
We can model variable relationships through regression, that is; simple linear regression and multiple linear regression. We can do this through the assumptions, construction, evaluation, and interpretation of linear regression models. These tools give us powerful ways to ask and explore questions about continuous variables. For example, if we were interested in understanding product sales, multiple regression would allow us to consider the impact of both digital video and print advertising on sales.
Now, we will focus on categorical variables. Here, we'll start with Chi-squared distribution and chi-squared test. Chi is the 22nd letter of the Greek alphabet, and looks like a fancy letter $X$ in English. We will also focus on t-tests and hypothesis testing. Chi-squared tests will help us determine if two categorical variables are associated with one another, and whether a categorical variable follows unexpected distribution. For example, if we flip a two-sided coin many times, we would expect about half of the time, one particular side comes up. About other half of the time, the other side comes up. Next, we will cover ANOVA or analysis of variance, ANCOVA or analysis of covariance, and MANOVA or multivariate analysis of variance.
By unpacking variable relationships with a focus on categorical data, we'll be able to expand the types of data we can effectively use to draw conclusions about various decisions, strategies, and practices in industry.
The core principles of data analysis continue to apply to each technique. We want to explore and better understand the relationships between different variables. We can use what we have learned to tell compelling stories about the data. Based on the kind of data we have available, we can determine which test or model will be most appropriate. But sometimes we have to use several test and models before we can determine the best approach to answer our questions. 
##### Hypothesis testing with chi-squared
In hypothesis testing, there is one-tail and two-tail t-tests. T-tests help us answer questions about whether the mean of two different groups is significantly different. Here, we will explore if our data is what we expect it to be. We'll introduce two hypothesis tests: the chi-squared goodness of fit test and the chi-squared test of independence. These tests will help us compare our expected and observed data.
There is $H_o$ and $H_a$ when defining chi-squared tests. When using linear regression, we were primarily focused on continuous variables, but what if our variables aren't continuous?
###### the chi-squared goodness of fit test
Chi-squared tests can address questions involving categorical variables. For example, let's say you're working at a movie theatre which sells small, medium, large, and extra-large portions of popcorn. You have projections or some expected counts of how each size will sell. In a bar chart, it can be difficult to determine if the expected counts are statistically that same as what you observed, but you can run a chi-squared goodness of fit test to answer the question.
The chi-squared goodness of fit test determines whether an observed categorical variable follows an expected distribution. As a data practitioner for the movie theatre example, you're working on the popcorn sale problem. An employee claims that 25% of the people order each size on any given day. Now you can create a table of counts based on the number of popcorn buyers on a given day.
Let's say 100 people bought popcorn yesterday, then you can multiply the percentage by the total to figure out the expected counts for each cell in the table. That figure is 25. You expect that 25 people buy each size of popcorn.
For the chi-squared goodness of fit test, the $H_o$ states that 25 people buy each size of popcorn on any given day. Basically, the $H_o$ states that the variable follows the expected distribution. If we accept the $H_o$ , then we can say that the observed distribution of popcorn sales is the same as what the employees claims. 
The $H_a$ states that the variable doesn't follow the expected distribution. Basically, the distribution of the observed data is significantly different from what we expected it to be. This means that different number of people buy each size of popcorn on any given day.
Once you have gathered the observations about popcorn sales, you can then use the chi-squared statistic to calculate the p-value and determine if you can reject the $H_o$ at the given confidence level.
###### the chi-squared test for independence or a test for homogeneity
The chi-squared test for independence determines whether or not two categorical variables are associated with each other. For example, let's say you are wondering if weather is associated with popcorn sales. Maybe when it rains, people are more likely to but hot buttery popcorn. To state the hypothesis, you first have to determine your variables.
In the popcorn case, the first variable could be whether there's rain or not. The second variable could be if more than 100 people buy popcorn, or if 100 people or fewer buy popcorn. Now we are ready to state our hypothesis. The $H_o$ in the test for independence is that the variables are independent and not associated with each other. The $H_a$ states that the variables are not independent and are associated with each other.
For the chi-squared test, it's important to construct 2x2 table of counts to see how many observations fall under each category. Fill in each cell with the number of observations for each category and the calculate the expected value for each cell in the 2x2 table using a formula. The formula comes from the definition of independence, where two events are independent if the probability of both occurring, probability of A and B is equals the probability of A multiplied by the probability of B. You can use the same formula as the goodness of fit test to calculate the chi-squared statistic. As with the goodness of fit test, you can use the chi-squared statistic to calculate the p-value and determine if the two categorical variables are independent or not.
The key here is connecting concepts you have learned about hypothesis testing to the chi-squared test as well.
##### Chi-squared tests: Goodness of fit versus independence
Hypothesis tests are used to see significant differences among groups. Chi-squared tests are used to determine whether one or more observed categorical variables follow expected distribution(s). For example, you may expect that 50% more movie goers attend movies on weekends in comparison to weekdays. After observing movie goers attendance for a month, you then can perform a chi-squared test to see if your initial hypothesis was correct.
This reading will cover the two main chi-squared tests --goodness of fit and test of independence--which can be used to test your expected hypothesis against what actually occurred. Data professionals perform these hypothesis tests to offer organizations actionable insights that drive decision making.
###### The Chi-squared goodness of fit test
Is a hypothesis test that determines whether an observed categorical variable with more than two possible levels follows an expected distribution. The null hypothesis ($H_o$) of the test is that the categorical variable does not follow the expected distribution. The alternative hypothesis ($H_a$) is that the categorical variable does not follow the expected distribution.
Chi-squared goodness of fit scenario
Imagine you work as a data professional for an online clothing company. Your boss tells you that they expect the number of website visitors to be the same for each day of the week. You decide to test your boss's hypothesis and pull data every day for the next week and record the number of website visitors. Here are the main steps you will take:
###### Step 1: Identify the Null and Alternative hypothesis
The first step in performing a chi-squared goodness of fit test is to determine your null hypothesis ($H_o$) and alternative hypothesis ($H_a$). Since you are testing if the number of website visitors follows your boss's expectations, below are your null hypothesis ($H_o$) and alternative hypothesis ($H_a$):
$H_o$: The week you observed follows your boss's expectations that the number of website visitors is equal on any given day.
$H_a$: The week you observed does not follow your boss's expectations; therefore, the number of website visitors is not equal across the days of the week.
###### Step 2: Calculate the chi-square test statistic
Next, calculate a test statistic to determine if you should reject or fail to reject your null hypothesis. This test statistic is known as the chi-squared statistic and has a formula. 
The intuition behind the formula is that it should quantify the extent of any discrepancies between observed frequencies and expected frequencies for each categorical level. The chi-squared goodness of fit test does not produce reliable results when there are any expected values less than five.
###### Step 3: Calculate or find the p-value
Now that you have calculated the chi-squared statistic, ask the question that the p-value--or observed significance level--will answer. Calculate the degrees of freedom for the model. There are 7 categorical levels (one for each day of the week). In our website visitor examples, the degrees of freedom are 7-1 which equals 6. This is because the counts of each level (day) are free to fluctuate, but once you know the count of 6 days, the 7th cannot vary. 
Find the p-value for our example.
###### Step 4: Make a conclusion
After finding the p-value, you decide whether to reject the $H_o$ or to fail to reject the $H_a$.
Coding
You don't need to manually calculate the chi-squared or determine p-value by hand. You can use the chisquare() function from Python's scipy.stats package to do this. For our website example:
```Python
import scipy.stats as stats
observations = [650, 570, 420, 480, 510, 380, 490]
expectations = [500, 500, 500, 500, 500, 500, 500]
result = stats.chisquare(f_obs=observations, f_exp=expectations)
result
```
The output can confirm manual calculations of the same chi-square test statistic and the associated p-value. Because the p-value is less than the significant level of 5%, you can reject the null hypothesis ($H_o$).
###### The Chi-squared test for independence
Is a hypothesis test that determines whether or not two categorical variables are associated with each other. It is valid when your data comes from a random sample and you want to make an inference about the general population. The null hypothesis ($H_o$) of the test is that two categorical variables are independent. The alternative hypothesis ($H_a$) is that two categorical variables are not independent. 
Chi-squared test for independence scenario
Now imagine that you have been asked to expand your analysis to look at the relationship between the device that a website user used and their membership status. To do this, you must use the chi-squared test for independence test. In this example, it determines whether the type of device a visitor uses to visit the website (Mac or PC) is dependent on whether he or she has a membership account or browses as a guest (member or guest).
###### Step 1: Identify the null ($H_o$) and alternative hypothesis ($H_a$)
You are comparing if the device used to visit your clothing store (Mac or PC) is independent from the visitor's membership status (member or guest). From that information you can determine that your null and alternative hypotheses are as follows:
$H_o$ : The type of device a website visitor uses to visit the website is independent of the visitor's membership status.
$H_a$ : The type of device a website visitor uses to visit the website is not independent of the visitor's membership status.
###### Step 2: Calculate the chi-squared test statistic
To calculate the chi-squared test statistic, arrange the data as a table that contains m by n values, where m and n are the number of possible levels contained within each respective categorical variable. Then derive totals from these figures. These totals can be used to calculate the expected values, which are necessary to get the chi-squared test statistic. Use a formula to do the calculation.
The logic of the calculation is as follows: if device and membership status are truly independent, then the rate of Mac users who are members should be the same as the rate of Mac users who are guests.
###### Step 3: Find the p-value
find the degrees of freedom, then calculated the p-value. You can use Python programming language to do this.
###### Step 4: Make a conclusion
After finding the p-value, you decide whether to reject the $H_o$ or to fail to reject the $H_a$.
Coding
You can use the scipy.stats package's chi2_contingency() function to obtain the chi-squared test statistic and p-value of a chi-square independence test. The chi2_contingency() function only needs the observed values; it will calculate the expected values for you. Here's the Python code:
```Python
import numpy as np
import scipy.stats as stats
observations = np.array([[850, 450], [1300, 900]])
result = stats.contingency.chi2_contingency(observations, correction=False)
result
```
The output above is in the following order: the chi-squared statistic, p-value, degrees of freedom, and expected values in an array format. The behavior of the Python package is that it is designed to be used when it's possible for an expected frequency in the table to be small (generally < 5).
Key takeaways
The chi-squared goodness of fit test is used to test if an observed categorical variable follows a particular expected distribution.
The chi-squared test for independence is used to test if two categorical variables are independent of each other or not (when samples are drawn at random and you want to make an inference about the whole population).
Both chi-squared tests follow the same hypothesis testing steps to determine whether you should reject or fail to reject the null hypothesis to drive decision making.
### Analysis of Variance
##### Intro to the analysis of variance
Knowing our data well helps us determine what we can do with the data. It helps us understand what tests we can run, which tests we can't run and which tests we might be able to run if we transform the data a bit more.
There are different types of variables; continuous and categorical variables. Linear regression can estimate continuous variables of interest but there is so much categorical data out there. Chi-squared tests can be used to estimate categorical variables of interest. The chi-squared goodness of fit test and test of independence examines the relationship between categorical variables.
Now we'll focus on Analysis of Variance or ANOVA, which helps examine the relationship between categorical variables and continuous variables. ANOVA is a group of statistical techniques that test the difference of means between three or more groups. It is an extension of t-tests which are a common statistical test. While t-tests examine the difference of means between two groups, ANOVA can test means between several groups.
There are two main types of ANOVA tests: One-way and two-way. One-way ANOVA testing compares the means of one continuous dependent variables in three or more groups. You can use a categorical variable to represent the groupings. When using ANOVA, we have a null hypothesis and an alternative hypothesis.
To generalize, we can use one-way ANOVA when the null hypothesis states that the means of each group are equal. The alternative hypothesis would then be that the means of each group are not all the same. Only one of the mean from either of the group has to be different to reject the null hypothesis.
The null hypothesis is represented as $H_o$ and the alternative hypothesis is represented as $H_1$ or H1 because you may run more complex tests in the future where you are testing more than two hypothesis at once. 
Sometimes you may encounter situations in which there are two factors that are associated with the continuous dependent variable. Two-way ANOVA testing compares the means of one continuous dependent variable based on two categorical variables. You are now testing more than two null and alternative hypothesis at once.
Regression lets you understand how independent variables impact dependent variables. ANOVA allows you to zoom in on some of those relationships to tell a complete story by unpacking relationships in a pairwise fashion. 
##### More about ANOVA
ANOVA or analysis of variance is a group of statistical techniques that test the difference of means between groups. It is helpful when you want to test a hypothesis about group differences based on categorical independent variables. For example, if you wanted to determine whether changes in people's weight when following different diets are statistically significant or due to chance, you could use ANOVA to analyze the results. Data professionals routinely must ascertain if there are meaningful differences between groups in their data. This reading will examine more closely the intuition behind ANOVA.
###### An overview of ANOVA
The intuition behind ANOVA is to compare the variability between different groups with the variability within the groups. If they are comparable, then the differences between groups are more likely to be due to sampling variability. On the other hand, if the variability between groups is much larger than the variability expected from the samples within their respective groups, then those groups are probably drawn from significantly different subpopulations.
The variation between groups and within groups is calculated as sums of squares, which are then expressed as a ratio. This ratio is known as the F-statistic. To review about the types of ANOVA tests:
One-way ANOVA: Compares the means of one continuous dependent variable based on three or more groups of one categorical variable.
Two-way ANOVA: Compares the means of one continuous dependent variable based on three or more groups of two categorical variables.
To help you understand the intuition behind ANOVA, we will break down a simple one-way ANOVA test.
###### One-way ANOVA
There are five steps in performing a one-way ANOVA test:
- Calculate group means and grand (overall) mean
- Calculate the sum of squares between groups (SSB) and the sum of squares within groups (SSW)
- Calculate the F-statistic
- Use the F-distribution and the F-statistic to get a p-value, which you use to decide whether to reject the null hypothesis.
###### Assumptions of ANOVA
ANOVA will only work if the following assumptions are true:
- The dependent values from each group come from normal distributions. 
	Note that this assumption does not mean that all of the dependent values, taken together, must be normally distributed. Instead, it means that within each group, the dependent values should be normally distributed.
	ANOVA is generally robust to violations of normality, especially when sample sizes are large or similar across groups, due to central limit theorem. However, significant violations can lead to incorrect conclusions.
- The variance across groups are equal.
	 ANOVA compares means across groups and assumes that the variance around these means is the same for all groups. If the variances are unequal (i.e. heteroscedastic), it could lead to incorrect conclusions.
- Observations are independent of each other.
	ANOVA assumes that one observation does not influence or predict any other observation. If there is autocorrelation among the observations, the results of the ANOVA test could be biased.
Key Takeaways
ANOVA tests are statistical tests that examine whether or not the means of a continuous dependent variable are significantly different from one another based on the different levels of one or more independent categorical variables.
It is sufficient for one group's mean to be significantly different from the others to reject the null hypothesis; however, ANOVA testing is limited in that it doesn't tell you which group is different. To make such a determination, other tests are necessary.
ANOVA works by comparing the variance between each group to the variance within each group. The greater the ratio of variance between groups to variance within groups, the greater the likelihood of rejecting the null hypothesis.
ANOVA depends on certain assumptions, so it is important to check that your data meets them in order to avoid drawing false conclusions. At the very least, if your data does not meet all of them, identify these violations.
### ANCOVA, MANOVA, and MANCOVA
##### ANCOVA : Analysis of covariance
There are exciting ways uncover stories about categorical variables. ANOVA helps us learn more about the differences in continuous Y variable based on different groupings. Using ANOVA, we can determine whether groups are different from one another. 
Simple linear regression can only take into account one independent variable to explain the variance of one dependent variable. Multiple regression allows us to incorporate lots of different factors into our analysis. But by working with simple linear regression, you build a strong foundation for understanding the mechanics of regression modeling. By working with multiple regression, you are able to expand the kinds of questions you could explore. For example, using simple linear regression, you could explore the relationship between the weather and ice coffee sales. But with multiple regression, you could then include temperature and distance to public transportation or other variables you think might be related to coffee sales. 
There's a similar relationship between ANOVA and ANCOVA or analysis of covariance. Analysis of covariances or ANCOVA is a technique that test the difference of means between three or more groups while controlling for the effects of covariance. Covariates are the variables that are not of direct interest to the question we are trying to address.
By taking covariates into account, we can better isolate the relationship between the categorical variable we are interested in and the Y variable. This allows us to draw more accurate conclusions about the relationship among the variables. 
For example, if examining ice coffee sales ANCOVA allows you to analyze how the sales are different on workdays versus the weekends while controlling for the temperature of the day. Let's say we have a drop in sales on weekend. We could say that no one buys coffee on weekends because they're not going to work, but perhaps it was especially cold that weekend. ANCOVA allows us to double-check whether temperature was a factor.
So, why might a data analytics professional use ANCOVA when we already have linear regression analysis we can use? There are many similarities between ANCOVA and linear regression. For example, both allow for continuous and categorical independent variables. Also, both focus on a continuous Y variable and both center on understanding relationships between variables. But the use case depend on which variable we are most interested in understanding.
With ANCOVA, while we're not focused on the covariates, we are including covariates to gain a clear understanding of the categorical variables. With regression, we might be interested in all of the independent variables or in predicting the Y variables for unseen data.
In ANCOVA we are dealing with hypothesis tests. So its important to state our null and alternative hypothesis before running a test. These tests can be run using Python.
What kinds of questions can ANCOVA help us answer? Let's say that you're working at a bookstore and you're interested in the relationship between book genres and sales. It seems new books tend to get more attention because authors are travelling to promote their recent work. The good news is that ANCOVA will let us control for publication year.
Controlling for other variables is important so we don't draw conclusions that are not accurate. In this example, the categorical independent variable is book genre. The covariate is the years since the book was published. The continuous dependent variable is the number of books sold in the last month. The $H_o$ is that the book sales are equal for all genres regardless of the number of years since publication. The $H_1$ is that book sales are not equal for all genres regardless of number of years since publication. 
You can work this out using a computer in Python but you need to interpret the results. Typically, if the test yields a p-value of less than 0.05, you can reject the $H_o$ that all means were the same even when controlling for the covariates.
##### More dependent variables: MANOVA and MANCOVA
In your data journey, you have built and extended a number of models and tests from their most basic form to include more variables, and different kinds of data. As you've explored various models, you've encountered different questions you can answer, and different use cases for each model. You've made the shift from ANOVA to ANCOVA, much like you extended simple linear regression, to multiple regression. You added more independent variables to help isolate the effect of each X variable on the Y variable in question.
Here, we add more dependent variables to allow us to perform new and different kinds of comparative analysis. The two tests will introduce MANOVA and MANCOVA. Based on their names, you can relate to ANOVA and ANCOVA.
MANOVA or multivariate analysis of variance is an extension of ANOVA that compares how two or more continuous outcome variables vary according to categorical independent variables. The two most common versions of MANOVA involve one or two categorical independent variables, and are referred to as:
- One-way MANOVA
	One categorical independent variable
- Two-way MANOVA.
	Two categorical independent variables
The independent variable must be categorical, and the outcome variables must be continuous. Since we're still dealing with hypothesis testing, we need some hypotheses to test. Let's return to the bookstore example to generate, and test hypotheses about factors relating to book sales. The one categorical variable will be book genre, the two continuous depending variables could be the number of books sold per month, and profits from book sales.
Let's say you're working with one-way MANOVA test, in this case, the $H_o$ would be that the means of both continuous variables are equal for every book genre. So, the number of books sold per month is the same for each book genre, and the profit from selling books is the same for each book genre. The $H_a$ would be that the means of both continuous variables, are not equal for every book genre. 
This indicates that the only two genres of books must differ for just one of the outcome variables we are examining. For example, perhaps the profit made from self-help books differ from the profit made from science fiction books. Or maybe the number of graphic novels sold per month differs from the number of historical fiction books sold per month. In either case, you could reject the $H_o$ . 
MANOVA allows us to think of each data point as having a number of characteristics which are the continuous Y variables that we want to understand based on one or two sets of groups we care about; the one or two categorical X variables. If however we're only interested in one categorical variable, and we want to control for another variable, we can use MANCOVA.
MANCOVA or multivariate analysis of covariance is an extension of ANCOVA and MANOVA that compares how two or more continuous outcome variables vary according to categorical independent variables while controlling for covariates.
Let's say you're still interested in whether book genre is related to the number of books sold and the amount of profit, but you want to control for the popularity of the author, then you could use MANCOVA. The categorical X variable is still book genre, but now you add a covariant which is the author's social media follower count across social media platforms, which you are controlling for. Then the two Y outcome variables remain the same; the number of books sold per month and the monthly profit.
The $H_o$ is that books sale and monthly profits are equal for all genres regardless of the author's social media following. The $H_1$ is that book sales and monthly profits are not equal for all genres regardless of the author's social media following.
As you continue expanding your data toolkit, you'll encounter more tests, tools, and models that you build off each other. Identifying the connections, and use cases is important as you continue throughout your career as a data professional.
