You'll consider how multiple regression builds on simple linear regression at every step of the modelling process. You'll also get a preview of some key topics in machine learning: selection, overfitting, and the bias-variance tradeoff.
### Understand multiple linear regression
##### Welcome to module 3
Simple linear regression is a great foundational technique but it can feel limiting since it only allows for one independent variable. There are many cases where you might be interested in two, three, or four independent variables.
For example, there could be many factors that are associated with product sales in a given month, such as holidays, new product launches, changes to the retail website, or changes in marketing campaigns. We need a new technique to figure out how each of these variables is correlated with product sales. This is where multiple linear regression or multiple regression comes in. We'll also use PACE framework in multiple regression.
While simple linear regression only allows one independent variable X, multiple regression allows us to have many independent variables that are associated with changes in one dependent variable Y. We'll go back to our statistical basis and revisit PACE. Since this isn't our first model, we're going to start with Analyze, where we review model assumptions for multiple linear regression. EDA will continue helping us determine if our assumption holds true. Then, we'll focus on the Construct phase, where we'll learn how to build the model using Python. Next in the Execute phase, we'll focus on interpretation and telling a story from multiple linear regression.
With more independent variables, we have to think more carefully about the insights we derive and how we communicate our results. Then, we'll iterate back to the Construct phase and learn a bit more about the nuances of multiple linear regression.
As a data analytics professional, one of the most important skills is figuring out the best model for each use case, which goes back to the Plan in PACE. In pursuit of finding the best model, and as models become more complex, we need new ways to evaluate them effectively. As part of finding the best model, we'll learn about variable selection and regularization.
These tools will help you figure out which variables are statistically important when using any datasets. And also help you calculate a possible set of variables for a given model, output and evaluation metrics. As a data professional, reading the output which the computer is performing is important so that you can understand and communicate the relationships the computer has uncovered.
##### Introduction to multiple regression
Multiple regression can help us answer this types of questions:
- a company might be interested in exploring the characteristics of each advertisement  to determine what kinds of advertisements are related to more website clicks. Perhaps shorter advertisements are correlated with more clicks. Or maybe advertisements containing a CTA such as donate or subscribe are associated with more clicks.
Multiple regression or multiple linear regression is a technique that estimates the linear relationship between one continuous dependent variable and two or more independent variables. A full multiple regression equation maybe:$${\text{y}} = B_0 + B_1X_1  + B_2X_2$$Even though we are just adding more Beta coefficients and independent variables, we can still reap the benefits of simple linear regression. Just like simple linear regression, multiple linear regression can yield highly interpretable and communicable results. But because the underlying math is a bit more complex, we have to be mindful of how we convey our results and what the coefficients mean. To ensure we are ethically communicating our results as clearly as possible, we'll go over two concepts: 
- one-hot encoding,
	It allows us to use categorical independent variables in our multiple linear regression. For example, we might have print advertisements and digital advertisements. This is a categorical variable and one hot encoding will allow us to incorporate it in our regression model.
- interaction terms.
	Used when we want to account for how two independent variables affect the Y variable together. 
These two topics are important because they will change how we interpret the coefficients in our model. 
##### Multiple linear regression scenarios
In this reading, you'll explore three scenarios in which multiple regression models can help a company or organization understand a business problem. The goal of the reading is to understand the versatility of multiple linear regression, and to get you thinking about various applications of this powerful and flexible regression technique.
###### Scenario 1: Selling graphic design services
Let's say that you're a data professional working at a company that sells graphic design services. The company you work for might be interested in understanding the factors related to customer satisfaction and retention. There are many ways you can measure this, and you can use many of the following factors to develop a promising multiple linear regression model.
Potential dependent variables (Y)
	Customer satisfaction
	Number of returning customers
	Net promoter score
	Satisfaction with customer service
Potential independent variables (X)
	Cost of service
	Customer service response time
	Adding new graphic design
	Changing page layout
###### Scenario 2: Running a restaurant
Imagine that you are working at a restaurant, and you want to determine how to improve the success of your business. Like any other client-facing business, you want to keep your costs down, your revenue high, and your customers happy. Similar to the prior example, there are many ways to measure the restaurant's success. There are also a number of variables that could be correlated with the chosen metric of success.
Potential dependent variable (Y)
	Total revenue
	Number of review online
	Number of five-star reviews online
	Number of reservations per week
Potential independent variables (X)
	Spending on advertising/marketing
	Operational costs
	Size of menu
	Foot traffic
	Cancellation of reservations
	Business partnerships (ex: delivery apps, farmers' markets, community organizations)
###### Scenario 3: Agricultural production
Suppose you are working in agricultural production, perhaps on a farm or ranch. Even though this is a very different environment from a restaurant or online service, multiple regression can still be helpful. For example, let's say that you are trying to predict crop yield, revenue for the season, or amount of crops sold. From the weather to soil conditions to labor and resource usage, there are many factors that could contribute to a good or a bad year for a farm or any kind of agricultural production. Multiple regression can be used to help better plan and predict for worse years.
Potential dependent variables (Y)
	Crop yield
	Revenue
	Crops sold
Potential independent variables (X)
	Weather (rainfall, temperature)
	Nutrients in the soil
	Historic crop yield
	Cost of fertilizer
	Cost of fuel, water, or energy used to maintain crops
	Cost of labor
	Partnerships with local restaurants or, grocery stores
Key Takeaways
- Multiple regression is a versatile and effective way to understand and describe more complex relationships between variables.
- Multiple regression can be used in a variety of industries and contexts.
### Model assumptions revisited
##### Represent categorical variables
As a data analytics professional, you might encounter scenarios in which you're interested in variables that are not continuous. They could be categorical. For example, in the case of website clicks and advertisement, some ads are black and white and some ads are all in color. Or maybe some ads have a call to action (CTA) while others don't. Or perhaps some product ads are on different streaming services. These are all categorical variables that could be related to how many clicks the website receives.
In the case where we have a categorical independent variable, we have to represent the categories as numbers for the computer to understand the data. There are two main ways to handle categorical data; one-hot encoding and label encoding.
One-hot encoding
This is a data transformation technique that turns one categorical variable into several binary variables. Let's take the example of an advertisement having a call to action (CTA) or not, we would create a new variable in the dataset. Let's call it action, and mathematically, we'll denote it as $X_a$ . If an advertisement has a CTA then $X_a$ = 1. If an advertisement commercial does not have a CTA, the $X_a$ = 0. Our equation would become: 
- $y = B_0 + B_1 X_1 + B_2 X_2 + B_a X_a$ where, $X_1$ measures the number of people in the advertisement and $X_2$ measures the length of the advertisement. 
Now let's assume that we have two advertisements where $X_1$ and $X_2$ are the same. But one has a CTA and one doesn't. We can then assume that the advertisement with the CTA had $B_a$ more website clicks than the advertisement without CTA.
We can also use the same method to represent more than two possibilities. For example, What if we are interested let's say a company is running ads on three services A, B, and C. Let's also assume that ads can only run on one platform at a time. If an ad is on service A, it's not on service B or C. Now we have a categorical X variable that has three possibilities. 
Let's figure out how to represent it mathematically. To represent two possibilities, we use one binary variable. In order to represent three possibilities, we need two binary variables. The equation becomes;$$y = B_0 + B_1 X_1 + B_2 X_2 + B_a X_a + B_b X_b$$
- where, $X_a$ and $X_b$ represents advertisement playing on service A, and B respectively. 
If the ad is playing on service A, then its not playing on service B, and the ad is not playing on service C. In the equation you'll notice that there is no variable X service C or $X_c$ because it would not provide us with more information. 
The interpretation is a little bit different in that, we can think of service C as the default streaming service. $B_a$ or Beta service A is the difference in website clicks for two ads that are exactly the same, except one ad is played on service C and one is played on service A. Similarly, $B_b$ or Beta service B is the difference in website clicks for two ads that are exactly the same, except one ad is played on service C and one is played on service B.
Now you can start estimating how many clicks the website will get based on variables about the ad.
##### Multiple linear regression assumptions and multicollinearity
This reading will help you review assumptions that apply to both simple linear regression and multiple linear regression, and then focus more heavily on the concept of multicollinearity.
Recall that simple linear regression has four main assumptions that provide validity to the results derived from the analysis. To this list of four assumptions, we add the no multicollinearity assumption when working with multiple linear regression.
- Linearity: Each predictor variable ($X_i$) is linearly related to the outcome variable (Y).
- (Multivariate) normality: The variation of the errors is constant or similar across the model.
- Independent observations: Each observation in the dataset is independent.
- Homoscedasticity: The variation of the errors is constant or similar across the model.
- No multicollinearity: No two independent variables ($X_i$ and $X_j$) can be highly correlated with each other.
Note on errors and residuals
'Residuals' and 'errors' are sometimes used interchangeably, but there is a difference. We use residuals to estimate errors when we are checking the normality and homoscedasticity assumptions of linear regression.
- Residuals are the difference between the predicted and observed values. You can calculate residuals after you build a regression model by subtracting the predicted values from the observed values.
- Errors are the natural noise assumed to be in the model.
Extending prior assumptions
Much of what you have learned about the first four assumptions with regard to simple linear regression can be directly applied to multiple linear regression. The code might be slightly different or longer, but the rationale is the same.
###### Linearity
With multiple linear regression, you need to consider whether each X variable has a linear relationship with the Y variable. You can make multiple scatterplots instead of just one, using seaborn's pairplot() function or the scatterplot() function multiple times. Other libraries with plotting capabilities will have similar functions.
###### Independent observations
The independent observations assumption is still primarily focused on data collection. You can check the validity in the same way you would with simple linear regression.
###### (Multivariate) Normality
Just as with simple linear regression, you can construct the model, and then create a Q-Q plot of the residuals. If you observe a straight diagonal line on the Q-Q plot, then you can proceed in your analysis. You can also plot a histogram of the residuals and check if you observe a normal distribution that way.
Note: Its a common misunderstanding that the independent variable and/or dependent variables must be normally distributed when performing linear regression. This is not the case. Only the model's residuals are assumed to be normal.
###### Homoscedasticity
As with simple linear regression, for multiple linear regression, just create a plot of the residuals vs. fitted values. If the data points to be scattered randomly across the line where residuals equal 0, then you can proceed.
###### How to check the no multicollinearity assumption
The no multicollinearity assumption is unique to multiple linear regression as it focuses on potential relationships between different independent (X) variables. When assessing the no multicollinearity assumption, you're interested in identifying any linear relationships between the independent (X) variables. X variables that are linearly related could muddle the interpretation of the model's results. If there are X variables that are linearly related, it is usually best remove some independent variables from the model.
Note, however, that the assumption of no multicollinearity is most important when you are using your regression model to make inferences about your data, because the inclusion of collinear data increases the standard errors of the model's beta parameter estimates. But there may be times when the primary purpose of your model is to make predictions and the need for accurate predictions outweighs the need to make inferences about your data. In this case, including the collinear independent variables may be justified because it's possible that their inclusion would result in better predictions.
There are a few ways to check the no multicollinearity assumption. This reading will cover two of them; one being purely visual while the other is numerical in nature. Both can be done prior to building the linear regression model.
###### Scatterplots or scatterplot matrix
A visual way to identify multicollinearity between independent (X) variables is using scatterplot matrices. The process is similar to when you checked the linearity assumption, except now you're just focusing on the X variables, not the relationship between the X variables and the Y variable. If you're using the seaborn library, you can use the pairplot function, or the scatterplot function multiple times.
###### Variance Inflation Factors (VIF)
Calculating the variance inflation factor, or VIF, for each independent (X) variable is a way to quantify how much the variance for each variable is 'inflated' due to correlation with other X variables. Its important to know that the square root of $VIF_i$ represents the amount that the standard error of coefficient $B_i$ increases relative to a situation in which all of the predictor variables are uncorrelated.
Here's how you calculate VIF in python:
```Python
from statsmodels.stats.outliers_inflence import variance_inflation_factor
X = df[['col_1', 'col_2', 'col_3']]
vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif = zip(X, vif)
print(list(vif))
```
The smallest value a VIF can take on is 1, which would indicate 0 correlation between the X variable in question and the other predictor variables in the model. A high VIF, such as 5 and above, according to statsmodels documentation, can indicate the presence of multicollinearity.
###### What to do if there is multicollinearity in your model
The easiest way to handle multicollinearity is simply to only use a subset of independent variables in your model. For example, if your linear regression model is something like this:$$y = B_0 + B_1 X_1 + B_2 X_2 + B_3 X_3$$But if $X_1$ and $X_3$ are highly correlated, then you can choose to include only $X_1$ or $X_3$ in your final model, but not both.
There are few specific statistical techniques you can use to select variables strategically. 
- Forward selection
- Backward selection
###### Advanced Techniques
These are advanced techniques that you'll come across in your career as a data professional, such as:
- Ridge regression
- Lasso regression
- Principal component analysis
These techniques can result in more accurate and predictive models, but can complicate the interpretation of regression results.
Key Takeaways
Many of the assumptions of simple linear regression extend to multiple linear regression.
You can use scatterplots and VIF to check for multicollinearity in a regression model.
There are different techniques for variable selection to remove multicollinearity in a model.
### Model Interpretation
##### Interpret multiple regression coefficients
If we want to account for how two variables values affect each other, we include an interaction term. An interaction term represents how the relationship between two independent variables is associated with changes in the mean of the dependent variable. Typically we represent an interaction term as the product of the two independent variables in question. 
The interaction is represented with a multiplication symbol. We take into consideration the interaction between the independent variables using the interaction term. 
### Variable selection and model evaluation
##### Underfitting and overfitting
A multiple regression model is built using sample data from the population of interest with the goal of applying the model to unseen data from the population and getting reliable results. Underfitting and overfitting are two obstacles that the multiple regression model must mitigate so it can be applicable. 
###### The two ways a model can be unreliable.
Underfitting
In the case of underfitting, a multiple regression model fails to capture the underlying pattern in the outcome variable. An underfitting model has a low R-squared value.
A model can underfit the data for a variety of reasons. 
- The independent variables in the model might not have a strong relationship with the outcome variable. In this situation, different or additional predictors are needed. 
- It could be the case that sample dataset is too small, and this prevents the model from being able to learn the relationship between the predictors and the outcome. Using more sample data to build the model might reduce the problem of underfitting.
Consider the example of a multiple regression model that predicts the resale price of a pre-owned car. This model has two predictors: the color of the car and the year it was manufactured. The model's R-squared value is quite low. This indicates that the model is underfitting because the current predictors do not have a strong relationship with the car's resale price. There are likely other predictors missing from the multiple regression model, like the mileage on the car or the make of the car.
There are additional reasons that a multiple regression might underfit the data, and the methods used to reduce this obstacle depends on the specific context. Because an underfitting multiple regression model is not able to capture the relationship between predictors and outcome in the sample data, this model will also not be able to produce reliable results when it is used on unseen data from the population.
###### The difference between training data and test data
There is a step data scientists learn before building a multiple regression model. They divide the sample data into two categories called training data and test data. Training data is used to build the model, and test data is used to evaluate the model's performance after it has been built. Splitting the sample data in this way is called holdout sampling, with the holdout sample being the test data. Holdout sampling allows data scientists to evaluate how a model performs on data it has not yet experienced yet.
The holdout sample might also be called the validation data. Regardless, the general idea remains the same: this is the data that is used to evaluate the model.
Data scientists obtain the training and test data by randomly splitting the sample dataset so that each record exclusively belong to one of the two categories. This way, some records are used as the training data and other records are used as the test data.
###### Overfitting
Underfitting causes a multiple regression model to perform poorly on the training data, which indicates that the model performance on test data will also be substandard. In contrast, overfitting causes a model to perform well on training data, but its performance is considerably worse when evaluated using the unseen test data. That's why data scientists compare model performance on training data versus test data to identify overfitting.
Why is there a discrepancy between an overfitting model's performance on training data versus test data?
An overfitting model fits the observed or training data too specifically, making the model unable to generate suitable estimates for the general population. The multiple regression model has captured the signal (i.e. the relationship between the predictors and the outcome variable) and the noise (i.e. the randomness in the dataset that is not part of that relationship). You cannot use an overfitting model to draw conclusions for the population because this model only applies to the data used to build it.
###### Why does overfitting result in a higher R-squared value?
R-squared is a goodness of fit measure because it tells you the proportion of variance in the outcome variable that is captured by the independent variables in the multiple regression model. However, as you add more independent variables to a model, the associated R-squared value will increase regardless of whether or not those predictors have a strong relationship with the outcome variable.
In the example of the multiple regression model that predicts car resale price, you could continue to add more independent variables to the model, such as the number of letters in the name of the person selling the car and the favorite food of the person who bought the car (if you had this data, of course). These predictors are very unlikely to have a relationship with the resale price of the car, but if you add them to your multiple regression model, the R-squared value would still increase. Although this could lead you to think that the model with more predictors is performing better, the inflated R-squared value is a false sign of improvement.
Generally, R-squared will continue to increase with more predictors because the model will become overly specific to the data it was built on even if the predictors do not have a strong relationship with the outcome variable. This is why a high R-squared value is not enough by itself to indicate that the model will perform well and might instead be a sign of overfitting.
###### When to use adjusted R-squared instead
Along with the R-squared value, a multiple regression model also has an associated adjusted R-squared value. The adjusted R-squared penalizes the addition of more independent variables to the multiple regression model. Additionally, the adjusted R-squared only captures the proportion of variation explained by the independent variables that show significant relationship with the outcome variable. These differences prevent the adjusted R-squared value from becoming inflated like the R-squared value.
When comparing between multiple regression models with varying numbers of predictors, you might find that models with more predictors have a higher R-squared value. This could be a result of overfitting. To avoid selecting an overfitting model with an inflated R-squared, use the adjusted R-squared metric to select the optimal model.
###### Bias versus variance
A model that underfits the sample data is described as having a high bias whereas a model that does not perform well on new data is described as having high variance. In data science, there is a phenomenon known as the bias versus variance tradeoff. This tradeoff is a dilemma that data scientists face when building any machine learning model because an ideal model should have low bias and low variance. This is another way of saying that it should neither underfit or overfit. However, as you try to lower bias, variance inevitably increases and vice versa.
This is why you can never fully resolve the problems of underfitting. Instead, focus on reducing these problems in your multiple regression model as much as possible. 
Key Takeaways
Both underfitting and overfitting are obstacles to building a reliable multiple regression model. Although underfitting can be identified by model performance on training data, you must evaluate both training and test performance to identify overfitting. Because overfitting will result in an inflated R-squared value, use the adjusted R-squared value when comparing among multiple regression models with varying number of predictors. Although you cannot fully eliminate underfitting and overfitting from the model because of bias versus variance tradeoff, you can significantly reduce these problems after they have been identified.
##### Top variable selection methods
Overfitting is a problem when we model the observed or training data too specifically and are unable to generate suitable estimates for the general population. Adjusted R-squared is one way to better evaluate how good a model is while factoring in overfitting. Adjusted R-squared is most effective when we can compare multiple models that are using different subsets of independent variables. This is why as a data professional, its important to understand what is variable selection is, since it's a task that happens in your machine.
Variable selection or feature selection is the process of determining which variables or features to include in a given model. Variable selection is iterative. As you grow as a data analytics professional, you will develop a stronger intuition for how to go about variable selection. Here, we will cover forward selection and backward elimination, which are based on extra sums of F-tests.
Forward selection and backward selection essentially work from opposite directions of the problem. We know that a model with zero independent variables is probably not the best choice. We know that a model with all of the possible independent variables is also probably not the best choice. 
Forward selection is a stepwise variable selection process that begins with the null model, with 0 independent variables and considers all possible variables to add. It incorporates the independent variables that contributes the most explanatory power to the model based on the chosen metric and threshold. The process continues one variable at a time until no more variables can be added to the model.
Backward elimination is a stepwise variable selection process that begins with the full model with all possible independent variables and removes the independent variable that adds the least explanatory power to the model based on the chosen metric and threshold. The process continues one variable at a time until no more variables can be removed from the model. 
Both forward selection and backward elimination requires more cutoff points or threshold to determine when to add or remove variables respectively. One common test is the extra sum of squares F-test. 
Extra-sum-of-squares F-test quantifies the difference between the amount of variance that is left unexplained by a reduced model that is explained by the full model. For the F-test, data professionals usually evaluate it based on a p-value. Based on the p-value we can be fairly confident that important variance is being explained by a given model.
This provides a great start to variable selection and making intentional decisions when constructing a multiple regression model.
##### Regularization: Lasso, Ridge, and Elastic Net regression
Linear regression models are versatile in impacting business decisions and strategy, but no tool is perfect. A problem like overfitting happens when a regression model is fit too closely to the training data and therefore has trouble properly estimating the population data. This problem is related to the bias-variance tradeoff, a concept at the heart of statistics and machine learning. The bias-variance tradeoff balances to model qualities, bias and variance. To minimize overall error for unobserved data, the ideal model has some bias and some variance.
Bias simplifies the model predictions by making assumptions about the variable relationships. A highly biased model may oversimplify the relationship, underfitting to the observed data, and generating inaccurate estimates.
Variance in a model allows model flexibility and complexity, so the model learns from existing data. A model with high variance can overfit to observed data and generate inaccurate estimates for the unseen data. Note: This variance is not to be confused with the variance of a distribution. 
We can think of bias and variance as two ends of a scale. We don't want want there to be too much bias or too much variance. So we have to ask ourselves as data professionals how to balance some bias and some variance to minimize error and get the best model possible.
Keep learning as a data professional by attending conferences in your field and also remember studying so as to update your data analytics knowledge. Collaborate and ask questions and you'll learn a lot from colleagues. 
Regularization is a set of regression techniques that shrinks regression coefficient estimates towards zero, adding in bias to reduce variance. By shrinking the estimates, regularization helps avoid the risk of overfitting the model. There are three common regularization techniques: Lasso regression, Ridge regression, and Elastic-net regression. 
For all kinds of regularized regression some bias is introduced to lower variance in the model. Lasso regression completely removes variables that are not important to predicting the Y variable (dependent) of interest. In ridge regression, the impact of less relevant variables is minimized but none of the variables will drop out of the equation. Ridge regression is a great option if you want to include all of the variables. When working with large datasets, we can't always know if we want variables to drop out of the model or not. So we can use Elastic-net regression to test the benefit of lasso, ridge and a hybrid of lasso ridge regression all at once.
Each regularized regression technique is trying to help us better fit our model. But keep in mind that estimated parameters are much harder to interpret than with simple linear regression or multiple regression.