### Getting started with the course.
#### Introduction.
Taught by Robb - A Consumer Product Marketing Leader @ Google.
- Explore how to find and script stories.
- Discuss how PACE applies to telling stories using data.
- Perform EDA --Exploratory Data Analysis on a dataset in Python.
Later in the course, you'll learn:
- Data sources, data types, data structuring, and data cleaning.
- How to work with missing values, outliers, and categorical data.
- Ethics of exploring and cleaning raw data.
- How to communicate your questions and findings to various audiences.
- Tableau - a visual analytics platform.
- How to enhance your data stories with visuals and presentations.
- Tips on shaping your data stories for different audiences. eg how to build data visualizations that target an audience's needs.
Identify the essential skills that are fundamental in working with data. Stories can change lives and data driven stories are particularly compelling because they are based in numbers. They can also communicate principles, concepts, cautions and new ideas to others. The stories are out there just waiting to be found.
### Course 3 overview.
Here is what you'll learn:
- How to find and tell stories using data.
- How the elements of storytelling can enrich our understanding of data and how storytelling can help stakeholders interpret our data.
- How data professionals explore data, find intriguing trends and insights, and then share those stories with others in a data-informed presentation.
- Basic principles and best practices of data exploration using Python.
- Hands-on experience with data visualizations in Tableau, a specialized data visualization tool and explore how to create interesting visualizations, presentations, and talking points about your data.
- Prepare and deliver great presentations.
All these skills will be foundational in your growing knowledge of data exploration and support your journey to become a data-informed storyteller.
### Find stories using the six Exploratory Data Analysis practices.
Data professionals explore data and learn about the trends along the way. As you uncover and clean up what is in front of you, the individual parts and pieces within the data will begin to tell a larger story. You need to make sense of raw data by reordering, categorizing, and reshaping it. There are a lot of different terms in the world for this process. Example of names are, Data wrangling, data remediation, data munging, and data cleaning are the most common names. We combine all these terms practices into familiar term that most data professionals are familiar with: Exploratory Data Analysis or EDA.
#### Exploratory Data Analysis (EDA).
The process of investigating, organizing, and analyzing datasets and summarizing their main characteristics, often employing data wrangling and visualization methods.
The six practices of EDA are:
###### 1. Discovering.
This is EDA's first practice. Data professionals familiarize themselves with the data so they can start conceptualizing how to use it. They review the data, and ask questions about it. During this phase, data professionals may ask:
	what are the column headers, and what do they mean?
	How many total data points are there?
###### 2. Structuring.
After the initial discovery. The process of taking raw data and organizing or transforming it to be more easily visualized, explained, or modeled. It refers to categorizing, and organizing data columns based on the data already in the dataset. Example, calendar data might be categorized into months or quarters, rather than years.
Bias (in data structuring) - is organizing data in groupings, categories, or variables that don't accurately represent the whole dataset. Most experts would agree that eliminating all bias from how data is structured is almost impossible because each person's individual ideas, training and experiences are different. As professionals, its also important to avoid bias. Example, creation of new categories as you make more discoveries. 
Bias exists in many dimensions.
###### 3. Cleaning.
This is the process of removing errors that may distort data or make it less useful. The common issues to address here are missing values, extreme outliers, misspellings, duplicate entries etc.
###### 4. Joining.
The process of augmenting or adjusting data by adding values from other datasets. That is, you might add more value or context to the data by adding more information from other data sources. You might find that during the discover, structuring, or cleaning processes that the dataset doesn't have enough data for you to complete a specific project. In this case, adding more data is necessary.
###### 5. Validating.
The process of verifying that the data is consistent and high quality. This is the process for checking for misspellings, and inconsistent numbers or date formats, and checking that the data cleaning process did not introduce more errors. Digital tools are used by data professionals to check for inconsistencies, data types and errors in the dataset such as R, Python, JavaScript. You can also use subject matter experts' knowledge to validate data.
###### 6. Presenting.
Making your cleaned dataset or data data visualizations available to others for analysis or further modelling. This is sharing what you have learned through EDA and asking for feedback whether in the form of a cleaned dataset or data visualization.
Data visualization - A graph, chart, diagram, or dashboard that is created as a representation of information.
Presenting can come at any point of the EDA process. They should be used through out EDA. They help you understand data, and point out trends and insights to others. You can also present the progress you have made during the data analysis process. What you have found out up to now.
In the workplace as a data professional, presenting might look like preparing visuals, slice presentations to share with your team. As you begin to plan your presentation, you should consider people with visual or auditory impairment by providing robust descriptions of the data. Here, you can use things like alt text, descriptive text, or caption recording of the data so that you audience can explore the data themselves.

These practices do not necessarily have to follow this particular order. Depending on the needs of the data team, and the type of data they study, they may perform EDA in different ways. You will also find that often, the EDA process is iterative meaning that you will go through the six practices multiple times and in no particular order to prepare the data for further use. 
Its important to make sure that your EDA work does not misrepresent the data itself. The story you uncover should come from the data, not your mind or biases in the data. 
It is your duty to convey your data in both an ethical and accessible way. Give an accurate account of the details and don't over or underestimate. In the workplace, for example, communicating data ethically would be presenting sales numbers in context year-over-year so that the rises and falls don't appear exaggerated in data visualizations. You should also confirm any information using historical records before sharing. 
##### key takeaways on EDA.
The six practices of EDA are iterative and non-sequential. EDA is not like a cake recipe. Its not a step-by-step process you follow. Instead, the six practices of EDA are iterative and non-sequential.
	- Iterative: Relating to or involving repetition of a process.
	- Non-sequential: Not arranged in or following an order or sequence.
Because of the varying nature of datasets, the approach to exploring that data will be different each time. That means that you will need to use your logic and experience throughout the EDA process to determine which of the six practices to utilize, how many times to apply them, and when in the process you should apply them.
### Use PACE to inform EDA and data visualization.
As data professionals our curiosity and excitement for finding stories in data might cause us to forget the original purpose for our data exploration. We need to focus that curiosity or maintain it on what problems need to be solved. We seek a balanced mindset, one of targeted curiosity and it can be achieved by using PACE. PACE-- Plan, Analyze, Construct, and Execute is a framework some data analysts use to remain focused. 
##### How do you combine PACE and EDA practices?
EDA stands for Exploratory Data Analysis. It applies to every part of the PACE workflow. Data insights are guided by a project's purpose and goals. The Plan or project plan or the stated goal of the company may come from a stakeholder or manager. When you start your EDA, and realize that the dataset you've been given contain a lot more data than you need, it would benefit you and your company to extract only the columns you need to work with eg to predict sales. While cleaning, joining, validating, you can exclude data you don't require for the task at hand. Cleaning this data is much more manageable than the whole dataset. 
Miscommunication happens even in the data workspace. If the stakeholders, engineers, and data specialists are not clear on the plan, the result will not tell a cohesive or effective story. Instead, the results will likely lead to confusion, disagreement or a waste of time. A strategy to bring clarity is to share the PACE plan with everyone involved. Or sharing the results of analysis with a work group before sharing the analysis more broadly. And its important to understand stakeholder's most important goals for the company before presenting them.
- When you have a dataset in front of you, remind yourself of the reasons for your analysis. They will guide your EDA practice when performing an analysis.
- When using PACE workflow as a data professional, you are able to keep priorities in order and also focus on the project's purpose. 
- As data professionals, its important to accurately represent the data. If your company's project plan does not align with what the data is telling you, its your responsibility to communicate that to stakeholders.
- Timelines, stakeholders' pressure or client needs should never cause a data professional to bypass what is required by the data. Misrepresentation of data is never warranted.
- Data professionals strife to align their work with the Plan part of PACE.
- Keeping the focus on PACE helps determine the most effective ways to perform EDA practices while maintaining ethical representations of data.
##### Example of EDA at work.
Imagine you are assigned a dataset that has only 200 rows and five columns of data about trees in a coniferous forest in Norway. You know that to complete your full analysis you'll need more than 1000 rows and at least two more columns. Even without much more detail than that, your entire EDA process might look something like this:
	1. Discovering: You check out the overall shape, size, and content of the dataset. You find it short on data.
	2. Joining: You add more data.
	3. Validating: You perform a quick check that the new data doesn't have mistakes or misspellings.
	4. Structuring: You structure the data in different time periods and segments to understand trends.
	5. Validating: You do another quick check to ensure the new columns you've made in structuring are correctly designed.
	6. Cleaning: You check for outliers, missing data, and needs for conversions or transformations.
	7. Validating: After cleaning, you double check the changes you made are correct and accurate.
	8. Presenting: You share your dataset with a peer.
Notice you performed 'validating' practice iteratively, or multiple times, to make sure your changes to the data did not unwittingly introduce errors. Also, because you recognized the need for more data up front, the practice of 'joining' was performed immediately following the practice of 'discovering'.
After you present your cleaned dataset to a peer, there is a good chance you will receive notes or ideas for more exploration and/or cleaning. Because of that, you will see even more iterations.
	Pro tip: Data scientists expect to perform the practice of EDA multiple times on a dataset before they feel comfortable declaring it 'clean' and ready for modelling or machine learning algorithms.
#### The importance of EDA in ethical machine learning
As algorithms and machine learning networks begin to make more and more decisions on behalf of individuals, companies, and even governments, the discussion of ethics and regulation becomes more and more important. According to the Institute for Ethical AI & Machine Learning, there are eight principles for developing machine learning systems in a responsible way.
##### Key principles of the EDA process.
The following two principles are inherently part of the EDA process:
	Human augmentation: This principle ensures humans are inserted throughout the AI or machine learning algorithm systems for oversight. Thorough EDA, performed by data scientists, is perhaps one of the best ways to limit bias, imbalance, and inaccuracies being fed into an algorithm.
	Bias evaluation: Without human interference, bias is too easily injected and reproduced in machine learning models. Performing methodical EDA processes will lead data scientists to be aware of and act on biases and imbalances in the data.
The importance of assuring adherence to ethical standards cannot be overstated in the data career space. Data professionals need to continuously grow their capacities to recognize bias and discrimination by consistently applying an ethical mindset to their EDA work. Beyond machine learning, EDA is applicable to nearly any important data-based decision.
### PACE with data visualization.
Data visualization are far more effective at quickly communicating complex information than the data tables. How does this relate to EDA? Data visualizations are important tools that data professionals use to tell stories with data throughout their workflow particularly the EDA practice of presenting. This will help you and others to understand the data no matter where you are in the EDA process. 
Since you will be working with very large datasets, you'll also require to create more visualizations to help you understand how each variable impacts the other. When you start looking at new data, visualize it to understand it or get a good idea of its distribution. Data visualization can also help you explain your datasets to stakeholders and other data professionals. Its your job as a data professional to discover the important points, trends, biases and stories that need to be shared and design data visualizations in an effective way for different audiences. 
When you share your findings, some stakeholders may not understand the data insights without the help of data visualizations. The way data visualizations are designed will communicate different messages to different stakeholders with vested interests in different parts of the project. Data visualizations needs to be designed based on the needs of your audiences. The balance of words and  data visuals in a presentation impacts the business decisions made from data insights. 
A carefully prepared data visualization can mean the difference between changing the mind of a stakeholder and the data being ignored. However, visualizations can also cause confusion or even misrepresent the data. Because you will be the person most familiar with the data story, it is essential that your visualizations not mislead your audiences. Your data driven storytelling, is an opportunity to present facts and visualizations that are ethical, accessible and representative of the data. Being an ethical presenter of the data you analyze, means being honest and very clear about what is and isn't in the data. Design your data visualizations in a way that is accessible to everyone. 
Data visualization Python packages: Matplotlib, Seaborn, Plotly.
Other data visualization software: PowerBI, Tableau.
These tools will be part of your everyday work as a data professional. Remember, creating data visualizations about your dataset will help you throughout the entire EDA process. There often no better tool for telling a good data driven story than a well made visualization.