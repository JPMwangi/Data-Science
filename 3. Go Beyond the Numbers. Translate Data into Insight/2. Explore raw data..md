Finding stories in data using EDA is all about organizing and interpreting raw data. Python can help you do this quickly and effectively. You will learn how to use Python to perform the EDA practices of discovering and sculpting.
### 'Discovering' is the beginning of an investigation.
When you are trying to teach a story you are not an expert on, it can be challenging to explain the details and give clear instructions. As a data professional, you never want to be in this situation with the data you analyze. In fact, your goal should be to know the data very well. When you are reviewing a table of data, its better to know where the data is from, what the column headers mean, what the data will be used for, what the imperfections are, and the small details in between. The reason why we are here is making sense of raw data. 
What you'll learn:
	Data types and data sources.
	Discovering - to get to know the information in the datasets.
	Structuring - apply Python functions to large datasets for many different operations including sorting, filtering, extracting, slicing, joining, and indexing large datasets.
	Formatting - learn how to perform basic corrections or formatting improvements to whole data columns or entire datasets.
	Workplace skills and PACE.
This will help you learn about your dataset and get to tell the story that needs to be told. You also learn what to do when questions about the dataset are raised that you can't answer like why there are missing data fields.
This part of the course is about digging through a dataset for the first time and investigating it as meticulously as possible. It is up to you (the data professional) to find the stories in the data. Comprehensive EDA is the key to useful visualization and models. If there are questions left unanswered or misunderstanding still present in the data after EDA, any presentation or ML model based on it will not be particularly useful. Remember that all datasets have stories to tell.
To understand and drive value in your organization, you have to really understand the underlying data. The right view is to have insight into a thing and to influence decision making.
### Where the data comes from.
Understanding raw data: As a data professional, you will be handling all kinds of different data in a number of different formats and file types. To learn the following:
##### Data sources 
This is the location where the data originates. Its important to know how and when to contact subject matter experts such as engineers, or data base owners. They are in charge of generating the data or delivering the datasets. When you have questions about your data, as you do your discovery, they are the people you reach out to. Knowing the ownership and source of the data is critical because by understanding the data source and who is responsible for it, you can determine its reliability. Does the data owner have the experience in collecting and storing data? Does the data owner have any financial stake in the data's output? Understanding the source of the data will help you in telling the story of the data and in make ethical decisions about its use.
Understanding how data is collected. Examples of data sources:
	Report from a computer system.
	Selection from a large online database.
	Data table that has been manually entered.
Knowing about how the data was gathered will help you understand questions that may come up during EDA. 
##### Data formats
Common files you'll interact with as a data professional are: 
	Tabular files - organize data in tables with table variables organized with columns(representing aspects of objects) and rows(representing objects). Patterns between variables are easily identified.
	XML files. 
	CSV files - comma-separated-values files, easy to create and manipulate, an easily read file type.
	Excel files. 
	DB files - storing data in tables, indexes or fields. Search and storage, require SQL knowledge. 
	JSON files - JavaScript Object Notation files store data in JavaScript format, may contain nested objects within them. Small size, readable by any programming language.
	Other sources - Audio files, HTML files, Text files, Image/photos.
There is no best format for the data. It will depend on the project and storage type. You are just looking for the format that best fits that particular dataset.
##### Data types.
Categories or types of data:
	First-party data - Data that was gathered from inside your own organization.
	Second-party data - Data that was gathered outside your organization but directly from the original source.
	Third-party data - Data gathered outside your organization and aggregated.
Knowing the types of data will help you to be able to more efficiently answer or seek answers to the questions you may have that arise during analysis.
### Python functions.
###### Import datasets with Python
In your career as a data professional, you will come across various datasets that have different file types or are stored in various databases. As you’ve learned previously, it is critical for you to know what these data types are and how to import data using Python. Below you will find examples of importing both databases through connections and data files into Python.
How to import a dataset from a CSV file
```python
import csv
```
Next you can use the with statement and open() function to define the **file name (or file path)** and then the **mode:**
```python
With open(“filename / file path”, ‘mode’)
```
So the syntax so far is:
```python
import csv
with open(“filename / file path”, ‘mode’)
```
The mode is telling the Python library what to do with the file. When defining the **mode**, you use one of the following options:
	- ‘r’ – read
	- ‘w’ – write
	- ‘a’ – append
	- ‘+’ – create new file
Typically, you’ll be defining the mode inside the “with open()” argument field as ‘r,’ because you want Python to open and read the CSV file.
Next, we’ll add “as file” to the end, which you’ll recall is giving the file a data frame name, in this case, we’ll name it csv_df.
```python
import csv
with open("file path",'r') as file:
	csv_df = file.read()
```

###### **Importing a CSV file using pandas**
Instead of using the CSV package, you can also use pandas to import a CSV data file. First, of course, you’ll want to import pandas in your Python notebook.
```python
import pandas as pd
```
Next, you’ll use the “read_csv” function to load the data into a dataframe. Most data professionals use abbreviations, like “DF.” The file path then goes in the argument field.
```python
df = pd.read_csv ('#input file path here')
```
_**Note:**_ _You can also use this same syntax for importing a CSV file that is stored on the internet. In the place of the filename, you would simply copy and paste the URL._

#### How to import data by connecting to a database
There are a number of database solutions that you can connect to with Python, such as BigQuery, MySQL, SQLite, and Oracle. Databases are a convenient way for companies and organizations to store huge amounts of data.
If the dataset is small enough, it can be downloaded and manipulated locally on your computer. However, often the datasets kept in databases are too large to access in their entirety on a personal computer. In this case you have a number of different options, most of which involve querying the database with SQL to obtain specific tables of interest. In other words, you extract select parts—usually specified rows and/or columns—from the whole dataset. The manner in which querying is done can vary with respect to systems, platforms, and interfaces. Because of this variability, this reference guide will only present a couple of different ways to query databases. Specifically, it will explore BigQuery, Google’s data warehouse that provides a wide range of tools and services to facilitate analysis.
**Downloading data from BigQuery**
**Step 1: Access BigQuery**
BigQuery allows you to upload data for storage, and it also has a number of publicly available datasets to explore. You can access these public datasets for free using [BigQuery Sandbox](https://cloud.google.com/bigquery/docs/sandbox), which requires a free Google account. Sandbox gives you 10 GB of active storage and 1 TB of processed query data each month for free.
**Step 2: Perform a query**
Once you have authenticated your account and created a new project as indicated in the instructions linked in step one, you’re ready to query a database. Note that if it is your first time logging in, you may encounter a window asking “New to the BigQueryUI?” with a link to a quickstart guide.
	- From the “Welcome to your SQL Workspace!” page, click the “Compose a new query” button.
	- Click into the search bar in the Explorer on the left side of the page. For example, you can search for “trees.” Initially, this will return zero results. However, click “Search all projects” and it will return applicable datasets from the biquery-public-data project and premade tables from those datasets.
	- Click the street_trees table in the san_francisco dataset. The metadata for this table will appear in a panel to the right. Then, click “Query” from the menu at the top of the metadata panel. You can opt to query in a new tab or in a split-pane of the current window.
	- Now, you can query the table using SQL. For example, the query in the following screenshot selects 5,000 rows with columns of tree_id, plant_type, species, plant_date, and dbh – defined as “depth, height.”
	- Once you are satisfied with your query, click the “Run” button at the top of the SQL query panel. The results will display below, and there is a button to “Save results,” which allows you to save the resulting table in different locations and formats. From there, you can read the data into your notebook.
**Using notebooks within BigQuery**
Another way to access data on BigQuery is by using the tools within the BigQuery platform itself. This workflow more closely resembles what data professionals would use when working with very large datasets stored in the cloud. Essentially, you set up a virtual machine on BigQuery. A virtual machine is a computer that has its own CPU, memory, software, etc., just like any other computer, only it does not have its own dedicated hardware; they most often exist as a partition on a server. You can work in a Jupyter notebook on the virtual machine on the BigQuery platform, from which you can query and pull in data directly.
This process requires you to set up a payment method. However, new users get a $300 credit, and a ML instance is only a few cents per minute, so you’ll get approximately 2,000 hours of free usage before incurring any charges. There are a lot of great tutorials for setting this up. For instance, if you search for “How to use Jupyter notebook in Google Cloud AI,” you’ll find a number of useful videos and blogs on the topic.
**Using notebooks outside of BigQuery**
It’s also possible to query data on BigQuery from notebooks that are not on the BigQuery platform. However, the details of this process are dependent on a number of factors, including the platform that is hosting the notebook, the operating environment, and the specific location of the data being accessed. Therefore, we will not go into depth on this method. Feel free to explore this on your own, though. You’ll find many helpful online resources that are just a search away.
Key takeaways
There are lots of different kinds of data, which means there are numerous ways to import data. Learning several methods to import data, whether it be from a data file or a database, will build a solid foundation for your career as a data professional.

### Pandas methods for the discovery of a dataset
**Head()**, **info()**, **describe()**, and **shape** are all Python functions that data scientists can use to understand a dataset at a high level. The information learned from running these functions will serve to inform the remainder of your EDA work when you use Python to analyze data throughout your career.

### Datetime manipulation.
Date string manipulations require the datetime package to be imported first.
```python
from datetime import datetime
```
Datetime in NumPy and pandas.
A preface regarding terminology in the following section: datetime refers to the specific module of that name in the Python standard library or to the specific class within that module. Datetime (or uncapitalized, datetime) refers to any date/time-related object from any library or language.
You’ve learned that the datetime [module in Python’s standard library](https://docs.python.org/3/library/datetime.html) contains a number of classes used to work with time data, including date, time, datetime, timedelta, timezone, and tzinfo. Remember, modules are similar to libraries, in that they are groups of related classes and functions, but they are generally subcomponents of libraries. Classes are data types that bundle data and functionality together.
NumPy and pandas have their own datetime classes that offer significant performance boosts when working with large datasets. Pandas datetime classes, like the rest of the pandas library, are built on NumPy. These classes have very similar (and in many cases identical) functionality to Python’s native datetime classes, but they run more efficiently due to NumPy and pandas’ vectorization capabilities. Therefore, although you _can_ use datetime data in pandas, it’s generally better to use NumPy or pandas datetime objects when working in pandas, if possible.
[NumPy’s datetime classes](https://numpy.org/doc/stable/reference/arrays.datetime.html) include, most notably, datetime64 and timedelta64. Like datetime objects, datetime64 objects contain date and time information in a single data structure; and, like timedelta objects, timedelta64 objects contain information pertaining to spans of time.
[Pandas’ datetime classes](https://pandas.pydata.org/docs/user_guide/timeseries.html) include Timestamp, Timedelta, Period, and DateOffset. Because these classes are efficient and dynamic in their capabilities, you often don’t need to import the datetime module when working with datetime data in pandas. Also, pandas will automatically recognize datetime-like data and convert it to the appropriate class when possible. Here’s an example:
```python
data = ['2023-01-20', '2023-04-27', '2023-06-15']
my_series = pd.Series(data)
my_series
```
This series contains string data, but it can be converted to datetime64 data using the pd.to_datetime() function:
```python
my_series = pd.to_datetime(my_series)
my_series
```
Refer to the [pandas to_datetime() documentation](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) for more information about this function. When a Series object contains datetime data, you can use dt to access various properties of the data. For example:
```python
print(my_series.dt.year)
print()
print(my_series.dt.month)
print()
print(my_series.dt.day)
```
Note that it’s not uncommon to import the datetime module from Python’s standard library as dt. You may have encountered this yourself. In such case, dt is being used as an alias. The pandas dt Series accessor (as demonstrated in the last example) is a different thing entirely. Refer to the [pandas dt accessor documentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.html) for more information.
Key takeaways
Use reference guides throughout your career to help remind you of the different ways to manipulate datetime objects. Even experts in the field use reference guides, rather than memorizing all this information. Getting familiar with guides will be beneficial because you will be using them throughout your career as a data professional.
### Pandas tools for structuring a dataset.
Functions for combining data
**merge()**
- Description: Use ‘merge()’ to take columns or indexes from other data frames and combine them.
- Example from Notebook:
```python
df1.merge([‘month’, ’year’],on=list, sort=True)
```
**concat()**
- Description: Use ‘concat()’ to join columns or rows.
- Example from Notebook:
```python
df3 = pd.concat([df1.drop(['weekday',' week'],axis=1), df2])
```
**df.join()**
- Description: Use ‘df.join()’ to combine columns from other data frames.
- Example from Notebook:
```python
df.join(other.set_index('key'), on='key')
```
A visual representation combination functions:
![[Pasted image 20240128175948.png|450]]

Functions for extracting data
**`df[[columns]]``**
- Description: Use ``df[[columns]]`` to extract select columns for a data frame.
- Example(s) from Notebook:
```
 df1[[‘day’, ‘week’, ‘weekday’, ‘month’, ‘year’]]
```
**df.select_dtypes**
- Description: Use ‘df.select_dtypes’ to filter a specific data type, like objects, strings, or integers.
- Example from Notebook:
```python
df.select_dtypes(exclude=['int64'])
```
Visual representation of extraction functions:
![[Pasted image 20240128180637.png|450]]
Functions for filtering data
**``df[condition]``**
- Description: Use ``df[condition]`` to filter a data column according to selected variables.
  - Example from Notebook:
```python
df1[‘year’ = > 2000]
```

Visual representation of filtering functions:
![[Pasted image 20240128180837.png|450]]
Functions for sorting data
**pd.sort_values()**
- Description: Use ‘pd.sort_values()’ to sort data according to selected parameters.
 - Example from Notebook:
```
df.sort_values(by=[date])
``` 
Visual representation of sorting functions:
![[Pasted image 20240128180943.png|450]]
Functions for slicing data
**df.iloc()**
- Description: Use ‘df.iloc()’ to slice data based on an integer location.
- Example from Notebook:
```python
df.iloc(1, 10)
```
**df.loc()**
- Description: Use ‘df.loc()’ to slice data based on label or Boolean array.
- Example from Notebook:
```python
df.loc([‘weekday’], 10)
```
Visual representation of slicing functions:
![[Pasted image 20240128181129.png|450]]
Understanding the above will help you handle questions and challenges as they arise during the analysis. 
### Histograms
The purpose of EDA-- exploratory data analysis is to explore and analyze the data. As a data professional, you'll almost always begin with a guiding question or objective, such as, 'Where are the highest emitters of carbon dioxide?' or 'Determine the characteristics of people most likely to buy product X.' Reflecting on this often throughout the process creates a driving force that keeps you on track.
A histogram is one of the most important tools at your disposal. Its a graphical representation of a frequency distribution, which shows how frequently each value in a dataset or variable occurs. It's essential for data professionals to understand the distributions of their data, because this knowledge drives many downstream decisions around experiment design, modelling, and further analysis. Here, you'll learn about histograms, what they are, how to make them, and how to interpret them.
##### Introduction to histograms.
Histograms are commonly used to illustrate the shape of a distribution, including the presence of any outliers, the center of the distribution, and the spread of the data. Histograms are typically represented by a series of bars, where each bar represents a range of values. Bar height represents the frequency or count of the data points within that range.
##### The importance of histograms.
Histograms are an essential tool for understanding the characteristics of a dataset. They provide a visual representation of the data's distribution  and enable data professionals to identify patterns, trends, or outliers within the data. Histograms can also help data professionals choose appropriate statistical tests and models for the data and determine whether the data meets any assumptions required for the analysis. Histograms are widely used in any field and any situation that requires any kind of data analysis, including finance, health care, engineering, and social sciences.
##### How to interpret histograms.
Interpreting histograms involves understanding the shape, center, and spread of the distribution. There are several common shapes of histograms, including:
1. Symmetric: A symmetric histogram has a bell-shaped curve with a peak in the middle, indicating that the data is evenly distributed around the mean. This is also known as normal, or Gaussian, distribution.
2. Skewed: A skewed histogram has a longer tail on one side than the other. A right-skewed histogram has a longer tail on the right side, indicating that there are more data points on the left side of the histogram. A left-skewed distribution has a longer tail on the left side, indicating more data points on the right side.
3. Bimodal: A bimodal histogram has two distinct peaks, indicating that the data has two modes. 
4. Uniform: A uniform histogram has a flat distribution, indicating that all data points are evenly distributed.
The examples provided above are not the only distributions you'll encounter, but they are some of the most common.
In addition to the shape of a distribution, its important to understand the center and spread. The center of the distribution is typically represented by the mean or median, while the spread is represented by the standard deviation or range of the data. The center and spread can provide insights into data concentration and variability.
##### How to create histograms.
Python's seaborn and matplotlib libraries provide simple and powerful options to create histograms.
```python
import matplotlib.pyplot as plt
plt.hist(x, bins=10,...)
```
To generate a histogram in matplotlib, use the hist() function in the pyplot module. The function can take different arguments, but the primary ones are:
	x: A sequence of values representing the data you want to plot. It can be a list, tuple, NumPy array, pandas series, and so on.
	bins : The number of bins you want to sort your data into. The default value is 10, but this parameter can be an integer, sequence, or string. If you use a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. In other words, if:
```python
bins=[1,3,5,7]
```
then, the first bin is 1-3 (including 1, but excluding 3) and the second 3-5. The last bin, however, is 5-7, which includes 7. A string refers to a predefined binning strategy supported by NumPy. For more information, refer to the documentation. 
An example demonstrating how to generate the Old Faithful geyser histogram using plt.hist() function.
```python
# plotting histogram with matplotlib pyplot
import matplotlib.pyplot as plt

plt.hist(df['seconds'], bins=range(40, 101, 5))
plt.xticks(range(35, 101, 5))
plt.yticks(range(0, 61, 10))
plt.xlabel('seconds')
plt.ylabel('count')
plt.title('Old Faithful geyser - time between eruption')
plt.show()
```
In this case, the data being plotted is the seconds column of the dataframe. The bins begin at 40 seconds and go to 100 seconds in steps of five, for a total of 12 bins.
```python
sns.histplot(x, bins, binrange, binwidth)
```
One way to generate a histogram on seaborn is to use the sns.histplot() function. Like the matplotlib function, sns.histplot() can take many arguments. Here are some important ones:
	x: The data sequence. Same as plt.hist()
	bins: same as plt.hist()
	binrange(): Lowest and highest value for bin edges; can be used either with bins or binwidth; defaults to data extremes.
	binwidth: Width of each bin, overrides bins but can be used with binrange()
Running the Old Faithful geyser example using seaborn histplot() function. It uses all of the above mentioned parameters. 
Notice in this case that binrange is defined from 40 to 100 and binwidth is set to 5. This produces the same results as setting bins = range(40, 101, 5). This example also makes use of a couple of style parameters by specifying a particular color using hex code notation and setting the color saturation level to 100%, as indicated by the alpha parameter.
```python
# plotting histogram with seaborn
import seaborn as sns
ax = sns.histplot(df['seconds'],binrange=(40, 100),binwidth=5,color='#4285F4',aplha=1)
ax.set_xticks(range(35, 101, 5))
ax.set_yticks(range(0, 61, 10))
plt.title('Old Faithful geyser - time between eruption')
plt.show()
```
### Key takeaway.
Histogram help data professionals understand the frequency distributions of their dataset and variables. Knowledge of the shape and type of data distribution will affect important downstream decisions, such as statistical tests and model architecture selection. Additionally, knowing the shape of your data gives valuable insights into the story that your data is telling you by helping you understand its distributional trends.