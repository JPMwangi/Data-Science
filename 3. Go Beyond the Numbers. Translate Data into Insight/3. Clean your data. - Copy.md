You'll explore 3 more EDA practices: cleaning, joining, and validating. You will discover the importance of these practices for data analysis, and you'll use Python to clean, validate, and join data.

### What we will learn:
- Missing values.
- Outliers.
- Transforming categorical into numerical data.
- Input validation.
- Workplace skills
	- When to communicate with stakeholders and engineers about missing and outlier values.
	- Ethical implications you must consider when dealing with missing and outlier data values.
### Methods of handling missing data.
When dealing with missing pieces, you should have a plan, strategy, or protocol for dealing with missing data. When you have missing entries in your dataset, you have to decide on a plan for dealing with them and be ready to communicate with stakeholders if missing values impact your analysis. 
Missing values is defined as a value that is not stored for a variable in a set of data. It is encoded as N/A, NaN, or Blank. This is different from a data point of 0. 
Data professionals often experience the challenge of missing data. Since every dataset is different, there is a wide variety of reasons for values to be missing. The impact of data fields with a NaN --Not a Number can range from negligible to substantial.
The impact will determine how you communicate to stakeholders or clients, ranging from a note at the bottom of a data visualization about the fact that there's missing data, to a face to face meeting about how analysis cannot be completed due to the large number of missing values.
Missing values in a dataset creates ethical questions too. If you encounter a level of missing values that could impact your analysis process as a data professional, you should communicate the inability to complete any analysis and suggest possible solutions. 
The challenge of the 0 (zero) value. In some datasets, the value zero could be considered a missing value, but in other datasets could be a legitimate data point. In some datasets, a NaN or blank space might be a mistake. Someone forgot to fill it in. Or the data may have been left blank because the column may not apply to that data point.
Ask the owner of the data about the blank spaces legitimacy when unsure of whether or not the blank spaces is intentional. Whenever you find missing data, you have a choice to how to handle it. As a data professional, you should consider how the missing data might impact stakeholders and who should be made aware. 
Here are 4 common ways to handle missing data.
- Request the missing values to be filled by the owners of the data.
If there are large quantities of missing values in the data you receive, the best methods to handle the missing data would be to contact the owners of the missing data and request that the data be filled in.
In some datasets we work with, the option to get missing data filled in may not be feasible for a variety of reasons. For example, when working on a time sensitive project, you might not have time to gather more data before your deadline. Or, the data cannot be retrieved because the study happened too long ago. 
- Delete the missing column(s), row(s), or value(s).
If  filling in missing data is not an option, you can choose the option of deleting the missing data. Though some might assume it's not appropriate to delete data, removing rows or whole columns of data is ideal. If there isn't a large percentage of NaN's or the values are not going to impact the business plan for the data. 
Watch out for deleting missing data that is not missing at random. Deleting values that have been left blank intentionally can skew the results of your analysis. 
- Create a NaN category.
This is a good strategy if the missing data itself is categorical rather than numerical. 
- Derive new representative values.
This strategy can be more helpful with business plans that call for a predicted value or forecast. There are multiple methods within this filling in strategy to choose from. They include:
	- Forward filling
	- Backward filling (backfilling)
	- Deriving mean values
	- Deriving median values
These are the most common methods for handling missing data. Choosing which method comes with experience, intuition, and reasoning. You may also use more than one approach in a dataset. Every dataset and project plan will be different. So you will be making this determination each time you encounter new data. Sometimes you will want to confer with peers, or managers, or stakeholders in order to make a decision. The impact missing values has on your analysis, should be the determining factor in whom you should contact. 
As a data professional, it is essential that your are thoughtful and intentional about how missing data are addressed. Consider the quantity of NaNs and their importance in relation to the project plan. Ask yourself, how will this approach impact this dataset? And, what are the ethical considerations?
Just like the board game scenario, it is essential to determine a strategy and decide on a plan.

### The challenge of missing or duplicate data.
##### Data deduplication with Python.
Data cleaning and validating practices include several different steps, including handling missing data, outliers, and label encoding; checking for misspellings; and, handling duplicates. As a data professional, it will be your task to know how best to handle data values in those categories.
In this reading, you'll learn more about handling duplicates. You will also learn to identify and decide whether deduplication is the right strategy for a dataset. You'll also learn some common functions for handling duplicates.
##### Identifying duplicates.
Before we make any decisions on whether to remove duplicate values or not, we should first determine if duplicate values are present in our dataset.
Ways to identify duplicates is to use this pandas function.
```python
# import pandas and numpy libraries
import pandas as pd
import numpy as np

# create a df
df = pd.to_DataFrame(1,2,3,4,4,4,5,6,7,7,8)

# check if there are duplicates in df
duplicated(df)
```
The duplicated() function is a DataFrame method which returns a series of 'true / false' outputs, with 'true' indicating the data value is a duplicate, and 'false' indicating it is a unique value. 
The process is different when identifying duplicates for an entire dataset as compared to the example above. The duplicated() function will only return entire rows that have exactly matching values found within a column. If you wish to identify duplicates for only one column or a series of columns within a dataframe, you will need to include that in the 'subset' portion of the argument field of the duplicated() function. Going further, if you'd like to specify which of the duplicates to keep as the 'original' as opposed to the duplicate, you can specify that in the keep portion of the argument field.

Below is an example of identifying duplicates in only one column (subset) of values and labeling the last duplicates as 'false' so that they are 'kept'.

```python
print(df)
print()
print(df.duplicated(subset=['type'], keep='last'))
```
##### Decision time: To drop or not to drop?
Every dataset is unique and you cannot treat every dataset the same. When you are making the decision on whether to eliminate duplicate values or not, think deeply about the dataset itself and about the objective you wish to achieve. What impact will dropping duplicates have on your dataset and your objective?
###### 1. Deciding to drop
You should drop or eliminate duplicate values if duplicate values are clearly mistakes or will misrepresent the remaining unique values in the dataset. 
###### 2. Deciding to NOT drop
You should keep duplicated data in your dataset if the duplicate values are clearly not mistakes and should be taken into account when representing the dataset as a whole. 
##### Don't be duped-- How to do deduplication.
Deduplication - The elimination or removal of matching data values in a dataset. 
The common ways of doing this:
```python
df.drop_duplicates()
```
This method is another DataFrame method that is used to create a new dataframe with all of the duplicate rows removed. It will drop duplicates of exact matches of entire rows of data. If you wish to drop duplicates within a single column, you will need to specify which columns to check for duplicates using the subset keyword argument. For example:

```python
df.drop_duplicates(subset='style')
```

And this example drops all rows (except the first occurrence) that have duplicate values in both the style and rating column.

```python
df.drop_duplicates(subset=['style', 'rating'])
```

##### Key Takeaways
Identifying duplicate data values in a dataset is an important part of EDA (or Exploratory Data Analysis) practices, specifically cleaning and validating. After identifying duplicates, think about the impact to the dataset and your analysis objective when choosing to eliminate duplicates or not eliminate duplicates.

