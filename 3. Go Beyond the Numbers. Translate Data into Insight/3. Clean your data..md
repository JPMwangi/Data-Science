You'll explore 3 more EDA practices: cleaning, joining, and validating. You will discover the importance of these practices for data analysis, and you'll use Python to clean, validate, and join data.
### What we will learn:
- Missing values.
- Outliers.
- Transforming categorical into numerical data.
- Input validation.
- Workplace skills
	- When to communicate with stakeholders and engineers about missing and outlier values.
	- Ethical implications you must consider when dealing with missing and outlier data values.
### Methods of handling missing data.
When dealing with missing pieces, you should have a plan, strategy, or protocol for dealing with missing data. When you have missing entries in your dataset, you have to decide on a plan for dealing with them and be ready to communicate with stakeholders if missing values impact your analysis. 
Missing values is defined as a value that is not stored for a variable in a set of data. It is encoded as N/A, NaN, or Blank. This is different from a data point of 0. 
Data professionals often experience the challenge of missing data. Since every dataset is different, there is a wide variety of reasons for values to be missing. The impact of data fields with a NaN --Not a Number can range from negligible to substantial.
The impact will determine how you communicate to stakeholders or clients, ranging from a note at the bottom of a data visualization about the fact that there's missing data, to a face to face meeting about how analysis cannot be completed due to the large number of missing values.
Missing values in a dataset creates ethical questions too. If you encounter a level of missing values that could impact your analysis process as a data professional, you should communicate the inability to complete any analysis and suggest possible solutions. 
The challenge of the 0 (zero) value. In some datasets, the value zero could be considered a missing value, but in other datasets could be a legitimate data point. In some datasets, a NaN or blank space might be a mistake. Someone forgot to fill it in. Or the data may have been left blank because the column may not apply to that data point.
Ask the owner of the data about the blank spaces legitimacy when unsure of whether or not the blank spaces is intentional. Whenever you find missing data, you have a choice to how to handle it. As a data professional, you should consider how the missing data might impact stakeholders and who should be made aware. 
Here are 4 common ways to handle missing data.
- Request the missing values to be filled by the owners of the data.
If there are large quantities of missing values in the data you receive, the best methods to handle the missing data would be to contact the owners of the missing data and request that the data be filled in.
In some datasets we work with, the option to get missing data filled in may not be feasible for a variety of reasons. For example, when working on a time sensitive project, you might not have time to gather more data before your deadline. Or, the data cannot be retrieved because the study happened too long ago. 
- Delete the missing column(s), row(s), or value(s).
If  filling in missing data is not an option, you can choose the option of deleting the missing data. Though some might assume it's not appropriate to delete data, removing rows or whole columns of data is ideal. If there isn't a large percentage of NaN's or the values are not going to impact the business plan for the data. 
Watch out for deleting missing data that is not missing at random. Deleting values that have been left blank intentionally can skew the results of your analysis. 
- Create a NaN category.
This is a good strategy if the missing data itself is categorical rather than numerical. 
- Derive new representative values.
This strategy can be more helpful with business plans that call for a predicted value or forecast. There are multiple methods within this filling in strategy to choose from. They include:
	- Forward filling
	- Backward filling (backfilling)
	- Deriving mean values
	- Deriving median values
These are the most common methods for handling missing data. Choosing which method comes with experience, intuition, and reasoning. You may also use more than one approach in a dataset. Every dataset and project plan will be different. So you will be making this determination each time you encounter new data. Sometimes you will want to confer with peers, or managers, or stakeholders in order to make a decision. The impact missing values has on your analysis, should be the determining factor in whom you should contact. 
As a data professional, it is essential that your are thoughtful and intentional about how missing data are addressed. Consider the quantity of NaNs and their importance in relation to the project plan. Ask yourself, how will this approach impact this dataset? And, what are the ethical considerations?
Just like the board game scenario, it is essential to determine a strategy and decide on a plan.

### The challenge of missing or duplicate data.
#### Data deduplication with Python.
Data cleaning and validating practices include several different steps, including handling missing data, outliers, and label encoding; checking for misspellings; and, handling duplicates. As a data professional, it will be your task to know how best to handle data values in those categories.
In this reading, you'll learn more about handling duplicates. You will also learn to identify and decide whether deduplication is the right strategy for a dataset. You'll also learn some common functions for handling duplicates.
#### Identifying duplicates.
Before we make any decisions on whether to remove duplicate values or not, we should first determine if duplicate values are present in our dataset.
Ways to identify duplicates is to use this pandas function.
```python
# import pandas and numpy libraries
import pandas as pd
import numpy as np

# create a df
df = pd.to_DataFrame(1,2,3,4,4,4,5,6,7,7,8)

# check if there are duplicates in df
duplicated(df)
```
The duplicated() function is a DataFrame method which returns a series of 'true / false' outputs, with 'true' indicating the data value is a duplicate, and 'false' indicating it is a unique value. 
The process is different when identifying duplicates for an entire dataset as compared to the example above. The duplicated() function will only return entire rows that have exactly matching values found within a column. If you wish to identify duplicates for only one column or a series of columns within a dataframe, you will need to include that in the 'subset' portion of the argument field of the duplicated() function. Going further, if you'd like to specify which of the duplicates to keep as the 'original' as opposed to the duplicate, you can specify that in the keep portion of the argument field.

Below is an example of identifying duplicates in only one column (subset) of values and labeling the last duplicates as 'false' so that they are 'kept'.

```python
print(df)
print()
print(df.duplicated(subset=['type'], keep='last'))
```
### Decision time: To drop or not to drop?
Every dataset is unique and you cannot treat every dataset the same. When you are making the decision on whether to eliminate duplicate values or not, think deeply about the dataset itself and about the objective you wish to achieve. What impact will dropping duplicates have on your dataset and your objective?
###### 1. Deciding to drop
You should drop or eliminate duplicate values if duplicate values are clearly mistakes or will misrepresent the remaining unique values in the dataset. 
###### 2. Deciding to NOT drop
You should keep duplicated data in your dataset if the duplicate values are clearly not mistakes and should be taken into account when representing the dataset as a whole. 
##### Don't be duped-- How to do deduplication.
Deduplication - The elimination or removal of matching data values in a dataset. 
The common ways of doing this:
```
df.drop_duplicates()
```
This method is another DataFrame method that is used to create a new dataframe with all of the duplicate rows removed. It will drop duplicates of exact matches of entire rows of data. If you wish to drop duplicates within a single column, you will need to specify which columns to check for duplicates using the subset keyword argument. For example:

```
df.drop_duplicates(subset='style')
```

And this example drops all rows (except the first occurrence) that have duplicate values in both the style and rating column.

```
df.drop_duplicates(subset=['style', 'rating'])
```

### Account for outliers.
Outliers are observations that are an abnormal distance from other values or an overall pattern in a data population. There are 3 main types of outliers. We'll study why its important to identify them in the data we analyze. 
As a data professional, be fully aware of the beginnings and ends, highs and lows, and extreme points of your data across every variable in the dataset. For these values will often be your outliers. Its essential that you recognize them, what their value is and where they are. Otherwise, they will likely skew any conclusions you draw or models you build from them. 
##### Types of outliers.
- Global outliers.
Values that are completely different from the overall data group and have no association with any other outliers. They are the easiest to spot. They may be inaccuracies, typographical errors, or just extreme values you don't typically see in a dataset. Typically, these outliers are thrown out to create a predictive model.

- Contextual outliers.
Normal data points under certain conditions but become anomalies under most other conditions. They can be trickier to spot. They are more common in time-series data. E.g. Movie sales are expected to be much larger when its first released, if there is a huge spike in sales a decade later, that would typically be considered abnormal or a contextual outlier.   
Another example might be an outlier only in a specific single category of data. For example, 2.5M maybe a normal enough size for a category called mammal heights, but if the mammal is found to be a mouse, we would likely consider this to be an outlier despite it fitting with other mammal heights. 

- Collective outliers.
A group of abnormal points that follow similar patterns and are isolated from the rest of the population. Example: To have a parking lot of a convenient store fully parked after its closing ours is considered a collective outlier.
#### How to spot outliers.
- Using visualizations.
It will be easier to see any dips or giant blips in the data when we plot it on a line graph or a bar chart. No matter how you discover that your data contains global, contextual, or collective outliers, it is essential that your EDA includes a strategy for dealing with them. 
It will be up to you to decide how outliers need to be represented or whether they need to be removed completely. The decision on what to do must always be done in the context to the dataset and the business plan for the data. Always consider the ethical implications of any decision you make about outliers. When working as a data professional, it may be tempting to remove outliers to improve results, predictions or forecasts. But, you need to ask yourself: Does this change the data story?


### Protect the people behind the data.
For data professionals, it is worth the time and effort to stop the process and fix the mistakes. 
#### The big picture.
Even seasoned data professionals can fail to see the big picture. As intuitive as our brains can be, we often fail to see how a small error, a tiny change, or a seemingly insignificant choice about data analysis can have significant implications on a process, a business, or even a whole population.
This means that the results of the dataset you are on are part of a bigger picture. And this is why you should be aware of the big picture of what you are working on.
#### An ethical mindset.
One principle that can help data professionals keep data in context is ethics and developing an ethical mindset. Here are three important concepts to help you foster an ethical mindset as you seek to tell stories with data:
##### Models have lives beyond code blocks and data strings.
There can be a tendency to think of data as strictly numbers and math. The reality is that data consists of people-driven inputs. Data professionals should never lose sight of the fact that analysis in all forms-- visualizations, models, decisions, and strategies--impact people.
The end goal of all data analysis and data science work should be to improve the lives of individuals and groups. Next time you are designing a data dashboard or coding a complex algorithm, take a moment to consider how the data might impact human beings, and the decisions you make alter that impact for the better. Remember that your own biases might prevent you from being able to anticipate many impacts, so gathering input from a diverse group of team members helps you mitigate your personal biases.
##### Data science needs regulatory compliance.
If you are unfamiliar with the laws and regulation of the industry you work in, your job will not only be much more difficult but also could land you in legal trouble. A data professional should always remain up to date and in compliance with all regulations in their particular field. As an example, when you begin a new job in the data career space, you can do the following:
- Ask your manager for a compliance guidance.
- Take time to research the data governance policies.
- Take time to research the regulatory body of your particular industry and its relevant policy documents.
Keep in mind that the data governance field is quite young and that regulations can and will change at a quick pace Be sure to keep up with the changes.
Its important to follow the rules not just for rules' sake. Here are a few important benefits of remaining compliant with regulatory bodies and data governance policies:
- Keeps clients and company data safe from security threats.
- Bolsters trust with clients, peer groups, companies, and public.
- Lessen the likelihood of lost or mismanaged data.
- Ensures business critical data remains useable, accessible, and available.
##### Customers should choose how their data is used.
With each passing day, technology gets more advanced and capable. As data becomes increasingly involved in our day-to-day lives, it stands to reason the data itself becomes more and more important and valuable. Because of this, data privacy and security is critical to a company's clients and a government's citizens. 
As data professionals, it is our ethical responsibility to treat client customer data with respect and dignity. Companies that store, analyze, and utilize data as part of their business processes should make sure those processes are transparent and compliant. Customers who divulge confidential information in order to procure a service should be able to trust that the data will remain confidential and not compromised due to breaches or cyber security threats.
Companies have began to treat customer data with more internal care and security. More companies are providing extensive employee training regarding the handling and securing of customer data. Many are adding multi-factor authentication systems and auditing their third-party vendors, requiring high levels of digital security systems and platforms.
Because of data breaches, corporations are increasingly ramping up cybersecurity, employee and vendor training, and transparency to customers and the public regarding data gathering and data use methods. Some have added data security into risk management plan, and built data security web pages that describe what data they gather and how they use it.
Data transparency is not only good for business, it is the right thing to do--the ethical thing to do. Data professionals need to understand that more than anyone.
### Sort numbers versus names
##### Categorical data
Data that is divided into a limited number of qualitative groups. Working with categorical data is frustrating. Example, demographic information tends to be categorical, like occupation, ethnicity, and educational attainment. Another way to think about it is that its data that uses words or qualities rather than numbers. Categorical data entries can typically be identified quickly because they are often represented in words and have a limited number of possible values. 
Many data models and algorithms don't work as well with categorical data as they do with numerical data. Assigning numerical representative values to categorical data is often the quickest and most effective way to learn about the distribution of categorical values in a dataset. There are some algorithms that work well with categorical variables in word form like decision trees. However, with many datasets the categorical data will need to be turned into numerical data. This conversion is often essential for predicting, classifying, forecasting and more. 
There are several ways to change categorical data to numerical data. These are creating dummy variables and label encoding. 
- Dummy variables
Variables with values of 0 or 1, which indicate the presence or absence of something. You can look at it as a 1 representing a 'Yes', and a 0 representing a 'No'. Dummy variables are especially useful in statistical models and machine learning algorithms. They are assigned for each value in a column. The same goes for each value in other categories.
- Label encoding
This is data transformation technique where each category is assigned a unique number instead of a qualitative value. This is assigned for each category in a dataset. For example; Imagine you have been given a dataset about mushrooms with a column titled types. And there are listed many different types of mushrooms. You could use label encoding to transform each mushroom type into a number. That is for each type of mushroom, assign a number to it. 
##### Why do data professionals use label encoding?
Data is much simpler to clean, merge and group when its all numbers. It also takes up less storage space. An algorithm or model typically runs smoother when we take the time to transform our categorical data into numerical data. 
Of course there will be models and algorithms in which you may not want to perform label encoding. Let the business need be your guide on whether or not you'll need to perform label encoding. The type of model or algorithm you choose will determine whether or not you encode labels. 
Much like making sure you have the right phone charger to plug into your phone, as a data professional, you need to make sure your data is ready to use with models or algorithms.

### Other approaches to data transformation.
As you know, data comes to us in many different forms. For categorical or qualitative data types, data professionals often need to transform (or encode) this type of data into numeric digits to complete their analysis, design their data visualization, or build their machine learning algorithm. In this reading, you will learn about the two main types of categorical data encoding and when to use each type.
##### Label encoding
Which is one type data transformation technique where each data value is assigned a distinct number instead of qualitative value. This happens for each type of value in a category. Example a fruit in fruits category.
##### Some potential problems with label encoding
Label encoding may introduce unintended relationships between the categorical data in your dataset. When you are making decisions about label encoding, consider the algorithm you'll apply to the data and how it may or may not impact label encoded categorical data.
Fortunately, there is another method for categorical encoding that may help with these problems.
##### One-hot encoding
This is the creation of dummy variables in Python. A dummy variable is a variable with values of 0 or 1, which indicate the presence or absence of something. The idea is to create a column for each category type, then for each value indicate 0 or a 1 --0 meaning, no, and 1 meaning, yes.
This creation of dummies is called one-hot encoding.
With this method, we solve the problem of the unintended and problematic relationships that label encoding presented.
But one-hot encoding does present its own set of problems, particularly when it comes to linear and logistic regression. 
#### Label encoding or one-hot encoding: How to decide?
There is no simple answer to whether you should use label encoding or one-hot encoding. The decision need to be made on a case-by-case, or dataset-by-dataset basis. But there are some guidelines to help you.

Use label encoding when:
- There are a large number of different categorical variables-- because label encoding uses far less data than one-hot encoding.
- The categorical values have a particular order to them (for example, age groups can be grouped as youngest to oldest or oldest to youngest)
- You plan to use a decision tree or random forest machine learning model.

Use one-hot encoding when:
- There is relatively small amounts of categorical variables-- because one-hot encoding uses much more data than label encoding.
- The categorical variables have no particular order.
- You use a machine learning model in combination with dimensionality reduction (like Principal Component Analysis (PCA))
### Input validation.
Performing EDA on a dataset is similar to checking for the freshness of vegetables. You are searching, exploring and checking that the data is as error free as possible. You should check and recheck your datasets to make sure that they are correct. And as a data professional, you should know your datasets thoroughly. One way to make sure you know a dataset is validating it. Its one of the six practices of EDA. These are; discovering, structuring, cleaning, joining, validating, and presenting.
##### Input validation
The practice of thoroughly analyzing and double-checking to make sure data is complete, error-free, and high-quality. It is intended to be an iterative practice meaning that you should perform it again and again in between and during the other five practices of EDA.
When is it done? When starting a new analysis project or getting familiar with a new data source. 
##### Why should we take the time to validate data? And, What exactly are we looking for?
- To help make more accurate business decisions.
- We improve complex model performance.
The more careful we are in checking and rechecking our data after each manipulation of the data during EDA, the less likely we are to have problems later. 
- Clean and validated data can help prevent future system crashes, coding issues, or wrong predictions. 
So, while we are performing our validation work, what is it that we're searching for?
- No two datasets are alike. There will be different things to check for based on the type (of validation). 
Here are questions to consider/ask while validating data.
- Are all the data entries in the same format? For example, are people's ages expressed as solitary numbers like 23 and 47 or within a range like 18 to 35, and 35-50?
- Are all data entries in the same range? For instance, in finance, are some of the values expressed in thousands of euros and others in millions of euros?
- Are the applicable data entries expressed in the same data type? For example, are all the dated entries expressed in the same format of month, day, or year?
While asking these questions or performing EDA in general, you may find that the data you have been given is not sufficient to answer the business questions that you have been tasked with. The EDA practice of joining is helpful here. Joining is different from the structuring technique merging.
##### Joining
Is used when you realize that the data you have been given is not sufficient to answer the business question. It is the process of augmenting data by adding values from other datasets. 
	Augment - Verb; to increase; to make larger or supplement. To grow to increase; to increase an interval (in music).
The practice of joining will be most useful if we validate the data to ensure formatting and data entries aligned and are the same data type. You will need to use your own logic ,common sense and experiences to understand the ways in which you should join or validate each particular dataset you work on. There wont be a rigid process to show you down to the detail how to handle each data file.
Data science takes a lot of analytical thinking and shifting up perspectives to be thorough in your EDA and validation. Experience and effort will be the best way to improve your performance in analytical thinking. Keeping your validation practices in line with the PACE workflow will also help you keep a focus on ethics.
Validation should be about cleaning and correcting data for the sake of quality and correctness. The EDA practice of validating fit in the Analyze phase of PACE workflow, but is also a practice you should use for all four phases. PACE stands for Plan, Analyze, Construct, and Execute. This means that what and how we join and validate data should be in alignment with the Plan phase of the workflow. 
For example: if your task is to discover which week within a given month is most profitable for a business, it will be important that you check and recheck that you have grouped the revenue dates into weeks correctly. Your analysis won't be effective if you think you've grouped revenue by week but actually done it by day or month. 
When you look to validate your data to this level of detail, it will help you towards meeting your PACE goals, specifically for the construct and execute phases. 
Another thing that can help you with validating and joining aspects of EDA is looking to your peers and managers to help you. A peer reviewed dataset is one of the best ways to ensure that you check yourself on bias, keep ethics as a focus and ensure you are on track with the PACE workflow. Remember, validating data is like checking your vegetables to find if they're the right choice for the recipe. It may take extra time upfront but it may save you from headaches and stomachaches down the road.
### Key Takeaways
Identifying duplicate data values in a dataset is an important part of EDA (or Exploratory Data Analysis) practices, specifically cleaning and validating. After identifying duplicates, think about the impact to the dataset and your analysis objective when choosing to eliminate duplicates or not eliminate duplicates.



