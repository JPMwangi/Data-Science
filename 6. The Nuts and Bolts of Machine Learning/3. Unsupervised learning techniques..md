You'll learn about unsupervised learning. The differences between supervised and unsupervised learning. The benefits of each approach. How to apply two unsupervised machine learning models: clustering and K-means.
### Explore unsupervised learning and K-means
##### Welcome
Linear regression, logistic regression, and Naive Bayes models are supervised learning techniques that make predictions on labeled data. Most machine learning applications today are based on supervised learning. But when we consider all the available data in the world, the vast majority of it is unlabeled. Photographs, voice recordings, videos, social media posts, these are all examples of unlabeled data.
Any data that's not organized in an easily identifiable manner is known as unstructured data, or unlabeled data. So, how do we make sense of all that unlabeled data? We use unsupervised learning techniques. When our data is unlabeled, these methods make it possible for data professionals to learn about the data's underlying structure and find out how different features relate to each other. A common type of unsupervised learning is recommendation systems. They are a subclass of machine learning algorithms which offer relevant suggestions to users such as new songs for your playlist or a new coat for winter. We are going to learn other methodologies and applications of unsupervised learning. These are:
- K-means models and how it clusters data based on each observation similarity to others in the data. 
- Build a K-means model and evaluate it using metrics called inertia and silhouette score.
##### Introduction to K-means
K-means is an unsupervised partitioning algorithm. It's used to organize unlabeled data into groups or clusters. It does this by creating a logical scheme to make sense of the data. With K-means, each cluster is defined by a central point or a centroid. Its position (a centroid) represents the center of the cluster, also known as the mathematical mean. Hence the name K-means. 
There are four steps to building a K-means model:
###### Step 1: Initiate k centroids
In step one, you choose the number of centroids and place them in the data space. K represents the number of centroids in your model, which is how many clusters you'll have. This is a decision you make. Sometimes you'll have an idea about the number of clusters necessary for a project. For example, if your company manufactures five different products, you might want to set your K value to five. Other times, you won't know how many clusters your data should be split into. So try different values for k, and determine what provides the best results. 
###### Step 2: Assign all points to their nearest centroid
In step two, assign each data point to its nearest centroid. The nearest centroid is the one that's closest in space. The observations are assigned to the centroids. An observation is simply any data point being observed. 
###### Step 3: Recalculate the centroids
In step three, recalculate the centroid of each cluster. Again, the centroid location is calculated by taking the mean of all the points in its cluster. The centroid is moved to the midpoint of the observations' cluster. This will happen each iteration until the algorithm reaches convergence. This is the stable point found at the end of a sequence of solutions. 
###### Step 4: Iteration
Step four is to repeat steps 2 and 3 until the algorithm converges. The convergence happens faster when the algorithm has little data and so the model is simple. If you had a lot more data, the centroids would get increasingly closer to their related clusters. You also might find that the cluster assignment of each data point changes as the centroid locations move within each iteration.

Its important to run the model with different starting positions for the centroids (or with different centroid initializations) . This helps avoid poor clustering caused by local minima. In other words, not having an appropriate distance between clusters. Note that this clustering isn't wrong, its still a valid resolution of the model, it just doesn't make much sense in the given context. After all, the goal is to find a clustering scheme that makes sense of your data.
Finally, note that even though K-means is a partitioning algorithm, data professionals typically talk about it as a clustering algorithm. The difference is that outlying points in clustering algorithms can exist outside of the clusters. However, for partitioning algorithms, all points must be assigned to a cluster. In other words, K-means does not allow unassigned outliers.
To recap, K-means is an unsupervised learning technique that groups unlabeled data into K clusters based on similarity. The clustering process has four steps that repeat until the model converges. The value for K is a decision that the modeler makes. And, its important to build multiple models to avoid poor clustering.
##### More about K-means
We'll examine the methodology behind K-means theory by studying how the algorithm behaves when different k values are used to cluster two-dimensional data. We'll compare cases where K-means works well and those cases where it works poorly. Being aware of the strengths and limitations of this methodology will enable you to use it appropriately and effectively as a data professional.
###### Consider the data
Suppose you're presented with data for 100 samples selected from three species of horned beetles. The data compares total body mass to horn length. Because it's two-dimensional, you can plot it and examine the results to identify patterns or trends. What do you notice about the data in the scatter plot? Does it fall into groups? Can you tell that there are three species represented in this data? 
What if we color the same data by species? Hopefully you were able to discern similar clusters on your own. Notice that the points in the scatter plot are loosely grouped into three clusters. In this case, each color represents a different species of beetle. You can use K-means to cluster this data.
###### Cluster with K-means
Before continuing, it's important to note that in this illustrative case:
- You know there are three species of beetles.
- By using a scatter plot, you can verify that they are clustered into three groups.
Remember, you will not always know how many clusters you should have, and you probably won't be able to visualize your data so easily because it will likely have more than three features (i.e., dimensions). We use this example because it is effective for teaching concepts to learners.
###### Modeling: ${\text{k=3}}$
Since you know there are three clusters, you know to set k equal to three when you build your model. Recall the steps of the K-means algorithm:
1. Randomly place centroids in the data space.
2. Assign each point to its nearest centroid.
3. Update the location of each centroid to the mean position of all points assigned to it.
4. Repeat steps 2 and 3 until the model converges (i.e., all centroid locations remain unchanged with successive iterations).
Now, consider these steps as the K-means algorithm iterates over the data. 
Iteration 0: The centroids are placed at random, and the points are assigned to their nearest centroid.
Iteration 1: Update centroid locations, reassign points to their nearest centroid.
Iteration 2: Repeat.
Iteration 3: Repeat.
Iteration 4: Repeat.
Stop.
Why stop here? When you notice that there is no meaningful difference between the results of iteration 3 and iteration 4. The algorithm has converged (the centroids have stopped moving and cluster assignment has stabilized). There are points that may be assigned to the wrong clusters after just few iterations, after the centroid locations have converged. K-means is both effective and efficient in clustering this data.
Note that in the example above, the centroids may still be moving after iteration 4, but if so, all movement is below a convergence tolerance that can be set by the modeler. The default in scikit-learn is 0.0001. If all centroid locations move less than this distance, the model is declared converged.
###### Modeling: ${\text{k=2}}$
Suppose you didn't know how best to cluster your data, and you decided to set the value of k to two. Note how clustering converges in this case. Not only did it take six iterations to converge, but the data in the top cluster was split, resulting in class assignment that, in this case, you know to be incorrect (because there are three species of beetle). This is an example of what happens when you don't use the best value for k. If you were to set k to a value greater than three, the algorithm could similarly split the data into unintuitive clusters.
###### When there are no correct answers
In the example above, you know that there are three different kinds of beetles, and therefore to set k to three. The purpose of using this labeled data for an unsupervised learning demonstration was to illustrate that K-means can effectively group data. However, more often, you won't have any class labels to determine how 'good' your model is.
Suppose you have the same data points, but they represent something else--in this new case, mobile phone owners plotted by age and cellular (non-Wi-Fi) data usage. You may want to perform a market segmentation analysis on this data. In this scenario, there is no 'correct' answer. You cannot verify your model's clustering against a baseline truth. It's possible that you could make a case to use two, three, four, or more clusters. You might not have any idea how many to choose. There are tools to help you make this decision when you cannot visualize your data or don't otherwise know how many clusters you should have.
###### A note on K-means++
Earlier, you learned the importance of running K-means multiple times with different initial positions of the centroids to help avoid using a model that gets stuck in local minima. Fortunately, most machine learning packages have improved implementations of K-means that make it easier for you by removing this requirement.
In scikit-learn, this implementation is called K-means++. K-means++ still randomly initializes centroids in the data, but it does so on a probability calibration. 
Basically, it randomly chooses one point within the data to be the first centroid, then it uses other data points as centroids, selecting them pseudo-randomly. The probability that a point will be selected as a centroid increases the farther is from other centroids. This helps to ensure that centroids aren't initially placed very close together, which is when convergence in local minima is most likely to occur.
K-means++ is the default implementation when you instantiate K-means in scikit-learn. If you don't want to use this implementation and would prefer to start with truly random centroids, you can change this by setting the 'init' parameter to 'random', but rarely would you want to do this.
###### Key takeaways
When using K-means to model your data, it's important to understand how the methodology works to cluster data. If you know the most common ways by which K-means can arrive at both good and bad clustering results, it will help you understand how to choose the best value for k and ultimately to make better decisions when building models as a data professional.
##### Clustering beyond K-means
K-means is a powerful and straightforward way to group data based on its proximity to other data. However, as with all models, K-means has its limitations. Here, we'll review the strengths of K-means and also demonstrate its weaknesses. Additionally, you'll learn about two additional clustering methodologies to explore as alternatives:
- DBSCAN
- Agglomerative clustering
The purpose is not to make you an expert in clustering, but rather to give you a map of the terrain that you can use to guide you if you'd like to learn more about this branch of unsupervised learning.
###### Why use other algorithms?
K-means works by minimizing intercluster variance. In other words, it aims to minimize the distance between points and their centroids. This means that K-means works best when the clusters are round. If you aren't satisfied with the way K-means is clustering your data, there are many other clustering methods available to choose from.
###### 1. DBSCAN
DBSCAN stands for density-based spatial clustering of applications with noise. Instead of trying to minimize variance between points in each cluster, DBSCAN searches your data space for continuous regions of high density. 
Hyperparameters
The most important hyperparameters for DBSCAN in scikit-learn are:
- eps: Epsilon (ε) - The radius of your search area from any given point.
- min_samples: The number of samples in an ε-neighborhood for a point to be considered a core point (including itself)
Since DBSCAN is designed to find cluster based on density, the shape of the cluster isn't as important as it is for K-means. Here, the clusters aren't as block-like as they are for K-means, and they correspond with the vertices of the pyramid a lot more closely than K-means. And this leads to different clustering arrangements giving different patterns for each of the centroids.
###### 2. Agglomerative clustering
It works by first assigning every point to its own cluster, then progressively combining clusters based on intercluster distance. How it works. Agglomerative clustering requires that you specify a desired number of clusters or a distance threshold, which is the linkage distance above which clusters will not be merged. If you do not specify a desired number of clusters, then the distance threshold is an important parameter, because without it the model would converge into a single cluster every time.
###### Linkage
There are different ways to measure the distances that determine whether or not to merge the clusters. This is known as linkage. Here are the most common:
- Single: The minimum pairwise distance between clusters.
- Complete: The maximum pairwise distance between clusters.
- Average: The distance between each cluster's centroid and other clusters' centroids.
- Ward: This is not a distance measurement. Instead, it merges the two clusters whose merging will result in the lowest inertia.
Note that these linkages strategies aren't specific just to agglomerative clustering. You'll encounter them in many other clustering methodologies if you choose to continue learning about them, and even beyond clustering in other areas of data science.
###### When does it stop?
The agglomerative clustering algorithm will stop when one of the following conditions is met:
	1. You reach a specified number of clusters.
	2. You reach an intercluster distance threshold (clusters that are separated by more than this distance are too far from each other and will not be merged).
###### Hyperparameters
There are numerous hyperparameters available for agglomerative clustering in scikit-learn. These are some of the most important one:
- n_clusters: The number of clusters you want in your final model.
- linkage: The linkage method to use to determine which clusters to merge (as describe above).
- affinity: The metric used to calculate the distance between clusters. Default = Euclidean distance.
- distance_threshold: The distance above which clusters will not be merged (as described above).
Agglomerative clustering can be very effective. It scales reasonably well, and it can detect clusters of various shapes. Agglomerative clustering gives even more definition to the data clustered along the vertices of the pyramid, which most closely represents the clusters that appear to the eye.
###### Other clustering algorithms
The are many other ways to cluster data than what is covered here. Scikit-learn documentation provides a helpful reference that illustrates some of the strengths and weaknesses of each methodology by running them on a series of toy datasets. As always, feel free to explore some of these algorithms on your own!
###### Key takeaways
K-means is a simple yet powerful clustering algorithm, but it's not the only one. Depending on the nature of the problem you're trying to solve, other algorithms might outperform it. DBSCAN and Agglomerative clustering are two methodologies that are accessible to novices and effective, and they can cluster your data even when the clusters themselves have unusual shapes.
### Evaluate a K-means model
##### Key metrics for representing K-means clustering
You can use the K-means methodology to plot points in two-dimensional space and also three-dimensional space. In these cases, you can identify if the model is correctly assigning points to clusters. Unfortunately, most cases data professionals encounter on the job are not so easy. Your data will have more than three dimensions, so you'll not be able to visualize how each observation relates to those around it. You might not even know how many clusters there should be. 
So how do you decide the value for k? And once you do, how do you know your model is working as intended? In linear and logistic regression, you used metrics such as $R^2$, mean squared error (MSE), area under the curve (AUC), precision, and recall to evaluate the effectiveness of your model. But in unsupervised learning, you don't have any data to compare your model against. The metrics aren't applicable. In fact, your model isn't predicting anything. Instead, its grouping observations based on their similarities. Its up to you to investigate and understand the different clusters. Remember, if you have domain knowledge or the problem you're trying to solve has its own constraints, use these things to your advantage.
For example, maybe you're investigating customer segmentation for a service that offers four different subscription levels, then you'd probably want to use four as your value for k. But if you have no way of knowing in advance what value to use for k, don't worry, there are other ways to figure it out.
Before we get started with evaluation metrics, consider what makes for a good clustering model. Basically, you want clearly identifiable clusters. This means that within each cluster or intracluster, the points are close to each other. It also means that between the clusters themselves or intercluster, you want lots of empty space. 
One way to evaluate the intracluster space is to identify its inertia. Its a different concept from inertia as it's defined in physics. Here, inertia is defined as the sum of the squared distances between each observation and its nearest centroid. Essentially, this is a measurement of how closely related observations are to other observations within the same cluster. That information is then aggregated across all the clusters to produce a single score for a particular metric being measured.
Another important metric for evaluating your K-means model is the silhouette score. This is a more precise evaluation metric than inertia because it also takes into account the separation between clusters. Silhouette score is defined as the mean of the silhouette coefficients of all the observations in the model. The silhouette score helps evaluate your model. It provides insight as to what the optimal value for k should be, and uses both intracluster and intercluster measurements in its calculation. 
##### Inertia and silhouette coefficient metrics
These are indispensable tools for data professionals who work with k-means models and any other model in the clustering family. What makes a good model? Ideally, you'd have tight clusters of closely-related observations. Each cluster is well separated from other clusters. 
Remember that inertia is the sum of the squared distances between each observation and its closest centroid. Its a measurement of intra-cluster distance. So it gauges how closely related each observation is to the other observations in its own cluster. 
The more compact the clusters, the lower the inertia, because there's less distance between each observation and its nearest centroid. Therefore, it's important for inertia to be as close to zero as possible. Can inertia ever be zero? Its possible, but this scenario wouldn't offer any new insight into the data. Here's why:
- In one case, if all the observations were identical, this would mean all data points are in the same location, then inertia equals zero for all values of k.
- The second case is when the number of clusters is equal to the number of observations. If each observation is in its own cluster, then its centroid is itself.
Inertia is a great metric because it helps us to decide on the optimal k value. We do this by using the elbow method. In the elbow method, we first build models with different values of k. Then we plot the inertia for each k value. Here's an example:
	![[elbow method.png|400]]
Notice that the greater the value for k, the lower the inertia. Should you always select high k values? Well, no. A low inertia is great, but if it results in meaningless or inexplicable clusters, it doesn't help you at all. A good way of choosing an optimal k value is to find the elbow of the curve (above image). This is the value of k at which the decrease in inertia starts to level off. In this example (above image), that occurs when we use three clusters. Sometimes it might be difficult to choose between two consecutive values of k. In that case, it's up to you to determine which is the best for your particular project.
The second important metric for evaluating k-means model is the silhouette score. This is a more precise evaluation metric than inertia because it takes into account the separation between clusters. Silhouette score is defined as the mean of the silhouette coefficients of all the observations in the model. Each observation has its own silhouette coefficient. The silhouette coefficient can be anywhere between negative 1 and 1: 
- If an observation has a silhouette coefficient close to one, it means that it's both nicely within its own cluster and well separated from other clusters. 
- A value of zero indicates that the observation is on the boundary between clusters.
- If your observation has a silhouette coefficient close to negative 1, it may be in the wrong cluster.
As you've just experienced, when using silhouette score to help determine how many clusters your model should have, you'll generally want to opt for the k value that maximizes your silhouette score. Inertia and silhouette score are important metrics to help you determine the most appropriate number of clusters for your k-means model.
##### More about inertia and silhouette coefficient metrics
The evaluation metrics that are used for supervised learning models don't apply to unsupervised learning models. This is because unsupervised learning model results cannot be categorized as 'correct' or 'incorrect'. While supervised learning models use predictor variables to predict a defined target variable, unsupervised learning methods have metrics that seek an underlying structure within the data.
Clustering models are a type of unsupervised learning that do this by grouping observations together. Data professionals often use inertia and silhouette score to evaluate their clustering models and help them determine which groupings make sense. This reading reviews these concepts and examines them in greater detail.
###### Inertia
Inertia is a measurement of intracluster distance. It indicates how compact the clusters are in a model. Specifically, inertia is the sum of the squared distance between each point and the centroid of the cluster that its assigned to. It can be represented by this formula, where:
- $n$ = the number of observations in the data,
- $X_i$ = the location of a particular observation,
- $C_k$ = the location of the centroid of cluster $k$ , which is the cluster to which point $X_i$ is assigned.
Therefore, inertia is equal to the sum of ${{(X_i - C_k)^2}}$.
The greater the inertia, the greater the distances between points and their centroids, which means the points within each cluster are farther apart from each other. The clusters are also less compactly positioned around their respective centroids. Note, however, that inertia only measures intracluster distance.
For the same dataset and the same number of clusters, lower inertia values are typically better than higher values, because low values indicate that points are closer together within their clusters. Why is it more meaningful for points in a dataset to be close together within their clusters? Well, notice that you're trying to make sense of data by identifying observations that are related to each other somehow. 
Clustering models do this by grouping points together based on their similarity. For some techniques, 'similarity' is about the distance between points themselves (and so points that are close to each other belong to the same cluster). With others, it's about the distance between points and a cluster center (such that points that are close to the same center belong to the same cluster)--like K-means.
In both cases, observations that are closer together are assumed to be more similar to each other.  In other words, a tighter cluster could be indicative of greater similarity between the real-world observations that are represented by those data points.
###### Evaluating inertia
Inertia is a useful metric to determine how well your clustering model identifies meaningful patterns in the data. But it's generally not very useful by itself. If your model has an inertia of 53.25, is that good? It depends. The measurement becomes meaningful when it's compared to the inertia values and $k$ values of other models on the same data. As you increase the number of clusters ($k$), the inertia value will drop, but there comes a point where adding more clusters will have only small changes in inertia. And it's this transition that we need to detect.
###### The elbow method
The elbow method is a great way to find this transition. It's a way to help decide which clustering gives the meaningful model of your data. It uses a line to plot visually compare the inertias of different models. With K-means models, this is done as a comparison between different values of $k$.
To use the elbow method to evaluate a model, you want to find the part of the curve that looks like an elbow--the sharpest bend in the curve. That's usually the model that will give you the most meaningful clustering of your data, because if inertia is dropping significantly with added clusters, it means distance between points and their centroids is shortening significantly. This implies denser clusters and thus more similarity between points in a given cluster. But when the drop in inertia is very minor, the distance between points and their centroids isn't changing much, while your model is becoming more complex due to the additional cluster. Adding clusters is not capturing real structure in the data. A model with $k$ = 1,000 will have low inertia, but is it any good? What meaning can you take away from 1,000 clusters?
Remember, you want inertia to be low, but if you add more and more clusters with only minimal improvement to inertia, you're only adding complexity without capturing real structure in the data.
###### There's not always an obvious elbow
Often the elbow curve wont be so obvious for there's not necessarily a 'correct' answer. It might be worth doing some analysis on the cluster assignments of both models to check for yourself which is more meaningful. There are other tools at your disposal to help you decide.
##### Silhouette analysis
One of these tools is a silhouette analysis. A silhouette analysis is the comparison of different models' silhouette scores. To calculate a model's silhouette score, first, a silhouette coefficient is calculated for each instance in the data. An instance's silhouette coefficient is defined by the following formula, where:
- ${\text{a}}$ = the mean distance between the instance and each other instance in the same cluster
- ${\text{b}}$ = the mean distance from the instance in the nearest other cluster (i.e., excluding the cluster that the instance is assigned to)
- ${\text{max(a, b)}}$ = whichever value is greater, ${\text{a}}$ or ${\text{b}}$.
The silhouette coefficient equals:
$${\text{silhouette score}} = \frac {\text{(b - a)}}{\text{max(a, b)}}$$
A silhouette score can range between - 1 and +1. A value closer to +1 means that a point is close to other points in its own cluster and well separated from points in other clusters (a higher density).  A value closer to zero means that a point is between clusters.  And a value closer to -1 means that a point is probably assigned to the wrong cluster, because it is closer to the points of another cluster than to the points in its own.
The silhouette score is the mean silhouette coefficient over all the observations in a model. The greater the silhouette score, the better defined the model clusters, because the points in a given cluster are closer to each other, and the clusters themselves are more separated from each other.
Note that, unlike inertia, silhouette coefficients contain information about both intracluster distance (captured by the variable $a$) and intercluster distance (captured by the variable $b$). As with inertia values, you can plot silhouette scores for different models to compare them against each other.
Key takeaways
Inertia and silhouette score are useful metrics to help determine how meaningful your model's cluster assignments are. Both are especially helpful when your data has too many dimensions (features) to visualize in 2-D or 3-D space. Use these metrics together to help inform your decision on which model to select.
Inertia:
 - Measures intracluster distance
 - Equal to the sum of the squared distance between each point and the centroid of the cluster that its assigned to
 - Used in elbow plot
 - All else equal, lower values are generally better
 Silhouette score:
 - Measure both intercluster distance and intracluster distance
 - Equal to the average of all points' silhouette coefficients
 - Can be between -1 and +1 (greater values better)
 