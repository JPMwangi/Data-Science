You'll learn how data professionals use a structured workflow for machine learning. You'll identify the main steps of the workflow and the importance of each step in the overall process. Then, you'll learn how to apply specific machine learning models to business problems.
### PACE in machine learning: The plan and analyze stages
##### Welcome to module 2
We'll apply the knowledge and skills you've been developing to build machine learning models from start to finish. A lot goes goes in the process of model building. Data professionals apply many different techniques when working to achieve a business goal. And they have tons of opportunities to keep learning and improving upon what they create.
As human beings, we are inherently imperfect and so are the models and the datasets we build. But that doesn't mean that they are not useful, it just means we need to be aware of their limitations. In fact, expecting an imperfection can be an advantage. There are also many tools to help us better understand data limitations and even turn them into effective data solutions. This is a valuable proficiency for you as you move forward into the data career space.
##### PACE in machine learning
We are going to focus on the PACE workflow for building effective models. PACE consists of four stages, Plan, Analyze, Construct and Execute stages. PACE helps guide the steps data professionals take as we:
- Align our data and models with the business needs. 
- Gain a more comprehensive understanding of your data.
- Practice feature engineering and techniques to manage class imbalances.
The content of this module
- Practice supervised machine learning model using Python called Naive Bayes.
- The goal will be to model bank customer churn rate to predict whether a customer will close their bank account. In this context, churn is the rate at which customers stop doing business with a company over a given period of time.
- Create different model iterations for continuous improvements.
- Use performance metrics to evaluate how successfully the model addressed the business needs.
Using the PACE workflow not only provides a good baseline for approaching the problem but also helps data professionals stay focused throughout the process. By applying and referencing this framework, you'll have the skills to manage data driven problems in your career.
##### Plan for a machine learning project
The Plan stage of the PACE workflow has two parts: 
###### Centering the business need,
When using PACE, it's essential to align your plan with the business and data teams. For machine learning, this means ensuring that the machine learning model you plan to construct meets the actual business needs. This may seem a bit obvious, but given the potential complexity of the problem, multiple departments will likely be involved in the output. You'll also have to consider the data that is available to you before getting into the rest of the process of building the model.
Since you'll most probably be building models for a company or an organization, you'll need to ensure that throughout the other stages of PACE, your data, modelling, metrics and optimizing strategies stay focused on what you developed during the Plan stage.
###### Considering the most appropriate machine learning model.
The second part of the Plan stage of your PACE workflow is the use of the context and requirement of the business need to consider what type of machine learning model would be best suited for the problem you are solving. Based on what you know about machine learning, you need to decide what type of machine learning model you'll build and what desired result or prediction you need from it. When you have your plan for the business problem and scope for the machine learning model you're ready for the next step, Analyze.
##### More about planning a machine learning project
The PACE workflow is something that can be used to keep the most experienced data professionals on track in their projects. Here, you'll learn more about the Plan stage of PACE and the things that must be considered and determined to ensure a smooth and successful model development process.
###### The Plan Stage
The PACE workflow is something that you can use to keep you on track, no matter the project you're working on. Each step is important to get your final product. However, like many things, the most important part is setting up the foundations of your project- The Plan Stage.
The Plan Stage is part of the process where you first start thinking about what the problem actually is, and what needs to be done to find a solution. You start to consider what tools you have available to you, and how you'll need to manipulate the dataset. Sometimes, this can be as straightforward as needing to create some visualizations for the data. Or, it can get as complex as needing to make a predictive model using the dataset.
The plan that you create during this stage will be carried through the whole process, so it is important to really make sure you've considered all the aspects and constraints of the project. However, that isn't to say that the plan you create must stay unchanging, you can absolutely reassess as you progress. It is there to get you started heading in the right direction.
###### What should your Plan Include?
This section focuses on the machine learning algorithms. However, you need to think about whether you need a model in the first place! Many analytical tasks do not require the creation of a model, and you could spend time creating something that is not necessary to what you're trying to achieve.
###### Knowing What You Need For a Problem
The first thing to do when forming your plan is to consider the end goal. What exactly are you trying to model, and what types of results from the model are needed? Something that can be determined immediately is what type of machine learning model you'll need. The two types are Supervised and Unsupervised models.
Supervised models are used to make predictions about unseen events. These types of models use labeled data, and the model will use these labels and their predictor variables present to learn from the dataset. And when given new data points, they're able to make a prediction of the label. So, for example, if you're tasked with predicting rainfall amounts, you already know that you need a supervised learning model.
Unsupervised models, on the other hand, don't really make predictions. They are used to discover the natural structure of the data, finding relationships within unlabeled data. So, for example, if you're tasked with discovering relationships between customer habits and segment users, you know you'll need an unsupervised model.
Now, let's go back to the rainfall example. Just from that problem statement alone, we know we need a supervised learning model. However, not all supervised machine learning models are the same. The two main types of supervised learning are Regression and Classification. There are different types of regression models with different models able to perform regression or classification tasks.
Linear regression models are used when the result must be a continuous variable. As you have learned, continuous variables are numerical values that can have an unlimited number of values between the highest and lowest points of measurement. So if you need rainfall amounts in inches or centimeters, you know a linear regression model is needed.
However, what if we don't need exactly rainfall amount predictions, but just whether or not it will rain that day? This is where a classification model, such as a logistic regression model, would be more appropriate. Classification models will deliver results as a categorical variable, where there is a finite set of values that the variable can be. In this example, the model would only ever predict two results: Will Rain or Won't Rain.
###### Figuring out the tools you need
After determining what type of model you're going to need, you must consider what you have at your disposal to complete the project. Most importantly, you need to figure out if you have the data you'll need to build the model.
If your dataset only has one or two predictor variables, it probably will not produce a model that will be useful. Or, if it has very few data points, the model's performance will similarly suffer. On the other hand, your dataset might be large and unwieldy, meaning that you'll either need to clean it up or cut it down to get it into a format that you can use to train the model. Having these issues means that you'll have to put in a little extra work to get it to a useable form, or look elsewhere for data that will be helpful to create the model.
Key Takeaways
- The PACE workflow for machine learning is very useful for planning out and solving data driven problems.
- The Plan stage of PACE is the most important, setting you up for success throughout the rest of the process.
- In the Plan stage, you first consider the problem at hand and what will be needed to solve it.
- You also verify that you have the tools and resources you need to solve the problem.
- The Plan is not set in stone. It just serves as a foundational starting point for the rest of the project.
##### Overcome challenges and learn from your mistakes
Data scientists figure out opportunities for optimization in improving processes when solving problems. Everyday is a new challenge for tomorrow's problem is different from today's problem. Whenever data scientists are running experiments example: process change, or launching a new feature, they run A/B tests to figure out if the change is truly helpful or if its working. What is the expected impact they're going to get from these changes?
This is part of the learning process to understanding the impact of change on processes. Having learning mentality will help you stop repeating mistakes through trying and doing.
##### Analyze data for a machine learning model
After the Plan stage, its time for the Analyze stage of PACE. Its important to keep the business need throughout the process of developing a machine learning model. The business need informs a data professional on what the model needs to produce. The results indicates what type of model is required. The business need also informs data professionals about the data that's necessary to train the model to achieve the desired result.
The main focus of the Analyze stage is to develop a deeper understanding of the data, while keeping in mind what the model needs to eventually predict. For example, if you're creating a supervised learning model, the first thing you'll need to know is what your model is trying to predict. In other words, you'll need to understand your response variable. This helps you decide what type of supervised learning model to use, categorical or continuous.
Let's use weather as an example. If you need to predict the exact amount of precipitation in inches, you'll probably go with a continuous model. But if the business needs a model that can predict whether it will be rainy, cloudy, or sunny, that requires a categorical model. 
Often, as a data professional, your data isn't structured exactly the way you need it to be. In this case, you can use the exploratory data analysis or EDA principles to develop an understanding of what data you have available and how its structured. In the example about predicting rainfall in inches; In your dataset, the rainfall amounts that were recorded might not be in exact units needed, or the data could be split between rain, snow, and other types of precipitation.
These are all the details a data professional needs to analyze before building the model itself. For the categorical model example, where you're predicting whether it's rainy, cloudy, or sunny, your dataset might not be labeled with the categories you need. The individual days in the dataset might be labeled with only cloud cover metrics, which is something you'll have to change to be able to analyze the results effectively. 
After getting a solid understanding of what your response variables are and how they're structured, the next step is exploring your predictor variables. Understanding the relationships that exist between variables in your dataset is essential to building a model that will produce valuable results. Similar to the response variables, the predictor variables might not be in the format or style that you want. In those cases, the same consideration apply. You'll need to figure out how you want your data structured before building your model. 
The process of carefully considering the variables you have and what you need is feature engineering. Its important to know about various techniques of feature engineering that are available, which scenarios to use them in, and what they can do for your data.
##### Introduction to feature engineering
The main focus of the Analyze stage is to develop a deeper understanding of the data. Carefully considering the variables you have and what you need leads into the next part of the Analyze stage: feature engineering. Feature engineering techniques solve the problems in how your data is structured, and, if done well, can improve your model's performance. Here you'll learn what it is, how it works, and when to use it.
Feature engineering is the process of using practical, statistical, and data science knowledge, to select, transform, or extract characteristics, properties, and attributes, from raw data. First, the process of feature engineering is highly dependent on the type of data you're working with. 
The type of features, or variables are continuous and categorical. Continuous variables are variables with values obtained by measurement. As a result, they can take on an infinite and uncountable set of values. Categorical variables, on the other hand, are variables that contain a finite number of groups, categories, or countable numerical values.
The process of feature engineering will be changing and altering these variables with the end goal of using them to train a model. This can often be a challenging process. Datasets used in the workplace can sometimes require multiple rounds of EDA and feature engineering to get everything in a suitable format to train the model. This process highlights one reason why the PACE framework is so beneficial to data professionals.
The Analyze stage builds directly from the Plan stage, or put simply, the plan informs how you analyze. Without a strategically aligned business and technical plan, the Analyze stage, and feature engineering process would be like trying to build a skyscraper without having blueprints. The three general categories of feature engineering are:
- feature selection
- feature transformation
- feature extraction
Let's learn more about these categories.
###### Feature selection
The goal of this type of feature engineering is to select the features in the data that contribute the most to predicting your response variable. In other words, you drop features that do not help in making a prediction. This can either be done manually or algorithmically. 
###### Feature transformation
Here, data professionals take the raw data in the dataset and create features that are suitable for modelling. This process is done by modifying the existing features in a way that improves accuracy when training the model.
###### Feature extraction
This type of feature engineering involves taking multiple features to create a new one that would improve the accuracy of the algorithm. 
The feature engineering techniques are designed to improve your model's performance. While you can make a lot of improvements by tweaking and optimizing your model, the most sizeable performance increases often come from developing your variables into a format that will best work for the model. This is one way data professionals better understand their data in the Analyze stage. During EDA, you're just beginning to develop an understanding of your data. Feature engineering is a step beyond that. You're selecting, extracting, or transforming variables, or features, from datasets, for the construction of machine learning models.
### Explore feature engineering
You will learn more about what happens in the Analyze stage of PACE--namely, feature engineering. The meaning of the term 'feature engineering' can vary broadly, but here it includes feature selection, feature transformation, and feature extraction. You will come to understand more about the considerations and process of adjusting your predictor variables to improve model performance.
###### Feature Engineering
When building machine learning models, your model is only ever going to be as good as your data. Sometimes, the data you will have will not be predictive of your target variable. For example, it's unlikely that you can build a good model that predicts rainfall if you train it on historical stock market data. In this case, it might seem obvious, but when you're building a model, you'll often have features that plausibly could be predictive of your target, but in fact are not. Other times, your model's features might contain a predictive signal for your model, but this signal can be strengthened if you manipulate the feature in a way that makes it more detectable by the model.
Feature engineering is the process of using practical, statistical, and data science knowledge to select, transform, or extract characteristics, properties, and attributes from raw data. Let's learn more about these processes, when and why to use them, and what good feature engineering can do for your model.
##### Feature Selection
Is the process of picking variables from a dataset that will be used as predictor variables for your model. With very large datasets, there are dozens if not hundreds of features for each observation in the data. Using all of the features in a dataset often doesn't give any performance boost. In fact, it may actually hurt performance by adding complexity and noise in the model. Therefore, choosing the features to use for the model is an important part of the model development process.
Generally, there are three types of features:
- Predictive: Features that by themselves contain information useful to predict the target.
- Interactive: Features that are not useful by themselves to predict the target variable, but become predictive in conjunction with other features. 
- Irrelevant: Features that don't contain any useful information to predict the target.
You want predictive features, but a predictive feature can also be a redundant feature. Redundant features are highly correlated with other features and therefore do not provide the model with any new information--for example, the steps you took in a day, may be highly correlated with calories you burned. The goal of feature selection is to find the predictive and interactive features and exclude redundant and irrelevant features.
The feature selection process typically occurs at multiple stages of the PACE workflow. The first place it occurs is during the Plan phase. Once you have defined your problem and decided on a target variable to predict, you need to find features. Keep in mind that datasets are not always prepackaged in a nice little tables ready to model. Data professionals can spend days, weeks, or even months acquiring and assembling features from many different sources.
Feature selection can happen once more during the Analyze phase. Once you do an exploratory data analysis or EDA, it might become clear that some of the features you included are not suitable for modelling. This could be for a number of reasons. For example, you might find that a feature has too many missing or clearly erroneous values, or perhaps its highly correlated with other features that must be dropped so as not to violate the assumptions of the model. It's also common that the feature is some kind of metadata, such as ID number with no inherent meaning. Whatever the case may be, you might want to drop these features.
During the Construct phase, when you are building models, the process of improving your model might include more feature selection. At this point, the objective usually is to find the smallest set of predictive features that still results in good overall model performance. In fact, data professionals will often base final model selection not solely on score, but also on model simplicity and explainability. A model with an R2 of 0.92 and 10 features might get selected over a model with an R2 of 0.94 and 60 features. Models with fewer features are simpler, and simpler models are generally more stable and easier to understand.
When data professionals perform feature selection during the Construct phase, they often use statistical methodologies to determine which features to keep and which to drop. It could be as simple as ranking the model's feature importance's and keeping only the top a% of them. 
Another way of doing it is to keep the top features that account for >= b% of the model's predictive signal. There are many different ways of performing feature selection, but they all seek to keep the predictive features and exclude the non-predictive features.
##### Feature Transformation
This is a process where you take features that already exist in the dataset, and alter them so that they're better suited to be used for training the model. Data professionals usually perform feature transformation during the Construct phase, after they've analyzed the data and made decisions about how to transform it based on what they've learned. 
###### Log normalization
There are various types of transformations that might be required for any given model. For example, some models do not handle continuous variables with skewed distributions very well. As a solution, you can take the log of a skewed feature, reducing the skew and making the data better for modelling. This is known as log normalization.
For instance, suppose you had a feature X1 whose histogram has a distribution that skews right. This is known as a log-normal distribution. A log-normal distribution is a continuous distribution whose logarithm is normally distributed. In this case, the distribution skews right, but if you transform the feature by taking its natural log, it normalizes the distribution.
Normalizing a feature's distribution is often better for training a model, and you can later verify whether or not taking the log has helped by analyzing the model's performance.
###### Scaling
Another kind of feature transformation is scaling. Scaling is when you adjust the range of a feature's values by applying a normalization function to them. Scaling helps prevent features with very large values from having an undue influence over a model compared to features with smaller values, but which may be equally important as predictors.
There are many scaling methodologies available. Some of the most common include:
###### Normalization
e.g. MinMaxScaler in scikit-learn, transforms data to reassign each value to fall within the range[0,1]. When applied to a feature, the feature's minimum value becomes zero and its maximum becomes one. All other values scale to somewhere between them. The formula for this transformation is:
$$X_{i,normalized} = \frac{{X_i - X_{min}}}{{X_{max} - X_{min}}}$$
For example, suppose you have feature 1, whose values range from 36 to 209; and feature 2, whose values range from 72 to 978. Its apparent that these features are on different scales from one another. Features with higher magnitudes of scale will be more influential in some machine learning algorithms, like K-means, where Euclidean distances between data points are calculated with the absolute value of the features (so large feature values have major effects, compared to small feature values). By min-max scaling (normalizing) each feature, they are both reduced to the same range.
###### Standardization
Another type of scaling is called standardization (e.g. StandardScaler in scikit-learn). Standardization transforms each value within a feature so they collectively have a mean of zero and a standard deviation of one. To do this, for each value, subtract the mean of the feature and divide by the feature's standard deviation.
$$ {{X_{i,standardized}}} =\frac {{X_i - X_{mean}}}{{X_{stand.dev.}}}$$
The method is useful because it centers the feature's values on zero, which is useful for some machine learning algorithms. It also preserves outliers, since it does not place a hard cap on the range of possible values. After applying standardization, the data points are distributed in a way that is very similar to the result of normalizing, but the values and scales are different.
###### Encoding
Another form of feature transformation is known as encoding. Variable encoding is the process of converting categorical data to numerical data. Consider the bank churn dataset. The original data has a feature called 'Geography', whose values represent each customer's country of residence--France, Germany, or Spain. Most machine learning methodologies cannot extract meaning from strings. Encoding transforms the strings to numbers that can be interpreted mathematically. 
The 'Geography' column contains nominal values, or values that don't have an inherent order or ranking. As such, the feature would typically be encoded into binary. This process requires that a column be added to represent each possible class contained within the feature. 
Tools commonly used to do this include:
```Python
# Tools commonly used to do encoding include:
pandas.get_dummies()
OneHotEncoder()
```
Often methods drop one of the columns to avoid having redundant information in the dataset. Note that information isn't lost by doing this.
Keep in mind that some features may be inferred to be numerical by Python or other frameworks but still represent a category. For example, suppose you had a dataset with people assigned to different arbitrary groups: 1,2, and 3. The 'Group' column might be encoded as type `int`, but the number is really only representative of a category. Group 3 isn't two units 'greater than' Group 1. The groups could just as easily be labeled with colors. In this case, you could first convert the column into a string, and then encode the strings as binary columns. This is a problem that can be solved upstream at the stage of data generation: categorical features (like a group) should not be recorded using a number.
A different kind of encoding can be used for features that contain discrete or ordinal values. This is called ordinal encoding. It is used when the values do contain inherent order or ranking. For instance, consider a 'Temperature' column that has values of cold, warm, and hot. In this case, ordinal encoding could reassign these classes to 0, 1, and 2. This method retains the order or ranking of the classes relative to one another.
##### Feature extraction
Feature extraction involves producing new features from existing ones, with the goal of having features that deliver more predictive power to your model. While there is an overlap between extraction and transformation colloquially, the main difference is that a new feature is created from one or more other features rather than simply changing one that already exists.
Consider a feature called 'Date of Last Purchase', which contains information about when a customer last purchased something from the company. Instead of giving the model raw dates, a new feature can be extracted called 'Days Since Last Purchase'. This could tell the model how long it has been since a customer has bought something from the company, giving insight into the likelihood that they'll buy something again in the future.
Features can also be extracted from multiple variables. For example, consider modelling if a customer will return to buy something else. In the data, there are two variables: 'Days Since Last Purchase', and 'Price of last purchase'. A new variable could be created from these by dividing the price by the number of days since the last purchase, creating a new variable altogether.
Sometimes, the features that you are able to generate through extraction can offer the greatest performance boost to your model. It can be a trial and error process, but finding good features from the data is what will make a model stand out in industry.
###### Key Takeaways
- Analyzing the features in a dataset is essential to creating a model that will produce valuable results.
- Feature Selection is the process of dropping any and all unnecessary or unwanted features from the dataset.
- Feature Transformation is the process of editing features into a form where they're better for training the model.
- Feature Extraction is the process of creating brand new features from other features that already exist in the dataset.
##### Solve issues that come with imbalanced datasets
We'll continue our exploration of the Analyze stage of PACE. Understanding what your variables are and how they're structured is only part of the process. It's also essential to understand the frequency in which the variables exist.
For classification problems, you need to specifically understand the frequencies of the response variable. As a data professional, you might encounter datasets that are unequal in terms of their response variables. One example of unequal datasets is in the context of fraud detection. You could have millions of examples of non-fraudulent transactions and only a few thousand examples of actual fraudulent transactions. How can a model be built to detect fraud with such limited data to train the model? This issue is known as class imbalance.
A class imbalance  is when a dataset has a predictor variable that contains more instances of one outcome than another. In the fraud detection example above, the dataset has more instances of non-fraudulent transactions than those of the fraudulent transactions. The class with more instances is called the majority class, while the class with fewer instances is called the minority class.
It's extremely rare for a dataset to have a perfect 50-50 split of the outcomes. There is normally some degree of imbalance. However, this isn't necessarily a problem, believe it or not, a 70-30 or 80-20 split can be fine. Major issues only arise when the majority class makes up 90% or more of the dataset. You'll know if there's an imbalance issue after the model is built. 
There are two techniques that allow us to fix any potential issues, upsampling and downsampling. Both of them involves altering the data in a way that preserves the information contained in the data while removing the imbalance. 
Downsampling involves altering the majority class by using less of the original dataset to produce a split that's more even. The number of entries of the majority class decreases, leading to more of a balance. You can use different techniques to achieve this, but generally, they're all based on this concept. One technique is to do this randomly by selecting entries to remove, or you can follow a formula. For example, you can take the mean of two data points in the majority class, remove those data points and add the average data point.
Upsampling is the opposite of downsampling. Instead of reducing the frequency of the majority class, you artificially increase the frequency of the minority class. Similar to downsampling, there are multiple ways you can achieve this. The simplest technique is called random oversampling, where random copies of data points in the minority class are copied and added back to the dataset. Or mathematical techniques can be used to generate non-identical copies, which are then also added to the dataset. 
So if upsampling and downsampling achieve the same result, you might be wondering which on to use. Most of the time, you won't know which one is preferred until you've built the model and observed how it performs. However, there are some general rules you can follow regarding when to upsample and when to downsample.
Downsampling is normally more effective when working with extremely large datasets. If you have a dataset that has 100 million points but has a class imbalance, you don't need all of that data to build a good model. You definitely don't need additional data that would come from upsampling. Alternatively, upsampling can be better when working with a small dataset. If you're working with a dataset that has 10,000 entries, removing any of that data will more than likely have a negative impact on the model's performance.
Keep in mind that class balancing is a fickle process and may require some trial and error. Building models with both upsample data and downsample data will determine which technique is better in any given situation. Additionally, you'll have to experiment with what sort of split your rebalancing achieves. Balancing the data so the classes are split 50-50, might not always be optimal. On the other hand, turning a 99-1 split into a 70-30 split might be fine. And that is something to consider as you develop your model.
##### More about imbalanced datasets
Here, we'll explore the idea of class imbalance in datasets. Understanding what it is, when it becomes problematic, and some issues that can arise if it isn't addressed. We'll also learn two of the general categories for balancing datasets, upsampling, and downsampling. And also understand when to use each, and what about a situation implies that one should be used over the other.
###### Imbalanced Datasets
As a data professional, you'll notice that many raw datasets that you will encounter require varying levels of work to get them in place where they are good for modelling. There might be missing values, or the variables might not be in the exact format that you need. You perform an exploratory data analysis or EDA to get a good understanding of the data. When working with classification models, however, there is something else to consider before moving forward: class balance.
For categorical variables, the different possible values that each can take are known as classes. This is true for both predictor variables and target variables. If you were trying to classify the weather on a given day as rainy or sunny, these would be considered two classes. The number of classes is equal to the number of unique values in the variable.
The number of occurrences of each class in the target variable is known as the class distribution. When predicting a categorical target, problems can arise when the class distribution is highly imbalanced. If there are not enough instances of certain outcomes, the resulting model might not be very good at predicting that class.
This is where the process of class balancing comes in, a process that allows you to manipulate the dataset, or the model fitting process, in a way that the class imbalance that exists doesn't affect the performance of the resulting model. In this reading, you will explore more about imbalanced datasets, the problems that can occur when working with them, and some methods of adjusting the class distribution to minimize the imbalance.
###### Situations of Imbalance
Classification is a very broad field, with many applications across different industries. Some business needs, however, require a model to be good at classifying things that occur relatively rarely in the data. These types of problems have several names that are used commonly, including rare event predictions, extreme event prediction, and severe class imbalance. However, they all refer to the same thing: at least one of the classes in the target variable occurs much less frequently than another.
Consider this example. You are tasked with creating a model that will classify emails that are sent to the company either as 'spam' or 'not-spam'. The company receives many thousands of emails daily, not to mention all the emails they've received in the past. However, the number of examples of spam is very small. For the sake of this example, say that 10 emails per day are identified as spam manually.
All the emails are collected into a dataset to train the model, with each example labeled as spam or not-spam. The problem is that the dataset contain many, many more examples of not-spam than spam. In this case, spam is known as the minority class and not-spam is known as the majority class.
This doesn't have the makings of a good model. With so few examples of spam relative to examples of not-spam, the model can have difficulty detecting the minority class, resulting in its being biased toward the majority class or possibly never predicting the minority class at all.
###### Balancing a Dataset
Class balancing refers to the process of changing the data by altering the number of samples in order to make the ratios of classes in the target variable less asymmetrical. It is a large field of study on its own, and there are several methods that allow you to balance the classes while maintaining the integrity of the data. Here, you'll learn about some of the most common methods that can be used to create a better model.
There are two general strategies to balance a dataset, and the method that is better to use generally is decided by how much data you have in the first place.
###### Downsampling
Is the process of making the minority class represent a larger share of the whole dataset simply by removing observations from the majority class. It is mostly used with datasets that are large. But how large is large enough to consider downsampling? Tens of thousands is a good rule of thumb, but ultimately this needs to be validated by checking that model performance doesn't deteriorate as you train with less data.
One way to downsample data is by selecting some observations randomly from the majority class and removing them from the dataset. There are some more technical, mathematically based methods, but random removal works very well in most cases.
###### Upsampling
Is basically the opposite of downsampling, and is done when the dataset doesn't have a very large number of observations in the first place. Instead of removing the observations from the majority class, you increase the number of observations in the minority class. 
There are a couple of ways to go about this. The first and easiest method is to duplicate samples of the minority class. Depending on how many such observations you have compared to the majority class, you might have to duplicate each sample several times over.
Another way is to create synthetic, unique observations of the minority class. On the surface, there seems to be something wrong about editing the dataset like this, but if the goal is simply to train a better-performing model, it can be a valid and useful technique. You can generate these synthetic observations from the observations that currently exist.
For example, you can average two points of the minority class and add the result to the dataset as a sample of the minority class. This can even be done algorithmically using publicly available Python packages.
###### How to do it
In both cases, upsampling and downsampling, it is important to leave a partition of test data that is unaltered by the sampling adjustment. You do this because you need to understand how well your model predicts on the actual class distribution observed in the world that your data represents. 
In the case of the spam detector example, it's great if your model can score well on resampled data that is 80% not-spam and 20% spam, but you need to know how it will work when deployed in the real world, where spam emails are much less frequent. This is why the test holdout data is not rebalanced.
###### Consequences
Manipulating the class distribution of your data doesn't come without consequences. The first consequence is the risk of your model predicting the minority class more than it should. By class rebalancing to get your model to recognize the minority class, you might build a model that over-recognizes that class. That happens because, in training, it learned a data distribution that is not what it will be in the real world.
Changing the class distribution affects the underlying class probabilities learned by the model. Consider, for example, how the Naive Bayes algorithm works. To calculate the probability of a class, given the features, it uses the background probability of a class in the data. Here's the formula:
$${\text{P(A|B)}} = \frac {\text{P(B|A) * P(A)}} {\text{P(B)}}$$ 
In the numerator, $P(A)$ (the probability of class A) depends on class probabilities encountered in the data. If the data has been enriched with a particular class, then this probability will not be reflective of meaningful real-life patterns, because it's based on altered data.
###### When to do it
Class rebalancing should be reserved for situations where other alternatives have been exhausted and you still are not achieving satisfactory model results. Some guiding questions include:
- How severe is the imbalance? A moderate (<20%) imbalance may not require any rebalancing. An extreme imbalance (<1%) would be a more likely candidate.
- Have you already tried training a model using the true distribution? If the model doesn't fit well due to very few samples in the minority class, then it could be worth rebalancing, but you won't know unless you first try without rebalancing.
- Do you need to use the model's predicted class probabilities in a downstream process? If all you need is a class assignment, class rebalancing can be a very useful tool, but if you need to use your model's output class probabilities in another downstream model or decision, then rebalancing can be a problem because it changes the underlying probabilities in the source data.
Key Takeaways
- Imbalanced datasets can be a problem when working on classification problems.
- Class imbalance isn't always a problem. The likelihood and severity of it negatively affecting model performance generally increases as the degree of the imbalance increases.
- Downsampling involves removing some observations from the majority class, making it so they make up a smaller percent of the dataset than before.
- Upsampling involves taking observations from the minority class and either adding copies of those observations to the dataset or generating new observations to add to the dataset.
### PACE in machine learning: The Construct and Execute stages
##### Introduction to Naive Bayes
You are now halfway through the PACE workflow. During the Planning stage, you develop a better understanding of the business need and the data available. In the Analyze stage, you investigate the data using exploratory data analysis or EDA. You apply feature engineering techniques to select, transform, and extract data into a format that is suitable for training. Here, we are in the Construct stage, where we'll bring the model to life. We'll do this by building a Naive Bayes model.
Naive Bayes is a supervised classification technique based on Bayes' theorem with an assumption of independence among predictors. The effect of the value of a predictor variable on a given class is not affected by the values of other predictors.
Bayes' theorem gives us a method of calculating the posterior probability, which is the likelihood of an event occurring after taking into consideration new information. In other words, when you calculate the probability of something happening, you take relevant observations into account. 
It can be represented by this equation which calculates the posterior probability of C given X. Probability of X given C is the probability of the predictor given the class, which is then multiplied by P of C, the posterior probability of the class. The product of these two terms is then divided by P of X, which is the prior probability of the predictor:
$${\text{P(C|X)}} = \frac {\text{P(X|C) . P(C)}}{\text{P(X)}}$$
The posterior probability equation can be rewritten to reveal what's going on behind the variables:
$${\text{P(C|X)}} = {{P(X_1|C) * P(X_2|C) ... P(X_n|C) * P(C)}}$$
The process of finding posterior probability needs to be done for every possible class that is potentially being predicted. Once these values are found, the prediction is made based on the class with the highest posterior probability.
##### Naive Bayes classifiers
Sometimes, the simplest solutions are the most powerful ones. When it comes to supervised machine learning techniques, Naive Bayes is a perfect example of that. The theoretical foundations of the model date back nearly 300 years, and though the field of data science and machine learning has grown immensely in recent years, Naive Bayes models remain relevant because they are simple, fast, and good predictors.
In certain situations, Naive Bayes is also known to outperform much more advanced classification methods. Even if a more advanced model is required, producing a Naive Bayes model can also be a great starting point. Therefore, the Naive Bayes classifier is something that every data professional needs in their machine learning skill set.
###### How do Naive Bayes model work?
A Naive Bayes model is a supervised learning technique used for classification problems. As with all supervised learning techniques, to create a Naive Bayes model you must have a response variable and a set of predictor variables to train the model.
The Naive Bayes algorithm is based on Bayes' Theorem, an equation that can be used to calculate the probability of an outcome or class, given the values of predictor variables. This value is known as the posterior probability. That probability is calculated using three values:
- The probability of the outcome overall P(A)
- The probability of the value of the predictor variable P(B)
- The conditional probability P(B|A) (Note: P(B|A) is interpreted as the probability of B, given A.)
The probability of the outcome overall, P(A), is multiplied by the conditional probability, P(B|A). This result is then divided by the probability of the predictor variable, P(B), to obtain the posterior probability.
Bayes' Theorem
$${\text{P(A|B)}} = \frac {\text{P(B|A) . P(A)}}{\text{P(B)}}$$
The goal of Bayes' Theorem is to find the probability of an event, A, given that another event B is true. In the context of a predictive model, the class label would be A and the predictor variable would be B. P(A) is considered the prior probability of event A before any evidence (feature) is seen. Then, P(A|B) is the posterior probability, or the probability of the class label after the evidence (feature) has been seen.
In a predictive model, this calculation is carried out for each feature, for each class. Then the probabilities are multiplied together. The class with the highest resulting product is the model's final prediction for that sample.
These models make a number of assumptions about the data to work properly. One of the most important is the assumption that each predictor variable (different Bs in the formula) is independent from the others, conditional on the class. This is called conditional independence. Variables B and C are independent of one another on the condition that a third variable, A, exists such that:
$$P(B|C, A) = P(B|A)$$
This equation can be interpreted as 'the probability of B, given C and A, is equal to the probability of B, given A.' 
In other words, given A, introducing C does not change the probability of B. Note that two features can only be considered conditionally independent of each other when considered in relation to a third variable. Furthermore, it's possible for variables B and C to be conditionally independent of one another with respect to A, but not with respect to another variable, say, Z.
In Naive Bayes, the predictor variables (B and C in the equation above) are assumed to be conditionally independent of each other, given the target variable (A). This is an assumption that very often is not actually true. However, Naive Bayes models still often perform well in spite of the data violating the assumption. The assumption is made to simplify the model. Otherwise, long probabilistic chains would have to be calculated to determine the probability of a feature's value with respect to the values of every other variable.
Another assumption of the data is that no predictor variable has any more predictive power than any other predictor. In other words, the individual predictor variables are assumed to contribute equally to the model's predictions. Like the assumption of class-conditional independence between the features, this assumption is also often violated by real-world data, but Naive Bayes still often proves a good model in spite of this.
###### Positives and Negatives
Of all the classification algorithms that are still used today, Naive Bayes is one of the simplest. However, it is still able to produce valuable results. A list of advantages:
- Its simplicity makes it easy to implement. 
- In spite of their assumptions, Naive Bayes classifiers work quite well in industry problems, most famously for document analysis/classification and spam filtering.
- Training time for a Naive Bayes model can sometimes be drastically lower than the other models because the calculations that are needed to make it work are relatively cheap in terms of computer resource consumption.
- This also means that its highly scalable and able to work with large increases in the amount of data it must handle.
Naive Bayes problems:
- Fewer datasets have truly conditionally independent features-- it is something that's very rare today. However, Naive Bayes models can still perform well even if the assumption of conditional independence is violated.
- Another issue that could arise is what is known as the 'zero frequency' problem. This occurs when the dataset you're using has no occurrences of a class label and some value of a predictor variable together. This would mean that there is a probability of zero. Since the final posterior probability is found by multiplying all of the individual probabilities together, the probability of zero would automatically make the result zero. Library implementations of the algorithms account for this by adding a negligible value to each variable count (usually 1) to ensure a non-zero probability.
###### Implementations in scikit-learn
There are several implementations of Naive Bayes in scikit-learn, all of which are found in the `sklearn.naive_bayes module`. Each is optimized for different conditions of the predictor variables. This reading will not delve into the mechanics of each variation. It is intended as a basic guide to using these models. Feel free to explore them on your own!
- `BernoulliNB`: Used for binary/Boolean features
- `CategoricalNB`: Used for categorical features
- `ComplementNB`: Used for imbalanced datasets, often for text classification tasks
- `GaussianNB`: Used for continuous features, normally distributed features
- `MultinomialNB`: Used for multinomial (discrete) features
Of course, datasets aren't always limited to features of just a single type. In this cases, it's often best to try the ones that make the most sense given your data. Often it's best to try the one that makes the most sense to your data. Often it's useful to try several. It might also be the case that none of them work very well. Remember that modelling can be a messy process. Things will break. Assumptions will be violated. Nothing will be perfect, so don't let perfect be the enemy of good. Careful planning, sound decision-making, and a lot of perseverance can go a long way.
###### Key Takeaways
- Naive Bayes is a classification technique that is based on Bayes' Theorem.
- The model will calculate the posterior probability of an event, given the values of the predictor variables.
- This model assumes class-conditional independence of the predictor variables, which can sometimes lead to poor performance in conditions when this assumption is violated.
- Naive Bayes models can be good to use because they are relatively simple to create and are highly scalable depending on the business need.
- scikit-learn has different implementations of the algorithm, each optimized for different conditions of the data.
##### Key evaluation metrics for classification models
The Execute phase of the PACE workflow is where model analysis happens and the model is finally production ready. Model metrics, and the options available to a data professional demonstrates about what model they have built. Types of models built are like supervised learning models, or a categorical model in the form of logistic regression.
The metrics you use to evaluate those models will also make it possible to evaluate a naive bayes model. As a review:
	Accuracy - is the number of correct predictions divided by the total number of predictions.
However, accuracy does not always tell the full story. Some datasets will feature a strong class imbalance. which occurs when the majority of items belong to only one class, then the dataset is considered imbalanced. Here's an example using a binary classification problem.
An IT professional wants to use a model to detect malware in the computer at their company. Perhaps there are 5,000 instances in their dataset, but only 500 positive instances where malware was actually present in a computer. This person would have an imbalanced dataset as the chances of finding malware among all the checks that happen is actually comparatively low. This is where the precision and recall metrics can help.
Precision measures what proportion of positive predictions were correct. In other words, if the model predicted that malware was going to be present, how many times was it actually on a computer? Precision is calculated by dividing the number of true positives by the sum of true and false positives.
The recall metric, on the other hand, finds the proportion of actual positives that were identified correctly. In the context of our example, recall indicates how many would-be malware threats were classified. Recall is calculated by dividing the number of true positives by the sum of the true positives and false negatives.
$F_1$ score combines both precision and recall in one metric. Accuracy, precision, recall and $F_1$ are top metrics in classification techniques. More specifically, precision, recall and $F_1$ score are especially useful for measuring unbalanced classes.
In any case, data professionals use all four metrics to evaluate supervised learning models. As you'll discover, each model performs differently and some algorithms work better than others. When building any model intended for production, it's essential to improve the results. You might change specific parameters to discover how the performance improves. You should keep in mind that model-building is an inherently iterative process. The first model that you produce will almost never be the one that gets deployed. The iterative process provides the information needed to get the model working optimally. 
After tweaking the parameters or changing how features are engineered in each model, the performance metrics provide a basis for comparing the models to each other and against themselves. These metrics make revelations about our models. We use them to evaluate other models we've built and examine how to improve model performance. Continuous improvement is a key part of being a data professional.
##### More about evaluation metrics for classification models
Classification tasks are among the most common applications of machine learning from logistic regression to Naive Bayes among others. Knowing these techniques will empower you to take on data challenges from fraud detection to predicting stock market events and World Cup winners.
In this reading, you will examine some different ways to evaluate the performance of classification models, and also learn some new ones. You will also review the confusion matrix and how it relates to accuracy, precision, and recall. You'll also learn about model evaluation using receiver operating characteristic (ROC) and the area under the ROC curve (AUC), as well as $F_1$ score and $F_β$ score.
###### Evaluation metrics for classification models
Linear regression models have their assumptions, theory, and use cases. They can be evaluated with metrics such as $R^2$, mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE). These metrics are useful when evaluating the error of a prediction on a continuous variable.
Logistic regression models cannot be evaluated using the same metrics as linear regression models. Consider why. With a linear regression model, your model predicts a continuous value that has a unit label (e.g., dollars, kilograms, minutes, etc.) and your evaluation pertains to the residuals of the model's predictions--the difference between predicted and actual values
But with a binary logistic regression, for example, your observations are represented by two classes; they are either one value or another. The model predicts a probability, and assigns observations to a class based on that probability.
Your evaluation must therefore pertain to the class assignment of your model. There ae a number of such evaluation metrics that can be used with classification models. They can all be derived from a confusion matrix, which is a graphical representation of your model. 
###### Accuracy
Accuracy is the proportion of data points that are correctly classified. Its an overall representation of the model performance, represented as:
$${\text{Accuracy}} =\frac {\text{TP + TN}} {\text{T.Pred}}$$
where: $TP$ is true positives, $TN$ is true negatives, and $T.Pred$ is total predictions.
Accuracy is often unsuitable to use when there is a class imbalance in the data, because it's possible for a model to have high accuracy by predicting the majority class every time. In such a case, the model would score well, but it may not be a useful model.
###### Precision
Precision measures the proportion of positive predictions that are true positives. Its represented as:
$$precision = \frac{\text{TP}}{\text{FP+TP}}$$
where: $TP$ is true positives, and $FP$ is false positives.
Precision is a good metric to use when it's important to avoid false positives. For example, if your model is designed to initially screen out ineligible loan applicants before a human review, then it's best to err on the side of caution and not automatically disqualify people before a person can review the case more carefully.
###### Recall
Recall measures the proportion of actual positives that are correctly classified. It is represented as:
$$recall = \frac{\text{TP}}{\text{FN+TP}}$$ 
where: $TP$ is true positives, and $FN$ is false negatives.
Recall is a good metric to use when it's important that you identify as many true responders as possible. For example, if your model is identifying poisonous mushrooms, it's better to identify all of the true occurrences of poisonous mushrooms, even if that means making a few more false positive predictions.
###### ROC curves
Receiver operating characteristic (ROC) curves visualize the performance of a classifier at different classification thresholds. In the context of binary classification, a classification threshold is a cutoff for differentiating the positive class from the negative class. In most modelling libraries-- including scikit-learn--the default probability threshold is 0.5 (i.e., if a sample's predicted probability of response is >= 0.5, then it's labelled 'positive'), but there are some cases where 0.5 might not be the optimal decision threshold to use.
Because you don't always know in advance what the best threshold is for the application, it may make sense to use an ROC curve to capture how good the model is over the full range of thresholds.  The curve is represented by a plot of the true positive rate against the false positive rate.
1. True Positive Rate:
Equivalent/synonymous to recall.
- $${\text{True Positive Rate}} = \frac {\text{TP}}{\text{TP + FN}}$$
where: $T.P$ is true positives, and $F.N$ is false negatives.
2. False Positive Rate:
The ratio between the false positive and the total count of observations that should be predicted as False. 
$${\text{False Positive Rate}} = \frac {\text{FP}}{\text{FP + TN}}$$
where: $F.P$ is false positives, and $T.N$ is true negatives.
For each point on an ROC curve, the horizontal and vertical coordinates represent the false positive rate and the true positive rate at the corresponding threshold.
The false positive rate and true positive rate change together over the different thresholds. 
How does the curve appear like for an ideal model? An ideal model perfectly separates all negatives from all positives, and gives all real positive cases a very high probability and all real negative cases a very low probability. So, imagine starting from a threshold just above zero: 0.001. In this case, it's likely that all real positives would be captured and there would be very few--if any-- false negatives, because for a model to label a sample 'negative', its predicted probability must be <0.001. The true positive rate would be ∾1, and false positive rate ∾0 (refer to the formulas above). Graphically, the more that the ROC curve hugs the top left corner of the plot, the better the model is at classifying the data.
###### AUC
AUC is the measure of the two-dimensional area underneath an ROC curve. AUC provides an aggregate measure of performance across all possible classification thresholds. One way to interpret AUC is to consider it as the probability that the model ranks a random positive sample more highly than a random negative sample. AUC ranges in value from 0.0 to 1.0. In the following example, the AUC is the shaded region below the dotted curve.
###### $F_1$ score
Is a measurement that combines both precision and recall into a single expression, giving each equal importance. It is calculated as:
$${F_{1}} = 2 \cdot \frac{precision \cdot  recall}{precision + recall}$$ 
This combination is known as the harmonic mean. $F_1$ score can range (0, 1), with zero being the worst and one being the best. The idea behind this metric is that it penalizes low values of either metric, which prevents one very strong factor--precision or recall--from 'carrying' the other, when it is weaker.
The $F_1$ score never exceeds the mean. In fact, it is only equal to the mean in a single case: when precision equals recall. The more one score diverges from the other, the more the $F_1$ score penalizes. (Note that you could swap precision and recall values in this experiment and the scores would be the same).
Plotting the means and $F_1$ scores for all values of precision against all values of recall results in two planes. While the coordinate plane of the mean is flat, the plane of the $F_1$ score is pulled further downward the more one score diverges from the other. This penalizing effect makes $F_1$ score a useful measurement of model performance.
###### $F_β$ score
What if you want to capture both precision and recall in a single metric, but you consider one more important than the other? There's a metric for that! Its called $F_β$ score, and $β$ is a factor that represents how many times more important recall is compared to precision. In the case of $F_1$ score, $β$=1, and recall is therefore 1x as important as precision (i.e., they are equally important). However, an $F_2$ score has $β$=2, which means recall is twice as important as precision; and if precision is twice as important as recall, then $β$=0.5. 
It has a formula, and you can assign whatever value you want to $β$. However, in scikit-learn, while most modules have built-in $F_1$ and $F_β$ scorers, some modules may only have an $F_1$ scorer and require you to define your own scoring function if you want to set a custom $β$ value.
###### Key Takeaways
- There are many metrics used to evaluate binary classification models, including accuracy, precision, recall, ROC curve, AUC, and F score.
- Accuracy captures overall model performance, while precision measures true positives and recall measures false negatives.
- $F_β$ score combines precision and recall into a single metric.
- You can set $β$ to any value you wish. $β$ is a factor that determines how many times more important recall is than precision in the score.
- $F_1$ score is simply an $F_β$ score where $β$=1. For $F_1$ score, precision is equally important to recall.
