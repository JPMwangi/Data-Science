Here, we'll focus on supervised learning and learn how to test and validate the performance of supervised machine learning models such as decision tree, random forest, and gradient boosting.
### Additional supervised learning techniques
##### Welcome to module 4
We'll revisit supervised machine learning by investigating some more advanced classification techniques. These advancements are very exciting for data professionals because they enable us to overcome some typical modelling limitations. We'll also learn how these models work and their use cases, that is:
- Tree-based learning - which is a type of supervised learning that performs classification and regression tasks. It uses a decision tree as a predictive model to go from observations about an item represented by the branches to conclusions about the item's target value represented by the leaves. You'll also learn how single decision trees provide a foundation for more advanced approaches to all kinds of data work.
- Ensemble learning techniques which enable the use of multiple decision trees simultaneously in order to produce very powerful models.
- Hyperparameter tuning. Knowing how and when to adjust or tune a model can help a data professional significantly increase performance.
##### Tree-based modeling
There are many techniques that can help you make predictions in the field of supervised learning. One popular tool for classification and prediction is the decision tree. It serves as a foundation for some of the most effective models used in industry today.
A decision tree is a flow-chart like supervised classification model and a representation of various solutions that are available to solve a given problem based on the possible outcomes of related choices. Like all supervised learning classification techniques, decision trees enable data professionals to make predictions about future events based on the information that is currently available. 
They also have some very specific advantages in certain areas over other supervised learning models:
- Decision trees require no assumptions regarding the distribution of underlying data.
- They can handle collinearity very easily.
- Often doesn't require data preprocessing, requiring little preprocessing if any at all.
However, like any other model, decision trees are not perfect. They can be particularly susceptible to overfitting. The model might get extremely good at predicting seen data, but as soon as new data is introduced, it may not work nearly as well. Keep this in mind when building these types of models.
A decision tree consists of nodes and edges. The edges connect together the nodes essentially directing from one node to the next along the tree. Decisions are made at each node. At each, a single feature of the data is considered and decided on. By the end, any relevant features will have been resolved, resulting in the classification prediction.
- The node where the first decision is made is called the root node. Its the first node in the tree and all decisions needed to make the prediction will stem from it. Its a special type of node because it has no predecessors. 
- The nodes where a decision is made are the decision nodes. Decision nodes always point to a leaf node or other decision nodes within the tree. 
- The node where the final prediction is made is a leaf node. The whole process ends here, so no further decisions are required after this point.
- The nodes that are pointed to--whether leaf nodes or other decision nodes, are child nodes. The node that is pointing to them is a parent node.
The algorithm decides what and where variables are split based on what will provide the most predictive power. These are the basics of decision trees and this foundation will be helpful as you continue learning about tree based modeling.
##### Explore decision trees
Tree-based learning is one of the most effective machine learning techniques that are currently used in the industry today. Many different algorithms use a tree-based architecture to make their predictions. The decision tree is the basic building block of these algorithms, as well as a powerful predictive algorithm in itself. Data professionals rely on decision trees as powerful tools that enhance modeling decisions. Here, we're going to look into decision trees, how they are structured, how they work, and how they are built.
###### What is a decision tree?
Decision trees are a flowchart-like structure that uses branching paths to: 
- Predict the outcomes of events,
- To find the probability of certain outcomes, or
- To reach a decision.
They can be used for: 
- Classification problems, where a specific class or outcome is predicted--like whether or not a sports team will win a game. 
- They can also be used for regression problems, where a continuous variable is predicted--like the price of a car.
Here, we are going to focus on classification trees, but in both cases, the models depend on the same underlying decision process.
Decision trees have been used for decades for analyzing problems. However, technological advances have made it so decision trees can be created by computers, offering much deeper and more accurate analysis than humans alone could ever achieve.
###### The structure of a classification tree
Decision trees only resemble actual trees if you flip them upside down, because they start with the root at the top and grow downward so the 'leaves' are at the bottom. Decision trees are made of nodes. Nodes are groups of samples. There are different types of nodes, depending on how they function in the tree. The first node in a decision tree is called a root node. The first split always comes off of the root node, which divides the samples into two new nodes based on the values they contain for a particular feature.
These two new nodes are referred to as child nodes of the root. A child node is any node that results from a split. The node that the child splits is known as the parent node. Each of these two new child nodes turn splits the data again, based on a new criterion. The process continues until the nodes stop splitting. The bottom-level nodes that do not split are called leaf nodes. All the nodes above the leaf nodes are called decision nodes, because they all make a decision that sorts the data either to the left or to the right.
###### Decisions and splits
In a decision tree, the data is split and passed down through decision nodes until reaching a leaf node. A decision node is split on the criterion that minimizes the impurity of the classes in their resulting children. Impurity refers to the degree of mixture with respect to class. Nodes with low impurity have many more of one class than any other. A perfect split would have no impurity in the resulting child nodes; it would partition the data with each child containing only a single class. The worst possible split would have high impurity in the resulting child nodes; both of the child nodes would have equal number of each class.
When building a tree and growing a new node, a set of potential split points is generated for every predictor variable in the dataset. An algorithm is used to calculate the 'purity' of the child nodes that would result from each split point of each variable. The feature and split point that generate the purest child nodes are selected to partition the data.
To determine the set of potential split points that will be considered for a variable, the algorithm first identifies what type of variable it is--such as categorical or continuous--and the range of values that exist for that variable.
- Categorical variables - If the predictor variable is categorical, the decision tree algorithm will consider splitting based on category.
- Continuous variables - If the predictor variable is continuous, splits can be made anywhere along the range of numbers that exist in the data. Often the potential split points are determined by sorting the values for the feature and taking the mean of each consecutive pair of values. However, there can be any number of split points, and fewer split points can be considered to save computational resources and time. This is very common, especially when dealing with very large ranges of numbers, to consider split points along percentiles of the distribution.
Here, the potential split points for the feature are identified by the algorithm. Each option includes the children of that split, but note that at this point none of them has been evaluated yet. That's the next step.
###### Choosing splits: Gini impurity
Now you know how to determine the potential split points. But how do you choose which split to use? This is where the 'purity' of the child nodes become relevant. Generally, splits are better when each resulting child node contains many more samples of one class than any other because this means the split is effectively separating the classes--the primary job of the decision tree! In such cases, the child nodes are said to have low impurity (or high purity). The decision tree algorithm determines the split that will result in the lowest impurity among the child nodes by performing a calculation.
There are several possible metrics to use to determine the purity of a node and to decide how to split, including Gini impurity, entropy, information gain, and log loss. The most straight forward is Gini impurity, and it's also the default for the decision tree classifier in scikit-learn, so we are going to focus on that method. The Gini impurity is calculated for each child node of each potential split point. Now that there are two Gini impurity scores for a split point--one for each child node. The final step is to combine these scores by taking their weighted average.
###### Calculate weighted average of Gini impurities
The weighted average accounts for the different number of samples represented in each Gini impurity score. You cannot simply add them together and divide by two. There is a formula for calculating Gini impurity. This process of calculating is repeated for every split option. The number of split options creates scores ranging from the lowest to the highest score. Since its a measure of impurity, the best scores are the closest to zero.
Now that the algorithm has identified the potential split points and calculated the Gini impurity of the child nodes that result from them, it will grow the tree by selecting the split point with the lowest Gini impurity.
###### Grow the tree
The root node would use a split option to split the data. A child becomes a leaf node because it contains just one class. However, a child that does not have class purity becomes a new decision node (in the absence of some imposed stopping condition). The steps outlined above will repeat on the samples in this node to identify the feature and split value that would yield the best result.
Splitting would continue until all the leaves are pure or some imposed condition stops the splitting. This process involves a lot of computation. As with many machine learning algorithms, the theory and methodology behind decision trees are fairly straightforward and have been around for many years, but it wasn't until the advent of powerful computing capabilities that these solutions were able to be put into practice.
###### Advantages and disadvantages of classification trees
Advantages:
- Require relatively few pre-processing steps.
- Can work easily with all types of variables (continuous, categorical, discrete).
- Do not require normalization or scaling.
- Decisions are transparent.
- Not affected by extreme univariate values.
Disadvantages
- Can be computationally expensive relative to other algorithms.
- Small changes in data can result in significant changes in predictions.
Key takeaways
Decision trees are powerful predictive tools that can identify patterns in data that other algorithms might not be able to. They're user-friendly because they require relatively few preprocessing steps compared to other models. They consist of a series of decision nodes, beginning at a root node and ending at leaf nodes. They operate by splitting data at particular feature values that are identified as thresholds. These split thresholds are determined by calculating the impurity of the child nodes that result from them, and selecting the split that yields the least impurity.
### Tune tree-based models
##### Tune a decision tree
Many models you work with as a data professional use evaluation metrics such as $F_1$ score to gauge their performance. An example of some of these models are; K-means, and decision tree classifier models. How can you gain additional performance increases from your models? A very popular and widely used technique is known as hyperparameter tuning. Hyperparameters are parameters that can be set before the model is trained. They can be tuned to improve model performance, directly affecting how the model is fit to the data.
Hyperparameter tuning is the process of adjusting the parameters to find the best values that will result in the most optimal model, just like a musician tuning the strings on their guitar. The idea is to achieve balance and a beautiful result for tree based modeling.
There are many hyperparameters that can be tuned and they can have a big impact on the model itself. For example, when working with K-means, when building a k-means model, you set the value for $k$ to produce different cluster results. But when you change its value, you perform hyperparameter tuning.
One of the more basic hyperparameters for a decision tree is called max depth. Setting this hyperparameter defines a limit of how long a decision tree can get. The depth of the decision tree is the number of levels between the root node and the farthest node from the root node, with the root node itself being level zero. What matters is the distance of the farthest node from the root and whether it's a leaf node or a decision node.
Advantages of using max depth as a data professional are:
- Setting a max depth can help reduce overfitting problems by limiting how deep the tree will go. Because when working with very large datasets you could potentially create massive trees that are very deep but this isn't necessarily what you want for your model.
- Additionally, setting a max depth can help reduce the computational complexity of training and using the model in the first place. For example, if you're finding that a decision tree has the same performance with a depth of 10 versus a depth of 100 you can set max depth to 10 and achieve the desired performance more quickly. 
Another very commonly used hyperparameter is called min samples leaf. This hyperparameter defines the minimum number of samples that must be contained in a leaf node. It means that split will only happen if there are enough samples in each of the result nodes to satisfy the required value. For example, may be part of the way down your tree, there's a decision node that currently has 10 samples. However, the min samples leaf hyperparameter is set to six. There would be no way to split the data so that each leaf node has six samples and therefore no further split can take place. These are but only two among many of the hyperparameters you'll use as a data professional.
How do you find the optimal values for the parameters? Here's where a grid search is useful. Grid search is a tool to confirm that a model achieves its intended purpose by systematically checking every combination of hyperparameters to identify which set produces the best results based on the selected metric. So at the end, you'll have values that produce optimal results for your model.
When performing a grid search, the first step is to specify which parameters you want to tune and the set of values that you want to search over. For example, maybe we want to tune max depth and min sample leaf. We would define potential values for each of these. For max depth we could check depths of 4, 8, 12, 20, and 30. For mean samples leaf, we could try 10, 50, and 100. The algorithm will check every combination of values to see which pair has the best evaluation metrics. It would first check the max depth of 4 with min sample leaf of 10, then 50, then 100. The algorithm will then check the max depth of 8 with min sample leaf of 10, then 50, then 100. This continues until every combination has been analyzed. Remember you can try any values and any number of values during grid search if you believe the benefits are worth the cost of your computing time.
##### Hyperparameter tuning
In this reading, you'll learn the following:
- Hyperparameters,
- How tuning hyperparameters can affect model performance,
- Develop a deeper understanding of some of the most commonly tuned hyperparameters for decision trees.
- Learn one process that is used to find optimal sets of hyperparameters.
###### Hyperparameter tuning
Modeling techniques help to make predictions or better understand data. Sometimes, the models perform well right out of the box, using the basic or default application of their underlying theory. Most often, however, this is not the case. Each dataset presents its own set of characteristics for which the data professional must tailor the model.
Depending on the characteristics of both the data and the algorithm used to model it, a model might overfit or underfit the data. Remember the aim of a predictive model is to identify underlying, intrinsic patterns and characteristics in data that are representative of all such distributions, and use these characteristics to make predictions on new data.
Overfitting is when the model learns the training data so closely that it captures more than the intrinsic patterns of all such data distributions, and ends up learning noise or idiosyncrasies particular to just the training data. This results in a model that scores very well on the training data but considerably worse on unseen data because it cannot generalize well.
On the other hand, underfitting is when the model does not learn the patterns and characteristics of the training data well, and consequently fails to make accurate predictions on new data. Its typically easier to identify underfitting, because the model performs poorly on both training and test data. The best models neither underfit nor overfit the data. They identify intrinsic patterns within it, but do not capture randomness or noise. One way of helping to achieve this balance is through the use of hyperparameters. Hyperparameters are aspects of a model that you set before the model is trained, and that affect how the model fits the data. They are not derived from the data itself. Hyperparameter tuning is the process of adjusting the hyperparameters to build a model that best fits the data. (Note that you'll often hear them referred to as 'parameters', much like you might hear the word 'theory' used to mean 'idea', when it actually has a very specific meaning. It's okay, as long as you understand the difference).
###### Hyperparameters for decision trees
There are many different hyperparameters available to control how a decision tree grows. Each hyperparameter affects something very specific related to the growth conditions. One might affect what causes a node to split, while another might limit how deep the tree is allowed to grow, and yet another might change the way node purity is calculated. This reading will introduce you to three hyperparameters:
- max_depth
- min_samples_split
- min_samples_leaf
###### Max_depth
Max_depth defines how deep the tree is allowed to grow. The depth of the tree is the distance, measured in number of levels, from the root node to the furthest leaf node. The root node would have a depth of zero, the child of the root node would have a depth of one, and so on.
An unrestricted decision tree will continue splitting until every leaf node contains only a single class. As you increase the max depth parameter, the performance of the model on the training set will continue to increase. (this means that as you increase the max depth parameter, the model learns more about the data). Its possible for a tree to grow so deep that leaves contain just a single sample. However, this overfits the model to the training data, and the performance on the testing data would probably be much worse. 
A quick intuition of why this happens: if you let a tree have so many nodes representing so many specific decision rules that perfectly align with the details of the training data, how likely do you think it is for those exact decision nodes to match new data in the real world? On the other hand, a tree that is not allowed to grow deeply enough will have high bias and fail to make accurate predictions. The best decision tree models are neither too shallow nor too deep, but just right.
###### Min_samples_split
Min_samples_split is the minimum number of samples that a node must have for it to split into more nodes. For example, if you set this to 10, then any node that contains 9 or fewer samples will automatically become a leaf node. It will not continue splitting. However, if the node contains 10+ samples, it may continue to split into child nodes. The greater the value you use for the min_samples_split, the sooner the tree will stop growing. The minimum possible value is two, because two is the smallest number that can be divided into two separate child nodes.
###### Min_samples_leaf
Min_samples_leaf is similar to min_samples_split, but with an important difference. Instead of defining how many samples the parent node must have before splitting, min_samples_leaf defines the minimum number of samples that must be in each child node after the parents splits.
###### Finding the optimal set of hyperparameters
The values that these hyperparameters can take are limited only by the number of samples in your dataset. That leaves open the possibility for millions of combinations! How do you know what the values should be? The answer is to train a lot of different models to find out. There are a number of ways to do this. Performing a grid search is one of them.
###### Grid search
A grid search is a technique that will train a model for every combination of preset ranges of hyperparameter values. The aim is to find the combination of values that results in a model that both fits the training data well and generalizes well enough to predict accurately on unseen data. After all these models have been trained, you then compare them to find this ideal model--if it exists.
Note: For decision trees (and all tree based models), restricting growth is a form of regularization. Recall from regression course that regularization refers to the process of reducing model complexity to prevent overfitting. The greater the complexity of a model, the more susceptible it is to overfitting the training data. Regularization helps to make the model more generalizable to new data.
With more hyperparameters and a more expansive array of values to search over, grid searches can quickly become computationally expensive. One helpful search strategy is to try a wider array of values for each hyperparameter--say, [3, 6 ,9], instead of [3, 4, 5]. If the best model has 6 as the value for this hyperparameter, perhaps try another grid search using [5, 6, 7] as potential values. This technique uses multiple search iterations to progressively home in on an optimal set of values.
Another technique is to define a more comprehensive set of search values (for any hyperparameter) from the beginning--say, [3, 4, 5, 6, 7, 8, 9]-- and let the model train for what could be a very long time. Which approach you take will depend on your computing environment, your computational resources, and how much time you take.
Key takeaways
- Hyperparameters are aspects of a model that are set before the model trains, affecting the training itself.
- Different model types have different sets of hyperparameters that can be changed.
- For decision trees, a few but some of the most important are: max_depth, min_samples_leaf, and min_samples_split.
- You can find the optimal set of hyperparameters using different types of algorithms. GridSearch is one of the more popular techniques, and involves specifying in advance all the values you want to try for each hyperparameter, and then training the model for every combination of those values.
##### Verify performance using validation
There are many tools involved with building powerful models. Model validation is the set of processes and activities intended to verify that models are performing as expected. This is achieved with a validation dataset, which is a sample of data that's held back during training. The validation dataset is instead used to give an unbiased estimate of the skill of the final tuned model. Validation data is different from test data and must remain unseen until the very end of the process. 
With validation, the data is actually split into three sets. The first two are training and testing sets, and the third set is the validation set. This validation set is used instead of the test set to evaluate the model, leaving the test set untouched. In addition, another popular method is cross-validation.
Cross-validation is a process that uses different portions of the data to test and train a model across several iterations. It works like validation, but with a slight twist. Instead of having one validation set to evaluate the model, the training data is split into multiple sections known as folds. Then the model is trained on different combinations of these folds. For example, perhaps we want five-folds. First, the data is split into training and test data. Then the training data would be split into the five-folds. The first model iteration will train with folds 1, 2, 3, and 4, using the 5th fold to get metrics for the model. The next will train with folds 1, 2, 3, and 5, using the 4th fold to get metrics for the model. This process repeats until every combination is done and the evaluation metrics are averaged to get final validation scores.
Which validation technique you choose mainly depends on the dataset you're working with. Cross-validation is particularly useful when working with smaller datasets as it maximizes the utility of the data available, more so than standard validation. However, cross-validation is not necessary when working with very large datasets.
There's so much data that maximizing the utility is not required and actually can be problematic depending on the computing resources at your disposal. However, if limited computing resources or constraints in the data are not issues then cross-validation is almost always applied.
Validation schemes are essential to building and selecting effective models. Data professionals working on these types of business projects are responsible for determining the best scheme to use. This comes with experience along with an understanding of the data and the available tools.
##### More about validation and cross-validation
The basic process of building models is where you split a dataset into training and testing data, fit a model to the training data and evaluate its performance on the test data. This basic process of building models with training data and evaluating them with held-out data is a fundamental part of machine learning and something that you should become very familiar with as a data professional. Here, you'll learn about validation and cross-validation, which are more rigorous ways of training and selecting a model.
###### Model validation
Fitting a model to training data and evaluating it on test data might be an adequate way of evaluating how well a single model generalizes to new data, but it's not a recommended way to compare multiple models to determine which one is best. That's because, by selecting the model that performs best on the test data, you never get a truly objective measure of future performance. The measure would be optimistic.
This may seem difficult to grasp or counterintuitive. You may be asking yourself: How is it not objective? After all, I'm not using the test data to tune my models. Well, if you're comparing how all of the models score on this data and then selecting the model with the best score as the champion, in a way, you're "tuning" another hyperparameter--the model itself! The selection of the final model would itself behave as a tuning process, because you'd be using the test data to go back and make an upstream decision about your model. 
Put simply, if you want to use the test data to get a true measure of future performance, then you must never use it to make a modeling choice. Only use the test data to evaluate your final model. As a data professional, you will likely encounter scenarios where the test data is used to select a final model. It's not best practice, but it's unlikely that it will break your model. However, there are better,  more rigorous ways of evaluating models and selecting a champion.
One such way is through a process called validation. Model validation is the whole process of evaluating different models, selecting one, and then continuing to analyze the performance of the selected model to better understand its strengths and limitations. This reading will focus on evaluating different models and selecting a champion. Post-model selection validation and analysis is a discipline unto itself and beyond the scope of this reading.
###### Validation sets
The simplest way to maintain the objectivity of the data is to create another partition in the data--a validation set--and save the test data for after you select the final model. The validation set is then used, instead of the test set, to compare different models.
When you have a very large dataset, you can split it to whichever ratios that make the most sense for your use case. The reason for this is that the more data you use for validation, the less you have for training and testing. However, if you don't have enough validation data, then your models' scores cannot be expected to give a reliable measure that you can use to select a model, because there's a greater chance that the distributions in the validation data are not representative of those in the entire dataset.
When building a model using a separate validation set, once the final model is selected, best practice is to go back and fit the selected model to all the non-test data (i.e., the training data + validation data) before scoring this final model on the test data.
###### Cross validation
There is another approach to model validation that avoids having to split the data into three partitions (train/validate/test) in advance. Cross validation makes more efficient use of the training data by splitting the training data into $k$ number of 'folds' (partitions), training a model on $k - 1$ folds, and using the fold that was held out to get a validation score. The training process occurs $k$ times, each time using a different fold as the validation set. At the end, the final validation score is the average of all $k$ scores. This is also commonly referred to as k-fold cross validation.
After the model is selected using cross validation, that selected model is then refit to the entire training set (i.e., it's retrained on all $k$ folds combined). 
The cross-validation process maximizes the usefulness of your data with the goal of getting a more accurate measure of model performance. It does so by averaging out the randomness imparted when splitting into training and validation folds. In other words, any time a dataset is split, the specific samples that go into each partition are usually random, which makes it possible for the distributions in each partition to diverge from those found in the full dataset.
Cross-validation reduces the likelihood of significant divergence of the distributions in the validation data from those in the full dataset. For this reason, it's often the preferred technique when working with smaller datasets, which are more susceptible to randomness. The more folds you use, the more thorough the validation. However, adding folds increases the time needed to train, and may not be useful beyond a certain point.
###### Model selection
Once you've trained and validated your candidate models, it's time to select a champion. Of course, your models' validation scores factor heavily into this decision, but score is seldom the only criterion. Often you'll need to consider other factors too. How explainable is your model? How complex is it? How resilient is it against fluctuations in input values? How well does it perform on data not found in the training data? How much computational cost does it have to make predictions? Does it add much latency to any production system? It's not uncommon for a model with a slightly lower validation score to be selected over the highest-scoring model due to it being simpler, less computationally expensive, or more stable.
Once you have selected a champion model, it's time to evaluate it using the test data. The test data is used only for this final model. Your model's score on this data is how you can expect the model to perform on completely new data. Any changes you make to the model at this point that are based on its performance on the test data contaminate the objectivity of the score. Note that this does not mean that you can't make changes to the model. For instance, you might want to retain the champion model on the entire dataset (train + validate + test) so it makes use of all available data prior to deployment. This is acceptable, but understand that at this point you have no way of meaningfully evaluating the model unless you acquire new data that the model hasn't encountered.
###### A review of the model development process.
There is no single way to develop a model. Project-specific conditions will dictate the best approach. Over the course of your development as a data science professional, you'll likely encounter different variations of the train-validate-test process, some of which are more rigorous than others.
A rigorous approach to model development might use both cross-validation and validation. The cross-validation can be used to tune hyperparameters, while the separate validation set lets you compare the scores of different algorithms (e.g., logistic regression vs. Naive Bayes vs. decision tree) to select a champion model. Finally, the test set gives you a benchmark score for performance on new data.
###### Key takeaways
- Model validation is the whole process of evaluating different models, selecting one, and then continuing to analyze the performance of the selected model to be better understand its strengths and limitations. (Note that each unique combination of hyperparameters is a different model).
- Validation can be performed using a separate partition of the data, or it can be accomplished with cross-validation of the training data, or both.
- Cross-validation splits the training data into $k$ number of folds, trains a model on $k - 1$ folds, and uses the fold that was held out to get a validation score. This process repeats $k$ times, each time using a different fold as the validation set.
- Cross-validation is more rigorous, and makes more efficient use of the data. It's particularly useful for smaller datasets.
- Validation with a separate dataset is less computationally expensive, and works best with very large datasets.
- For a truly objective assessment of model performance o future data, the test data should not be used to select a final model.
### Bagging
##### Bootstrap aggregation
Decision trees are useful because they're easy to understand and interpret, flexible in terms of the data they use and highly versatile. They can be good predictors. Decision trees are prone to overfitting and they are sensitive to variations in the training data. How do we solve these problems?
The answer is by using the wisdom of the crowd. This concept can apply to everyday situations as well. For example, if I have a jar filled with jelly beans and I ask a spatial math expert to examine it and guess how many jelly beans there are, their estimate will typically be less accurate than if I asked 1,000 ordinary people to do the same thing and then take the average of their guesses.
We can apply the same concept to modelling using a process called ensemble learning or simply ensembling. Ensemble learning involves building multiple models and then aggregating their outputs to make a final prediction. Just like our jelly bean example, predictions using an ensemble of models are very accurate, even when the individual models themselves are barely more accurate then a random guess.
A best practice when building an ensemble is to use very different methodologies for each model it contains, such as logistic regression, a Naive Bayes model, and a decision tree classifier. In this way, when the models make errors and they will, the errors will be uncorrelated. The goal is for them to not all make the same errors for the same reasons. 
You could build an ensemble using three models such as: logistic regression, a Naive Bayes model, and a decision tree classifier. You'd train each model on the same data, then use each model's individual predictions to make a final prediction, say by taking the majority vote if it's a classification task or averaging the results if it's a regression task.
But there is another way to build an ensemble, a way that uses the same methodology for every contributing model. In this ensemble, each individual model that comprises it is called a base learner. A base learner is each individual model that comprises an ensemble. For this method to work, you usually need a lot of base learners and each is trained on a unique random subset of the training data. If the base learners were all trained on the exact same data, there would be too much correlation between the errors. This would affect the strength of the base learners, and if a base learner's prediction is only slightly better than a random guess, it becomes a weak learner. To address this, data professionals do something called bagging in order to ensure random subsets of the data and strong leaners.
The word bagging comes from bootstrap aggregating. Let's break this down. In statistics, bootstrapping refers to sampling with replacement. That's what happens during bagging too. Each base learner samples from the data with replacement, for bagging this means the various base learners all samples the same observation, and a single learner can sample that observation multiple times during training.
The aggregation part of bagging, refers to the fact that the predictions of all the individual models are aggregated to produce a final prediction. For regression models, this is typically the average of all the predictions. For classification models, it's often whichever class receives the most predictions, which is the mode.
When bagging is used with decision trees, we get a random forest. A random forest is an ensemble of decision tree base learners that are trained on bootstrapped data. The base learners predictions are all aggregated to determine a final prediction. Random forest takes the randomization from bagging on step further. A regular decision tree model will seek the best feature to use to split a node. A random forest model will grow each of its trees by taking a random subset of the available features in the training data and then splitting each node at the best feature available to that tree.
This means that each base learner in a random forest model has different combinations of features available to it, which helps to prevent the problem of correlated errors between learners in the ensemble. Each individual base learner is a decision tree. It may be fully grown, so each leaf is a single observation or it may be very shallow depending on how you choose to tune your model. Ensembling many base learners helps reduce the high variance that you typically get from a single decision tree.
Ensemble learning is powerful because it combines the results of many models to help make more reliable final predictions, plus these predictions have less bias and lower variance than other standalone models.
##### Bagging: How it works and why to use it
Ensembles of base learners can combine to become powerful predictors. Bagging is one of the more commonly used modeling strategies. Here, we'll learn what this technique is and how it works and how it can be beneficial.
###### A review of bagging
Bagging stands for bootstrap aggregating, but knowing this doesn't clarify much so let's unpack these terms.
###### Bootstrapping
It refers to sampling with replacement. In ensemble modeling architectures, this means that for each base learner, the same observations can and will be sampled multiple times. Suppose you have a dataset of 1,000 observations, on average, and you bootstrap sample it to generate a new dataset of 1,000 observations, on average, you should find about 632 of those observations in your sampled data (~63.2%).
###### Aggregating
Building a single data with bootstrapped data wouldn't be very useful. To use the example above, if you start with 1,000 unique observations and use bootstrapping to create a sampled dataset of 1,000 observations, you'd expect to get an average of 632 unique observations in that new dataset. This means that you'd lose whatever information was contained in the 368 observations that didn't make it into the new sampled dataset.
This is where ensemble learning--or ensembling--comes to the rescue. Ensemble learning refers to building multiple models and aggregating their predictions. Sure, those 368 observations might not make it into that particular sampled dataset, but if you keep repeating the bootstrapping process--once for each base learner--eventually your overall ensemble of base learners will see all of the observations.
###### Why to use it:
- Reduces variance: Standalone models can result in high variance. Aggregating base models' predictions in an ensemble help reduce it.
- Fast: Training can happen in parallel across CPU cores and even across different servers.
- Good for big data: Bagging doesn't require an entire training dataset to be stored in memory during model training. You can set the sample size for each bootstrap to a fraction of the overall data, train a base learner, and string these base learners together without ever reading in the entire dataset all at once.
Key takeaways
Bootstrapping and aggregating together are known collectively as bagging. A simple way to understand bagging is to think of it as making a copy of your data to train each base learner, but each base learner's copy is slightly different. Bagging models reduce variance, are fast to train, and are good to use with very large datasets.
##### Explore a random forest
Let's examine what they are and how they function. Its important to understand this methodology because its commonly used in data work. And many of its component steps are used by other more advanced modeling strategies.
A random forest is an ensemble of learning trees whose predictions are all aggregated to determine a final result. Each tree in a random forest model uses bootstrapping to randomly sample the observations in the training data with replacement. Remember, this means that any tree in the model can use the same observation and the same observation can be sampled more than once by the same tree (or the same model). 
Bootstrapping is a critical component of random forest models. It ensures that every base learner in the ensemble is trained on different data while allowing each learner to train on a dataset that's the same size as the original training data. Because there are duplicate observations in the trees training data, each one will be missing some of the observations from the original training dataset.
Another important principle of random forest models is that all trees in the ensemble are trained on a random subset of the available features in the dataset. No single tree sees all the features. Again, this is to introduce another element of randomness and ensure that each tree is as different from others as possible.
You learned that one of the main weaknesses of decision trees is that they are very sensitive to new data, so they're prone to overfitting. Therefore, randomizing both the data and the features used by each base learner means that no single tree can fit all the data. This is because no single tree sees all the data. In fact, the trees underfit the data. They have high bias, but together they can be very powerful predictors that are more stable than a regular single decision tree. 
In addition, random forests are scalable. All the base learners they rely on can be trained in parallel using different processing units, even across many different servers. 
Finally, just like decision trees, random forest models need to be tuned to find the combination of hyperparameter settings that results in the best predictions. After all, random forests are made up of many decision trees and data professionals want them all to be as effective as possible.
##### More about random forests
$${\text{Bagging}} +{\text{random feature sampling}} = {\text{random forest}} $$
You know that bootstrap aggregating--or bagging--can be an effective way to make predictions by building many base learners that are each trained on bootstrapped data and then combining their results. If you build a bagging ensemble of decision trees but take it one step further by randomizing the features used to train each base learner, the result is called a random forest. Here, you'll learn how random forests use this additional randomness to make better predictions, making them a powerful tool for the data professional.
###### Why randomize?
Random forest models leverage randomness to reduce the likelihood that a given base learner will make the same mistakes as other base learners. When mistakes between learners are uncorrelated, it reduces both bias and variance. In bagging, this randomization occurs by training each base learner on a sampling of the observations, with replacement.
To illustrate this, consider a dataset with five observations: 1, 2, 3, 4, and 5. If you were to create a new, bootstrapped dataset of five observations from this original data, it might look like 1, 1, 3, 5, 5. It is still five observations long, but some observations are missing and some are counted twice. The result is that the base learners are trained on data that is randomized by observation.
Random forest goes further. It randomizes the data by features too. This means that if there are five available features: A, B, C, D, and E, you can set the model to only sample from from a subset of them. In other words, each base learner will only have limited number of features available to it, but what those features are will vary between learners.
###### How does all this sampling affect predictions?
The effect of all this sampling is that the base learners each see only a fraction of the possible data that's available to them. Surely this would result in a model that's not as good as the one that was trained on the full dataset, right?
No! In fact, not only is it possible for model scores to improve with sampling, but they also require significantly less time to run, since each tree is built from less data. It may seem counterintuitive, but you can often build a well-performing model with even lower bootstrapping samples.
Key takeaways
Random forest builds on bagging, taking randomization even further by using only a fraction of the available features to train its base learners. This randomization from sampling often leads to both better performance scores and faster execution times, making random forest a powerful and relatively simple tool in the hands of any data professional.
##### Tuning a random forest
Here, we'll build our understanding of how decision trees grow. This will be the basis on which we'll tune a random forest model. You've learned that random forests make predictions by sampling features and observations to grow trees.
With decision trees, splits are decided by which variables and which cut-off values offer the most predictive power. Now, let us consider that a decision tree continues to split until one of a certain set of conditions is met:
- The first condition has to do with the observations that a leaf contains. When all of the observations belong to the same class, this means that leaf node is pure.
- The second condition affecting where a tree splits is whether the minimum leaf size or maximum depth is reached.
- Also, a decision tree may stop growing if it achieves a certain performance threshold. The value and metric for this threshold can both be specified by the modelers. These settings are known as hyperparameters and they can be tuned to improve model performance directly affecting how the model is fit to the data.
Hyperparameters in a decision tree:
- Max_depth: Specifies how many levels the tree can have and ultimately determines how many splits it can make. Remember, every time a node splits, your data gets separated into smaller subsets. The model is drawing another decision boundary. 
- Min_samples_leaf: Defines the minimum number of samples for a leaf node. With min_samples_leaf, a split can only occur if it guarantees a minimum number of observations in the resulting nodes.
- Min_samples_split: can be used to control the threshold below which nodes becomes leaves. 
Random forest models have the same hyperparameters because they are ensembles of decision trees. These hyperparameters control how the learner trees are grown. But random forests also have some other hyperparameters which control the ensemble itself. They are:
- Max_features: Specifies the number of features that each tree randomly selects randomly from the training data to determine its splits during training.
- n_estimators: Specifies the number of trees your model will build in its ensemble. For example, if you set your number of estimators (n_estimators) to 300, you model will train 300 individual trees. If you're building regression trees, then the model's final prediction would average the predictions of all 300 trees. If you're building classification trees, the final prediction would be determined by whichever class received the majority vote from the 300 individual trees.
For random forest models, performance will typically increase as trees are added to the ensemble, but only to a certain point. After this point, improvements will level off and adding new trees will only increase your computing time. This happens because the new trees will become very similar to existing trees so they won't contribute anything new to the model. 
Many data professionals build models without hand setting each hyperparameter. In fact, when using scikit-learn, the model might perform well with no hyperparameters at all. That's because it has an effective default setting. Remember to make the most of grid search to help you iterate. Data professionals know how to experiment with combinations of hyperparameters in order to build the model that makes the very best predictions.
##### Case Study: Machine learning model unearths resourcing insights for Booz Allen Hamilton
One way that this consulting firm has used machine learning to help their business is by constructing a model to better understand the likelihood of winning contracts. Insights derived from the model help Booz Allen Hamilton anticipate its demand signal and subsequently understand and plan for future resourcing and staffing needs. Refer to this case study to learn more about how a machine learning model is developed and deployed at an enterprise level.
###### Company background
For more than a century, the government, military, and business leaders have turned to Booz Allen Hamilton to solve their most complex problems through their experts in analytics, digital solutions, engineering, and cyber work to help their organizations transform.
They work in partnerships with clients, using a mission-first approach to choose the right strategy and technology to help them realize their vision. 
###### The challenge
Booz Allen Hamilton's work comes from contracts awarded by federal government and commercial clients. To get this work, the company bids on contracts. The bidding process involves reviewing the job requirements and submitting a proposal for a solution and a cost to implement it. The organization that solicited the bid will then review these proposals and award a contract to the company whose bid best suits their needs and budget. Upon award, Booz Allen Hamilton must meet those requirements and deliver the best possible solution to their client. For a company of its size, the uncertainty that comes with bidding and winning work can make staffing and resource planning difficult. With this machine learning model they are able to better understand which contracts they'd be more likely to win and therefore understand the workforce and skills needed to meet its future demand.
###### Step 1: The stakeholders pave the way
Even though the model would only be used internally at Booz Allen Hamilton, it still involves numerous stakeholders, all of whom needed to work together to develop and deploy the model. At large organizations, this can take days, weeks, or even months. For this model, the stakeholders include: 
- Corporate finance team: The team that will use the model and have domain expertise of the model's applications and use cases. They dictate what the model should do, and they know what data is relevant and available to build it.
- Enterprise Data Science team: The team responsible for training, validating, and deploying the model. Throughout model development, they work closely with the model end-users (the Corporate Finance team) to collet feedback, explore use cases, and validate results.
- Enterprise Platforms and Engineering team: This is the team responsible for storing and maintaining the data in an enterprise data lake ( a large repository of structured and unstructured data). Large organizations typically have so much data that there is a team tasked with managing all of it. This team is also responsible for setting up the necessary computing platform and infrastructure where the model is developed and deployed.
###### Step 2: Develop the model
To provide more insight to leadership and decision-makers, the Data Science team created a tree-based classification model that predicts the likelihood of winning a contract.
Target variable and model output: The target variable was binary: whether or not a submitted bid won Booz Allen Hamilton the contract. The model's final output is the probability of winning a given contract.
Algorithms considered: The data science team explored a number of different algorithms for the model, including logistic regression, support vector machines, decision tree, and random forest. A random forest model was ultimately selected as the champion solution.
Class balance: There was a minor imbalance between the classes of the target variable. Both upsampling and downsampling were tried, but ultimately neither provided any substantial lift over the baseline.
Evaluation metrics: The model was evaluated based on its performance with respect to four different metrics: Area under the ROC curve (ROC/AUC), F1 score, Accuracy, Log-loss.
Splitting the data: The data comprises several full years of historical data as well as resolved bids from the current fiscal year. The data was split into training and test sets.
- Training data: Several years of historical data and 50% of resolved bids from the current fiscal year.
- Test data: The other 50% of resolved bids from the current fiscal year.
Model training & tuning: The data science team tuned four main hyperparameters using 5-fold cross-validation:
- Max samples: The number of observations sampled with replacement.
- Max features: The number of features to consider when looking for the best split.
- Number of trees: The number of base learners grown in the ensemble.
- Tree depth: The level to which each is allowed to grow.
Feature selection: of the 250+ initial features, approximately 40 features of varying types (numerical, categorical, and Boolean) were selected for use in the final model based on their relative performance.
Model selection: The final model was a random forest model that was selected based on its performance on the test data as indicated by the four metrics listed above.
###### Step 3: Deploy the model
Once the data science team finished building the model, they drafted a report to present to the Corporate finance team. This report contains details of the model development and validation process, including model architecture, performance results, important features, and relevant visualizations that support their conclusions. One of the most important criteria for model approval and deployment is that the model provides an improvement over existing methods of estimation. In this case, the final model proved to be 12% more accurate than existing methods.
After receiving buy-in from the Corporate finance team, the data science team, with support from the Platforms and engineering team, worked to deploy the model so end users can begin using its predictions. Enterprise-level data science is not performed on a personal laptop. The volume of data, computing requirements, and risk are all too great for this. The model must be developed and deployed on a platform that is powerful, reliable, and secure enough to support it. The data science team developed and deployed this model leveraging the enterprise data lake infrastructure, which includes a notebook-based model development environment as well as a platform to streamline machine learning development, management, and deployment. 
###### Key takeaways
Below is a summary of the main insights from this case study.
- Machine learning solutions are transforming the way businesses operate, and these solutions are not all customer-facing; they're also used to aid in internal processes and decision-making.
- Development of machine learning solutions at large businesses involves many stakeholders with diverse responsibilities and fields of expertise.
- Many of the algorithms taught in the online courses-including logistic regression, and random forests--are used by the biggest companies in the world.
- Even when a model performs well, its use must still be carefully explained and justified before deployment. It is not enough to simply build a model that scores well.
### Boosting
Boosting, just like random forests, is another methodology for building tree-based ensemble models. Boosting is one of the most powerful modeling methodologies in the field. It's used in nearly every industry that relies on predictive modeling. Many winning models from Kaggle and other competitions use boosting. It's an essential tool in any modeler's toolbelt.
Boosting is a supervised learning technique where you build an ensemble of weak learners. This is done sequentially with each consecutive base learner trying to correct the errors of the one before. Remember, a weak learner is a model whose prediction is only slightly better than a random guess, and a base learner is any individual model in an ensemble.
Similarities to random forest and bagging.
- Boosting is an ensembling technique
- Aggregates the predictions of many weak learners
Differences from random forest and bagging.
- Learners are built sequentially, not in parallel. This is because each new base learner is the sequence focuses on what the preceding learner got wrong.
- The methodology you choose for the base learner is not limited to tree-based methods.
We will use tree-based implementations here, because they are common and efficient ways of building boosting models. There are various different boosting methods available. We'll cover two of the most commonly used methodologies. The first is called adaptive boosting or AdaBoosting.
AdaBoost is a tree-based boosting methodology where each consecutive base learner assigns greater weight to the observations incorrectly predicted by the preceding learner. AdaBoost builds its first tree on training data that gives equal weight to each observation. Then the algorithm evaluates which observations were incorrectly predicted by the first tree. It increases the weights for the observations that the first tree got wrong, and decreases the weights for those that got it right. This process repeats until either a tree makes a perfect prediction or the ensemble reaches the maximum number of trees, which is a hyperparameter that is specified by the data professional. Once the ensemble has been built, the ensemble makes the predictions by aggregating the predictions of every model in the ensemble.
Because AdaBoost can be used for both classification and regression problems, this final step is a little different depending on which type is being addressed. For classification, the ensemble uses a voting process that places weight on each vote. Base learners that make more accurate predictions are weighted more heavily in the final aggregation. For regression, the model calculates a weighted mean prediction for all the trees in the ensemble.
There is one disadvantage about boosting. You can't train your model in parallel across different servers, because each model in the ensemble is dependent on the one that preceded it. In terms of computational efficiency, it doesn't scale well to very large datasets when compared to bagging. However, this generally isn't a concern unless you're working with particularly large datasets.
Advantages of boosting:
- One of the accurate methodologies available today.
- The problem of high variance is reduced because its based on an ensemble of weak learners.
- It reduces bias, easy to understand and doesn't require the data to be scaled or normalized.
- Boosting can handle both numeric and categorical features, and it can still function well even with multicollinearity among the features, plus it's robust to outliers.
##### Gradient boosting machines
Gradient boosting is a boosting methodology where each base learner in the sequence is built to predict the residual errors of the model that preceded it. This is different from AdaBoosting which assigns weights to incorrect predictions.
Imagine that a target is a continuous variable. Let's start with a set of features. We'll train the first base learner decision tree and call it `learner1`:
$${\text {learner1.fit(X, y)}}$$
`Learner1` makes its predictions which we will call: $y_1$:
$${{\text y_1}} = {\text{learner1.predict(X)}}$$
The residual errors of `learner1's` predictions are found by subtracting the predicted values from the actual values. Call the set of residual errors, $error_1$:
$${\text{error}}_1 = {\text{y}} - {{\text y_1}}$$
Now train a new base learner using the same X data but instead of the original Y data, use $error_1$ as the target. That's because this learner is predicting the error made by `learner1`. Call this new base learner, `learner2`.
$${\text {learner2.fit(X,}} {\text{error}}_1)$$
Learner2's predictions are assigned to $error_{h1}$ (error hat sub 1). :
$${\text{error}}_{h1} = {\text{learner2.predict(X)}}$$
Then compare learner2's predictions to the actual values and assign the difference to $error_2$. In this case, the actual values are the errors made by learner1:
$${\text{error}}_2 = {\text{error}}_1 - {\text{error}}_{h1}$$
This process will continue for as many base learners as we specify. For now, repeat it once more. Stopping here results in an ensemble that contains three base learners:
$$
{\text {learner3.fit(X,}} {\text{error}}_2)
$$
$${\text{error}}_{h2} = {\text{learner3.predict(X)}}$$
$${\text{error}}_3 = {\text{error}}_2 - {\text{error}}_{h2}$$
To get the final prediction for any new X, add together the predictions of all the three learners.
Ensembles that use gradient boosting are called gradient boosting machines or GBMs. Advantages of using them are:
- High accuracy: Many ML competition winners succeeded largely because of the accuracy of their boosting models. 
- GBMs are generally scalable. Even though they can't be trained in parallel, like random forests, because their base learners are developed sequentially they still scale well to large datasets.
- GBMs work with missing data. The fact that a value is missing is viewed as valuable information. So GBMs treat missing values just like any other value when determining how to split a feature. This makes gradient boosting relatively easy to use with messy data.
- Because they are tree-based, GBMs don't require the data to be scaled and they can handle outliers easily. 
Gradient boosting also has drawbacks:
- GBMs have a lot of hyperparameters, and tuning them can be a time-consuming process.
- They can be difficult to interpret. GBMs can provide feature importance but unlike linear models, they do not have coefficients or directionality. They only show how important each feature is relative to the other features. Because of this, they're often called black-box models. This is a model whose precise predictions cannot be explained. In some industries such as medicine and banking, its essential that your model's predictions be explainable. Therefore, GBMs are not well suited for some applications.
- GBMs can also have difficulty with extrapolation. Extrapolation is a model's ability to predict new values that fall outside of the range of values in the training data. 
- GBMs are prone to overfitting if not trained carefully. Usually this is caused by training too many hyperparameters, which can result in the trees growing to fit the training data, but not generalizing well to unseen data.
##### Tune a model
XGBoost - extreme gradient boosting; an optimized GBM package. Its a predictive modeling field package. Its used for fast training, effective regularization of features, and tunable hyperparameters. These are:
- max_depth - controls how deep each base learner tree will grow. Typically 2-10 depending on observations in the data.
- n_estimators - the maximum number of base learners your ensemble will grow. Typically 50-500. Best determined by grid search.
- learning_rate (shrinkage) - how much weight is given to each consecutive tree's prediction in the final ensemble. Typically 0.01 (10%) - 0.3. Best determined by grid search.
- min_child_weight - A tree will not split a node if it results in any child node with less weight than this value, instead the node will become a leaf. Very similar to min_samples_leaf. Typically 0-1 which is interpreted as a 10% of training data, and 1+ is interpreted as sample weight (number of observations).
