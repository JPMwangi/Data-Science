### To learn
- You'll explore the role of statistics in data science.
- Identify the difference between descriptive and inferential statistics.
- How descriptive statistics can help you quickly summarize a dataset and measure the center, spread, and relative position of data to better understand it.
- Explore how to use inferential statistics to draw conclusions and make predictions about data.
- Probability and discover useful ways to measure uncertainty. 
- Discuss the basic rules of probability and how to interpret different types of probability distributions such as the normal, binomial, and Poisson distributions.
- Sampling and what makes a good sample, the benefits and drawbacks of different sampling methods and how to work with sampling distributions.
- Confidence intervals (CI) which helps describe the uncertainty in an estimate.
- Constructing different kinds of confidence intervals (CI) and interpret their meaning.
- How to use hypothesis testing to compare and evaluate competing claims about your data.
- How to go over the steps for applying different tests to specific datasets and demonstrate how to interpret the results.
##### Introduction
Statistics is the study of the (the how to) collection, analysis, and interpretation of data. Often abbreviated as stats, it provides data professionals with powerful tools and methods for transforming data into useful knowledge. Stats is used for: 
- descriptive statistics which can help you quickly summarize a dataset and measure the center, spread, and relative position of data, and;
- inferential statistics which, based on a small sample of available data, data professionals can make informed predictions about uncertain events and make accurate estimates about unknown values.
Application of statistics by data professionals:
- used in the prediction of future sales revenue.
- the success of a new ad campaign.
- the rate of return (ROI) on a financial investment.
- the number of downloads for a new app.
- stats can tell you which version of a website will attract more new customers for longer periods of time or that new users will typically create an account after spending three minutes on the company's website.
The insights gained (the results) from statistical analysis help business leaders make decisions, solve complex problems and improve the performance of the products and services. This is why data professionals are in such high demand and why the data career keeps growing. Data professionals use stats and other tools within an organization to analyze and interpret data to help business leaders make informed decisions, this includes, for example:
- they help business leaders quantify uncertainty and identify if there is sufficient evidence to reject or fail to reject a hypothesis.
Econometrics - A branch of economics that uses statistics to analyze economic problems.
Learning constantly in the field of stats will help you find out that you can solve statistical problems using methods you're unfamiliar with. These advanced methods are built on a foundation of stats concepts. Here we are going to learn these concepts step by step. No prior knowledge is needed. With prior experience, you'll be able to apply your stats knowledge in a new way in data analytics. 
You'll discover how data professionals use statistical tools in their daily work. You'll also learn strategies for interpreting findings and sharing them with stakeholders who may not be familiar with stats concepts or all the technical details. 
### Welcome to Module 1
The use of statistics to gain insights from data helps organizations solve complex problems. 
##### The role of statistics in data science.
Today, humans generate and collect more data than ever before. Whenever we send a text message, make a purchase online, or post a photo on social media, we generate new data. As the amount of data grows, so does the need to analyze and interpret it. This is why stats and data driven work is so important and the field of data analytics is growing almost as fast as the data itself.
Data professionals use statistics to analyze data in business, medicine, science, engineering, government, and more. Data professionals use statistical methods to:
- Identify meaningful patterns and relationships in data.
- Analyze and quantify uncertainty.
- Generate insights from data.
- Make informed predictions about the future.
- Solve complex problems.
Since we use stats daily, it gives us useful knowledge to apply to our own lives. Data professionals use the same concept in their work. On the job, they use stats to transform data into insights that help stakeholders make decisions. Statistics is the foundation of data analytics and is the basis for the most advanced methods of analysis that data professionals use. It all begins with the fundamental concepts.
Consider the role grammar plays in your conversations. If you are having a conversation, then you already know how to use nouns, verbs, and adjectives. Knowledge of basic grammar makes it possible to use language in the first place. This is why it is so foundational.
In a similar way, shared knowledge of basic statistics allows data professionals to use a common language. Learning the basics will eventually let you join the conversation about more advanced topics. You will build on your foundation in stats with more complex methods like hypothesis testing, classification, regression, and time series analysis.
##### Statistics in action: A/B testing.
In today's economy, business leaders want to make data-driven decisions based on evidence and analysis. Companies that use insights gained from data to guide their decision-making process are more likely to be successful than companies that don't, and data professionals are the people that generate those insights. They use stats to transform data into knowledge, and help stakeholders make informed decisions.
A/B testing is a way to compare two versions of something to find out which version performs better. Its used by companies to compare everything from website design, to mobile apps, to online ads, to email marketing. Its become popular because it works well with many online applications. 
The A/B test analyzes a small group of users drawn from the total population known as a sample. A sample is a subset of a population. Data from a sample can be used to make inferences or draw conclusions about the entire population. Data professionals use inferential statistics to achieve this. This means that stats are a powerful tool for predicting outcomes you don't know using data you do know. For example, you have no way of knowing how the next 100,000 website visitors will behave. What you can do is observe the next 1000 visitors, and then use inferential stats to predict how the following 99,000 will behave. Stats can help you make that prediction with accuracy. This is why observing a sample through A/B testing can be so valuable to companies. They can use the results of the test to make changes that improve their business. 
Sampling, the process of selecting a subset of data from a population, is a critical part of an A/B test. Before you conduct the test, you need to decide on the sample size or the number of users in the test. Choosing the right sample size helps you get valid results and avoid statistical errors. Like any statistical test, an A/B test can't predict user behavior with 100 percent certainty. What stats can do is construct a confidence interval or a range of values that describes that uncertainty surrounding an estimate. Knowing how to construct and interpret a confidence interval helps you make informed decisions about all the users based on your test sample. Using stats, you can quantify the uncertainty of your A/B test and share this information with stakeholders to help them interpret the results. After the test is complete, you'll need to determine the statistical significance of your results. Statistical significance refers to the claim that the results of a test or experiment are not explainable by chance alone. A hypothesis test is done to help quantify whether the result is likely due to chance or if its statistically significant. 
Software can help you calculate complex math problems but having a working knowledge of stats lets you properly design, conduct, and interpret the results of a real test. 
##### Descriptive statistics vs Inferential statistics.
The two main types of statistical methods. Data professionals use each method to get different insights from their data. Here, we'll learn the difference between them and how they are used by data professionals to better understand the data. 
###### Descriptive statistics
They describe or summarize the main features of a dataset. They quickly allow you to describe a large amount of data. If you summarize large amount of data, you can instantly make it meaningful. There are two common forms of descriptive statistics: 
Visuals - Graphs and tables can help you explore, visualize, and share your data. Examples are histograms, scatter plots, and boxplots.
Summary stats- They let you summarize your data using a single number. There are two main types of summary stats:
- Measures of central tendency like mean let you describe the center of your dataset.
- Measures of dispersion like standard deviation let you describe the spread of your dataset or the amount of variation in your data points.
###### Inferential statistics
They make inferences about a dataset based on a sample of the data. The dataset that the sample is drawn from is called the population. 
The population includes every single element that you are interested in measuring. 
A sample is a subset of the population.
Data professionals use samples to make inferences about populations. In other words, they use the data they collect from a small part of the population to draw conclusions about the population as a whole. A statistical population may refer to people, objects, or events. For instance, a population might be the set of all residents of a country, all the planets in our solar system, or all the outcomes of 1,000 coin flips. A sample is a smaller group or subset of any of these populations.
Your sample should be representative of the population, meaning it accurately reflects the population. Otherwise the conclusions you draw from your sample will be unreliable and possibly biased. 
A parameter is a characteristic of a population.
A statistic is a characteristic of a sample.
#### Descriptive stats
##### Measures of central tendency.
These are values that represent the center of a dataset in different ways. They let you describe the center of your dataset using a single value. When you are working on a new dataset, identifying the central location of your data helps you quickly understand its basic structure and determine the next steps in your analysis. 
Here, we will learn about the three measures of central tendency: the mean, median, and mode, and discuss which measure is best to use based on your specific data.
Mean
The average value in a dataset. To calculate the mean, you add up all the values in your dataset and divide by the total number of values.
Median
The middle value in a dataset. This means that half the values are larger than the median, and half the values are smaller than the median. You can find the median by arranging values in a dataset from the smallest to the largest. If there are an even number of values in your dataset, the median is the average of the two middle values.
Mode
The most frequently occurring value in a dataset. A dataset can have no mode, one mode, or more than one mode.
##### When to use the mean, median, and the mode.
Whether you use the mean, median, or mode to describe the center of your dataset depends on the specific data you're working with and what insights you want to gain from your data. Here are some general guidelines for using each measure of central tendency.
Mean versus median
Both the mean and the median describe the central location of a dataset. However, as measures of central tendency, the mean and the median work better for different kinds of data. The disadvantage of the mean is that it is very sensitive to outliers. It will pull the average up or down or skew the mean in the dataset. An outlier is a value that differs greatly from the rest of the data. 
If there are outliers in your dataset, the median is usually a better measure of the center. If there are no outliers, the mean usually works well.
Mode
The mode is helpful when working with categorical data because it clearly shows you which category occurs most frequently. 
##### Measures of dispersion.
They let you describe the spread of your dataset, or the amount of variation in your data values. Measures of dispersion like standard deviation can give you an initial understanding of the distribution of your data, and help you determine what statistical methods to apply to your data. Let's examine out the definition of each measure of dispersion:
Range
The range is the difference between the largest and smallest value in a dataset. It is a useful metric because it's easy to calculate, and it gives you a very quick understanding of the overall spread of your dataset.
Variance
It is the average of the squared difference of each data point from the mean. Basically it's the square of the standard deviation.
Standard deviation
Definition
It measures how spread out your values are from the mean of your dataset. It calculates the typical distance of a data point from the mean. The larger the standard deviation, the more spread out your values are from the mean. The smaller the standard deviation, the less spread out your values are from the mean.
Visualization
The mean is the highest point on each curve in a probability distribution. The curve with the least spread has most of its data values close to the mean, giving it a small standard deviation. The curve with the most spread has its data values fall farther away from the mean giving it a larger standard deviation. 
Formula
There are different formulas to calculate the standard deviation for a population and a sample. Data professionals usually work with sample data, and they make inferences about populations based on the sample. 
As a data professional, you'll typically use a computer for calculations. Being able to perform calculations is important for your future career, but being familiar with the concepts behind the calculations will help you apply statistical methods to workplace problems.
Key takeaways
Data professionals use standard deviation to measure variation in many types of data like ad revenues, stock prices, employee salaries, and more. Measures of dispersion like the standard deviation, variance, and range let you quickly identify the variation in your data values, and get a better understanding of the basic structure of your data.
##### Measures of position: Percentiles and quartiles.
Measures of position let you determine the position of a value in relation to other values in a dataset. Along with center and spread, it's helpful to know the relative position of your values. For example, whether one value is higher or lower than another, or whether a value falls in the lower, middle or upper portion of your dataset. 
Percentile
Is a value below which a percentage of data falls. Percentiles divide your data into 100 equal parts giving relative position or rank of a particular value in a dataset. For example, percentiles are commonly used to rank test scores on school exams. Let's say that a test score falls in the 99th percentile. This means the score is higher than 99% of all test scores. If a score falls in the 75th percentile, the score is higher than 75% of all test scores. If a score falls in the 50th percentile, the score is higher than half, or 50%, of all test scores.
Percentiles and percentages are distinct concepts. For example, say you score 90/100, or 90%, on a test. This doesn't necessarily mean your score of 90% is in the 90th percentile. Percentile depends on the relative performance of all test takers. If half of all test takers score above 90%, then a score of 90% is in the 50th percentile.
Percentiles are useful for comparing values and putting data into context. For example, imagine you want to buy a new car. You'd like a midsize sedan with great fuel economy. In the U.S, fuel economy is measured in miles per gallon of fuel, or mpg. The sedan you are considering gets 23mpg. Is that good or bad? Without a basis for comparison, its hard to know. However, if you know that 23 mpg is in the 25th percentile of all midsize sedans, you have a much clearer idea of its relative performance. In this case, 75% of all midsize sedans have a higher mpg than the car you're thinking about buying.
Quartile
Used to get a general understanding of the relative position of values. A quartile divides the values in a dataset into four equal parts. Three quartiles divide the data into four quarters. Quartiles let you compare values relative to the four quarters of data. Each quarter includes 25% of the values in your dataset.
The first quartile, Q1, is the middle value in the first half or the dataset. Q1 refers to the 25th percentile. 25% of the values in the entire dataset are below Q1, and 75% are above it.
The second quartile, Q2, is the median of the dataset. Q2 refers to the 50th percentile. 50% of the values in the entire dataset are below Q2, and 50% are above it.
The third quartile, Q3, is the middle of the second half of the dataset. Q3 refers to the 75th percentile. 75% of the values in the entire dataset are below Q3, and 25% are above it.
Interquartile range (IQR)
The middle 50% of your data is called the interquartile range, or IQR. This is the distance between the first quartile (Q1) and the third quartile (Q3). This is the same distance between the 25th and 75th percentiles. IQR is useful for determining the relative position of your data values. For example, data values outside the interval Q1 - (1.5 * IQR) and Q1 + (1.5 * IQR) are often considered outliers.
Note: Technically, IQR is a measure of dispersion because it measures the spread of the middle half or middle 50% of your data (between Q1 and Q3). IQR is less sensitive to outliers than the range because it doesn't include the more extreme values in your dataset.
Five number summary
Finally, you can summarize the major divisions in your dataset with the five number summary. They include:
The minimum, the first quartile (Q1), the median, or the second quartile (Q2), the third quartile (Q3), and the maximum.
Its useful because it gives you an overall idea of the distribution of your data, from the extreme values to the center. You can visualize this with a boxplot.
The boxplot part of the box goes from Q1 to Q3. The vertical line in the middle of the box is the median (Q2). The horizontal lines on each side of the box, known as whiskers, go from Q1 to the minimum, and from Q3 to the maximum.
Key takeaways
Data professionals use measures of position such as percentiles and quartiles to better understand all types of data, from product sales to household income. Measures of position help you quickly identify the relative location of your data values, and give you a more precise sense of the distribution of your data.
