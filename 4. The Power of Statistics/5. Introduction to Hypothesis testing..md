### Introduction
Hypothesis testing helps data professionals determine if the results of a test or experiment are statistically significant or due to chance. Here, you'll learn:
- Basic steps for any hypothesis test.
- How hypothesis test can help you draw meaningful conclusions about data.
#### Welcome to module 5
Hypothesis testing is a statistical procedure that uses sample data to evaluate an assumption about a population parameter. For example, hypothesis tests are often used in clinical trials to determine whether a new medicine leads to better outcomes in patients.
Imagine a pharmaceutical company invents a medicine to treat the common cold. The company tests a random sample of 200 people with cold symptoms. Without medicine, the typical person experiences cold symptoms for 7.5 days. The average recovery time for people who take the medicine is 6.2 days. 
The company might then ask, are the results of the clinical trial statistically significant? Recall that statistical significance is the claim that the results of a test or experiment are not explainable by chance alone. In other words, did the drug actually have a positive impact on recovery time? Or are the results due to chance or sampling variability?
To answer these questions, the company may ask a data professional to conduct a hypothesis test. The test helps quantify whether the result is likely due to chance or if it's statistically significant. This knowledge will help the company determine if the drug is truly effective and if it should be approved for public use.
Coming up, we'll learn the general procedure for hypothesis testing, then we'll explore different types of hypothesis tests, one-sample and two-sample tests. Then we'll use python's stats module to conduct a two-sample hypothesis test to compare two population means.
#### Introduction to hypothesis testing
The steps for performing a hypothesis test.
- State the Null and Alternative hypothesis.
- Choose a significance level.
- Find the p-value.
- Reject or fail to reject the Null hypothesis.
Let's explore an example:
Imagine you are given a coin to use in a game. You are not sure if the coin is fair or rigged. That is, you don't know whether it's a standard coin or if its been specifically weighted to affect the outcome of a toss, for example, to always land on tails. Before using it in the game, you want to find out that the coin is fair or not. You decide to test the coin by tossing it six times in a row and recording the outcomes. In probability, if the coin is fair, the chance of landing on heads or tails is 0.5 or 50% for any given toss. If the coin is rigged for tails, the chance of landing on tails for any given toss will be much higher, perhaps 90 or even 100 percent. 
Before you test it out, you need to have a benchmark to evaluate the results of the test. For example, let's say the first two tosses land on tails, is the coin rigged? Recall that we use multiplication rule to calculate the probability of independent events. The probability of the coin landing on tails two times in a row is 0.25 or 25%. That's not unlikely. At this point, you can't reasonably conclude that the coin is rigged. 
Now, imagine the coin lands on tails four times in a row. The probability of this occurring is 0.5 multiplied 4 times which is 0.0625, or 6.25%. That's unlikely but not impossible. 
However, you want to feel more confident that the outcome is not due to chance. You decide to use 5% as a threshold to determine if the outcome is due to chance. In other words, if the probability of the outcome is less than 5% under the assumption that the coin is fair, you'll conclude that the coin is actually rigged.
For example, the probability for a fair coin to land on tails six times in a row is 0.5 multiplied six times which is 0.0156 or 1.56%. This is too unlikely because the likelihood is less than your threshold of 5%. If this occurs, you will conclude the coin is rigged.
Now, you are ready to proceed with your test. You toss the coin six times in a row and record the results. The coin lands on tails every time, you conclude that the coin is rigged. Unfortunately, the coin won't be of much use to you unless you're performing the magic trick and happened to need a coin that always lands on tails. This example is a simplified version of a hypothesis test. 
To test out whether or not the coin is fair, you went through each step of the hypothesis testing procedure.
###### Let's review the steps for conducting a hypothesis test.
1. First, state the null hypothesis and the alternative hypothesis.
Null hypothesis is a statement that is assumed to be true unless there is convincing evidence to the contrary. It typically assumes your observed data occur by chance.
Alternative hypothesis is a statement that contradicts the null hypothesis, and is accepted as true if there is convincing evidence of it. It typically assumes that the observed data does not occur by chance. 
In our example, you null hypothesis states that the coin is fair. Having a fair coin is the standard or typical state of things. The null hypothesis states that your observations result purely from chance. Your alternative hypothesis states the contrary claim, the coin is not fair. The alternative hypothesis says that the outcome was the result of rigging and did not happen by chance.
2. Second, choose a significant level.
This is the threshold at which you will consider results statistically significant. The significance level is also the probability of rejecting the null hypothesis when it is true.
In our example, you use 5 percent as a threshold to determine if the outcome of the coin toss occurred by chance. Typically, data professionals set the significance level at five percent. This is just a choice based on tradition and statistical research in education. You can adjust the significance level to meet the requirements of your analysis. Other common choices are 1 percent and 10 percent.
3. Third, find the p-value.
P-value refers to the probability of observing results as or more extreme than those observed when the null hypothesis is true. 
We already calculated that the probability of fair coin landing on tails six times in a row is 1.56 percent. If you assume the null hypothesis is true and the coin is fair, then the p-value in our example is 1.56 percent. Anything lower than that would mean there is stronger evidence for the alternative hypothesis. Remember, your alternative hypothesis is that the coin is not fair. 
For instance, the probability of tossing seven tails in a row is 0.79 percent, which is lower than the p-value of 1.56 percent. If you toss seven tails in a row, you'd have even stronger evidence for the alternative hypothesis that the coin is not fair. 
4. Fourth, and finally, you have to decide whether to reject or fail to reject the null hypothesis.
Statisticians always say, fail to reject rather than accept. This is because hypothesis tests are based on probability, not certainty. Acceptance implies certainty. In general, as data professionals, we try not to claim certainty about results based on statistical methods.
There are two main rules for drawing a conclusion about hypothesis test. 
- If your p-value is less than your significance level, you reject the null hypothesis.
- If your p-value is greater than your significant level, you fail to reject the null hypothesis. 
The example of the coin toss, your p-value of 1.56 percent is less than your significance level of 5 percent.
Conclusion: You reject the null hypothesis and conclude that your result of 6 consecutive tails is statistically significant and not due to chance.
Your decision to reject or fail to reject also depends on the significance level. Let's say, before your test, you chose a significance level of 1 percent instead of than 5 percent. In that case, you would fail to reject the null hypothesis because your p-value of 1.56 percent would be greater than your significance level of 1 percent.
A statistically significant result cannot prove with 100 percent certainty that our hypothesis is correct. Because hypothesis testing is based on probability, there's always a chance of drawing the wrong conclusion about the null hypothesis. 
In hypothesis testing, there are two types of errors you can make when drawing a conclusion;
- a Type I error
- a Type II error. 
A Type I error, also known as a false positive, occurs when you reject a null hypothesis that is actually true. In other words, you conclude that your result is statistically significant when in fact it occurred by chance. In our example, concluding that the coin is rigged when it's actually fair would be considered a Type I error. Even though you've got six tails in a row, this outcome could still be due to chance. It's highly unlikely, but it's possible. 
Your significance level is also the probability of rejecting the null hypothesis when it is true. A significance level of 5 percent means you are willing to accept a 5 percent chance you are wrong when you reject the null hypothesis. To reduce your chance of making a Type I error, choose a lower significance level.
Recall that if you choose a 1 percent significance level, you fail to reject the null hypothesis and conclude that the coin is fair. However, choosing a lower significance level means you are more likely to make a Type II error or a false negative. This occurs when you fail to reject a null hypothesis which is actually false. In other words, you conclude your results occurred by chance when it's in fact statistically significant. In our example, you would conclude that the coin is fair when it's actually rigged.
As a data professional, it helps to be aware of the potential errors built into hypothesis testing and how they can affect your results. Depending on the situation and the goal of your analysis, you may want to minimize the risk of either a Type I or Type II error.
Imagine you're testing the strength of a parachute manufacturer. You want to be very confident that the material you're using is strong enough for a functional parachute. A type I error or a false positive means you falsely identify the material as strong enough. Obviously, in this case, you want to minimize the risk of a Type I error. To do so, choose a significance level of 1 percent instead of the standard 5 percent. This change decreases the chance of a Type I error or false positive from 5 - 1 percent. Ultimately, its your responsibility as a data professional to determine how much evidence you need to decide that a result is statistically significant. How risky is a Type I error or a false positive? There is no single correct answer for all of these situations. It's up to you to decide.
The coin toss example shows you the main concepts involved in conducting a hypothesis test. As a data professional you'll use these concepts for any hypothesis test that you may want to conduct.
#### Differences between Null and Alternative hypothesis
Recently, you learned that hypothesis testing uses sample data to evaluate an assumption about a population parameter. Data professionals conduct a hypothesis test to decide whether the evidence from their sample data supports either the null hypothesis or the alternative hypothesis. Here, we will go over the main differences between the null and the alternative hypothesis, and how to formulate each hypothesis in different scenarios.
###### Statistical hypotheses
Let's review the steps for conducting a hypothesis test:
1. State the null hypothesis and the alternative hypothesis.
2. Choose a significance level.
3. Find the p-value.
4. Reject or fail to reject the null hypothesis.
The first step for any hypothesis test is to state the null and alternative hypothesis. The null and alternative hypothesis are mutually exclusive, meaning they cannot both be true at the same time.
The null hypothesis is a statement that is assumed to be true unless there is convincing evidence to the contrary. The null hypothesis typically assumes that there is no effect in the population, and that your observed data occurs by chance.
The alternative hypothesis is a statement that contradicts the null hypothesis, and is accepted as true only if there is convincing evidence for it. The alternative hypothesis typically assumes that there is an effect in the population, and that your observed data does not occur by chance. 
Note: The null and alternative hypotheses are always claims about the population. That's because the aim of hypotheses testing is to make inferences about a population based on a sample.
For example, imagine you're a data professional working for a car dealership. The company implements a new sales training program for their employees. They ask you to evaluate the effectiveness of the program.
- Your null hypothesis (H0): the program has no effect on sales revenue.
- Your alternative hypothesis (Ha): the program increased sales revenue.
Let's explore each hypothesis in more detail.
###### Null hypothesis
The null hypothesis has the following characteristics:
- In statistics, the null hypothesis is often abbreviated as H sub zero (H0).
- When written in mathematical terms, the null hypothesis always includes an equality symbol (usually =, but sometimes <= or >=).
- Null hypotheses often include phrases such as 'no effect,' 'no difference,' 'no relationship,' or 'no change.'
###### Alternative hypothesis
The alternative hypothesis has the following characteristics:
- In statistics, the null hypothesis is often abbreviated as H sub a (Ha).
- When written in mathematical terms, the null hypothesis always includes an inequality symbol (usually !=, but sometimes < or >).
- Null hypotheses often include phrases such as 'an effect,' 'a difference,' 'a relationship,' or 'a change.'
###### Example scenario
Typically, the null hypothesis represents the status quo, or the current state of things. The null hypothesis assumes that the status quo hasn't changed. The alternative hypothesis suggests a new possibility or different explanation. Let's check some examples to get a better idea of how to write the null and alternative hypotheses for different scenarios.
###### Example 1: Mean weight
An organic food company is famous for their granola. The company claims each bag they produce contains 300 grams of granola --no more no less. To test this claim, a quality control expert measures the weight of a random sample of 40 bags.
- Null hypothesis (H0): u = 300 (the mean weight of all produced granola bags is equal to 300 grams)
- Alternative hypothesis (Ha): u != 300 (the mean weight of all produced granola bags is not equal to 300 grams)
###### Example 2: Mean height
Suppose it's assumed that the mean height of a certain species of tree is 30 feet tall. However, one ecologist claims the actual mean is greater than 30 feet. To test this claim, the ecologist measures the height of a random sample of 50 trees.
- Null hypothesis (H0): u <= 30 (the mean height of this species of tree is equal to or less than 30 feet)
- Alternative hypothesis (Ha): u > 30 (the mean height of this species of tree is greater than 30 feet)
###### Example 3: Proportion of employees
A corporation claims that at least 80% of all employees are satisfied with their job. However, an independent researcher believes that less than 80% of all employees are satisfied with their job. To test this claim, the researcher surveys a random sample of 100 employees.
- Null hypothesis (H0): p >= 0.80 (the proportion of all employees who are satisfied with their job is equal to or greater than 80%)
- Alternative hypothesis (Ha): p < 0.80 (the proportion of all employees who are satisfied with their job is less than 80%)
Key takeaways
The null and alternative hypothesis testing are foundational concepts in hypothesis testing. To conduct an effective hypothesis test, it's important to understand the differences between the null and alternative hypothesis, and how to properly state each hypothesis.
#### Type I and Type II errors
Earlier, you learned that you can use a hypothesis test to help determine if your results are statistically significant, or if they occurred by chance. However, because hypothesis testing is based on probability, there's always a chance of drawing the wrong conclusion about the null hypothesis. In hypothesis testing, there are two types of errors you can make when drawing a conclusion: a Type I error and a Type II error. Here, we will discuss the difference between them and the risks involved in making each error.
###### Errors in statistical decision-making
Let's review the steps for conducting a hypothesis test:
1. State the null hypothesis and the alternative hypothesis.
2. Choose a significance level.
3. Find the p-value.
4. Reject or fail to reject the null hypothesis.
When you decide to reject or fail to reject the null hypothesis, there are four possible outcomes--two represent correct choices, and two represent errors. You can:
- Reject the null hypothesis when it's actually true (Type I error)
- Reject the null hypothesis when it's actually false (Correct)
- Fail to reject the null hypothesis when it's actually true (Correct)
- Fail to reject the null hypothesis when it's actually false (Type II error)
###### Example: Clinical trial
Let's explore an example to get a better understanding of Type I and Type II errors. Hypothesis tests are often used in clinical trials to determine whether a new medicine leads to better outcomes in patients.
Imagine you are a data professional who works for a pharmaceutical company. The company invents a new medicine to treat the common cold. The company tests a random sample of 200 people with cold symptoms. Without medicine, the typical person experiences cold symptoms for 7.5 days. The average recovery time for people who take the medicine is 6.2 days.
You conduct a hypothesis test to determine if the effect of the medicine on recovery time is statistically significant, or due to chance. In this case:
- Your null hypothesis (H0) is that the medicine has no effect.
- Your alternative hypothesis (Ha) is that the medicine is effective.
###### Type I error
Also known as a false positive, occurs when you reject a null hypothesis that is actually true. In other words, you conclude that your result is statistically significant when in fact it occurred by chance.
For example, in your clinical trial, if the null hypothesis is true, that means the medicine has no effect. If you make a Type I error and reject the null hypothesis, you incorrectly conclude that the medicine relieves cold symptoms when its actually ineffective.
The probability of making a Type I error is called alpha. Your significance level, or alpha, represents the probability of making Type I error. Typically, the significance level is set at 0.05, or 5%. A significance level of 5% means you are willing to accept a 5% chance you are wrong when you reject the null hypothesis.
###### Reduce your risk
To reduce your chance of making a Type I error, choose a lower significance level.
For instance, if you want to minimize the risk of a Type I error, you can choose a significance level of 1% instead of a standard 5%. This change reduces the chance of making a Type I error from 5% to 1%.
###### Type II error
However, reducing your risk of making a Type I error means you are more likely to make a Type II error, or false negative. A Type II error occurs when you fail to reject a null hypothesis which is actually false. In other words, you conclude your results occurred by chance, when in fact it didn't.
For example, in your clinical study, if the null hypothesis is false, this means that the medicine is effective. If you make a Type II error and fail to reject the null hypothesis, you incorrectly conclude that medicine is ineffective when it actually relieves cold symptoms.
The probability of making a Type II error is called Beta. And Beta is related to the power of a hypothesis test (power = 1- Beta). Power refers to the likelihood that a test can correctly detect a real effect when there is one.
###### Reduce your risk
You can reduce your risk of making a Type II error by ensuring your test has enough power. In data work, power is usually set at 0.80 or 80%. The higher the statistical power, the lower the probability of making a Type II error. To increase power, you can increase you sample size or your significance level.
###### Potential risks of Type I and Type II errors
As a data professional, it's important to be aware of the potential risks involved in making the two types of errors.
A Type I error means rejecting a null hypothesis which is actually true. In general, making a Type I error often leads to implementing changes that are unnecessary and ineffective, and which waste valuable time and resources.
For example, if you make a Type I error in your clinical trial, the new medicine will be considered affective even though it's actually ineffective. Based on this incorrect conclusion, an ineffective medication may be prescribed to a large number of people. Plus, other treatment options may be rejected in favor of the new medicine.
A Type II error means failing to reject a null hypothesis which is actually false. In general, making a Type II error may result in missed opportunities for positive change and innovation. A lack of innovation can be costly for people and organizations.
For example, if you make a Type II error in your clinical trial, the new medicine will be considered ineffective even though its actually effective. This means that a useful medication may not reach a large number of people who could benefit from it.
Key takeaways
As a data professional, it helps to be aware of the potential errors built into hypothesis testing and how they can affect your results. Depending on the specific situation, you may choose to minimize the error of either a Type I or Type II error. Ultimately, its your responsibility as a data professional to determine which type of error is riskier based on the goals of your analysis.
### One-sample tests
###### One-sample test for means
There are two different types of hypothesis tests:
- One-sample test - determines whether or not a population parameter, like a mean or proportion, is equal to a specific value.
- Two-sample test - determines whether or not two population parameters, such as two means or two proportions, are equal to each other. 
A data professional might conduct a one-sample hypothesis test to determine if:
- a company's average sales revenue is equal to a target value, 
- a medical treatment's average rate of success is equal to a set goal, or 
- a stock portfolio's average rate of return is equal to a market benchmark.
Here, we will explore an example involving data for an online delivery company. There are different types of hypothesis tests you can use based on the type of sample data you're working with. We will focus on z-tests and t-tests, two of the most commonly-used tests in data analytics. 
The one-sample z-test makes the following assumptions:
- The data is a random sample of a normally distributed population.
- The population standard deviation is known.
Imagine you're a data professional who works for an online delivery company. Typically, the mean delivery time for an online food order is 40 minutes with a standard deviation of five minutes. But recently, company management launched a new training program to make the delivery process more efficient.
After delivery drivers completed the training program, management tracked a random sample of 50 deliveries to understand how long a delivery takes. The sample of 50 deliveries had a mean delivery time of 38 minutes with a standard deviation of five minutes.
There is an observed difference of 2 minutes between the population mean of 40 minutes and the sample mean of 38 minutes. The management team asks you to determine if the decrease in average delivery time is statistically significant or if it's due to chance. If the decrease is statistically significant, the company wants to invest in developing and implementing the training program in other regions.
You decide to conduct a one-sample z-test to analyze the data. Let's review the steps for conducting a hypothesis test:
- State the null hypothesis and alternative hypothesis
In a one-sample z-test, the null hypothesis states that the population mean is equal to an observed value. In this case, the average delivery time equals 40 minutes. 40 minutes is the standard average delivery time. 
In a one-sample test, there are three main options for the alternative hypothesis: the population mean is not equal to, less than, or greater than an observed value. In this case, you want to test whether the training has decreased the average delivery time. Your alternative hypothesis says the average delivery time is less than 40 minutes.
- Choose a significance level
Here, set the significance level or the threshold at which you will consider our results statistically significant. This is also the probability of rejecting the null hypothesis when it is true. You choose a significance level of 5%, which is the company's standard for data analysis. 
- Find the p-value
Now, you find the p-value. Recall that your p-value is the probability of observing a difference in your results as or more extreme than the difference observed when the null hypothesis is true. 
Typically, the mean delivery time is 40 minutes. The mean delivery time for your sample is 38 minutes. Your null hypothesis claims that this  minute difference in delivery time is due to chance or sampling variability.
Your p-value is the probability of observing a difference that is 2 minutes or greater if the null hypothesis is true. If the probability of this outcome is very unlikely, in particular, if your p-value is less than your significant level of five percent, then you'll reject the null hypothesis.
As a data professional, you'll almost always calculate p-value on your computer using a programming language like Python or other statistical software. Being able to use code for calculations is important for your future career. But being familiar with the concepts behind the calculations will help you apply statistical methods to workplace problems. The p-value is calculated from a test statistic.
In hypothesis testing, the test statistic is a value that shows how closely your observed data matches the distribution expected under the null hypothesis, so if you assume the null hypothesis is true and the mean delivery time is 40 minutes, the data for delivery times follows a normal distribution. The test statistic shows where your observed data, a sample mean delivery time of 38 minutes, will fall on that distribution. Since you're conducting a z-test, your test statistic is a z-score. 
Recall that a z-score is a measure of how many standard deviations below or above the population mean a data point is. Z-scores tell you where your values lie on a normal distribution. Using the formula of the z-score, if you enter the numbers in the formula, you'll get a z-score of -2.82 (negative). This z-score lies far to the left, almost 3 standard deviations below the mean.
For a normal distribution, the probability of getting a value less than your z-score of -2.82 is calculated by taking the area under the curve to the left of the z-score. This is called a left-tailed test because your p-value is located on the left tail of the distribution. The area under this part is the same as your p-value. Again, your p-value is the probability of observing a test statistic as more or extreme than that observed when the null hypothesis is true. 
Your alternative hypothesis states that the mean delivery time decreased based on your sample data. That's why we are interested in the probability of getting any value lower than your z-score of -2.82.
- Reject or fail to reject the null hypothesis
To draw a conclusion about your null hypothesis, compare your p-value with the significance level.
- If your p-value is less than the significance level, you can conclude that there is a statistically significant difference in mean delivery times. In other words, you reject the null hypothesis. 
- If your p-value is greater than the significance level, you conclude that there is not a statistically significant difference in mean delivery time. In other words, you fail to reject the null hypothesis.
Your p-value of 020023 or 0.23 percent is less than the significance level of 0.05 or 5 percent, so you reject the null hypothesis and conclude that there is statistically significant difference in mean delivery time. Your results suggest that the faster delivery time is likely due to the positive effects of the training. Your analysis will help the company leadership decide whether to make a bigger investment in the training program going forward. Based on your results, they're likely to do so.
#### Determine if data has statistical significance
Statistical significance is the claim that the results of a test or experiment are not explainable by chance alone. A hypothesis test can help you determine whether your observed data is statistically significant, or likely due to chance. 
For example, in a clinical trial of a new medication, a hypothesis test can help determine if the medication's positive effect on a sample group is statistically significant, or due to chance. 
##### Statistical significance in hypothesis testing
Data professionals use hypothesis testing to determine whether a relationship between variables or a difference between groups is statistically significant. Let's explore an example to get a better understanding of the role of statistical significance in hypothesis testing.
### Example: Mean battery life
Let's review the steps for conducting a hypothesis test:
1. State the null hypothesis and the alternative hypothesis.
2. Choose a significance level.
3. Find the p-value.
4. Reject or fail to reject the null hypothesis.
Imagine you're a data professional working for a computer company The company claims the mean battery life for their best selling laptop is 8.5 hours with a standard deviation of 0.5 hours. Recently, the engineering team redesigned the laptop to increase the battery life. The team takes a random sample of 40 redesigned laptops. The sample mean is 8.7 hours.
The team asks you to determine if the increase in the mean battery life is statistically significant, or its due to random chance. You decide to conduct a z-test to find out. 
###### Step 1. State the null hypothesis and alternative hypothesis
The null hypothesis typically assumes that your observed data occurs by chance, and it is not statistically significant. In this case, your null hypothesis says that there is no actual effect on mean battery life in population of laptops.
The alternative hypothesis typically assumes that your observed data does not occur by chance, and is statistically significant. In this case, your alternative hypothesis says that there is an effect on mean battery life in the population of the laptops.
In this example, you formulate the following hypothesis:
- H0: u = 8.5 (the mean battery life for all redesigned laptops is equal to 8.5 hours)
- Ha: u > 8.5 (the mean battery life for all redesigned laptops is greater than 8.5 hours)
###### Step 2. Choose a significance level
The significance level, or alpha (a), is the threshold at which you will consider a result statistically significant. This is also the probability of rejecting the null hypothesis when it is true.
Typically, data professionals set the significance level at 0.05, or 5%. That means results at least as extreme as yours only have a 5% chance (or less) of occurring when the null hypothesis is true. 
Note: 5% is a conventional choice, and not a magical number. It's based on tradition in statistical research and education. Other choices are 1% and 10%. You can adjust the significance level to meet the specific requirements of your analysis. A lower significance level means an effect has to be larger to be considered statistically significant.
Pro tip: As a best practice, you should set a significance level before you begin your test. Otherwise, you might end up in a situation where you are manipulating the results to suit your convenience.
In this example, you choose a significance level of 5%, which is the company's standard for research. 
###### Step 3: Find the p-value.
P-value refers to the probability of observing results as or more extreme than those observed when the null hypothesis is true.
Your p-value helps you determine whether a result is statistically significant. A low p-value indicates high statistical significance, while a high p-value indicates low or no statistical significance.
Every hypothesis test features:
- A test statistic that indicates how closely your data match the null hypothesis. For a z-test, your test statistic is a z-score; for a test, it's a t-score.
- A corresponding p-value that tells you the probability of obtaining a result at least as extreme as the observed result if the null hypothesis is true.
As a data professional, you'll almost always calculate the p-value on your computer, using a programming language like Python or statistical software. In this example, you're conducting a z- or test, so your test statistic is a z-score of 2.53. Based on this test statistic, you calculate a p-value of 0.0057, or 0.57%.
###### Step 4: Reject or fail to reject the null hypothesis
In a hypothesis test, you compare your p-value to your significance level to decide whether your results are statistically significant. There are two main rules for drawing a conclusion about a hypothesis test:
- If your p-value is less than your significance level, you reject the null hypothesis.
- If your p-value is greater than your significance level, you fail to reject the null hypothesis.
Note: Data professionals and statisticians always say 'fail to reject' rather than 'accept'. This is because hypothesis tests are based on probability, not certainty --acceptance implies certainty. In general, data professionals avoid claiming certainty about results based on statistical methods.
In this example, your p-value of 0.57% is less than your significance level of 5%. Your test provides sufficient evidence to conclude that the mean battery life of all redesigned laptops has increased from 8.5 hours. You reject the null hypothesis. You determine that your results are statistically significant.
Key takeaways
As a data professional, its important to understand the concept of statistical significance to effectively conduct a hypothesis test and interpret the results. Insights based on statistically significant results can help stakeholders make more informed business decisions.
### Two-sample tests.
###### Two-sample means
While a one-sample test determines whether a population mean is equal to a specific value, a two-sample test determines whether two population means are equal to each other. In data analytics, two-sample tests are frequently used for A/B testing. 
Data professionals use conduct A/B tests to help companies improve their online business. Typically, they use two-sample t-test to analyze the data. In data analytics, the two-sample t-test is the standard approach for comparing two means. The two-sample t-test for means makes the following assumptions:
- The two samples are independent of each other.
- For each sample, the data is drawn randomly from a normally distributed population.
- The population standard deviation is unknown.
Typically, data professionals use a z-test when the population standard deviation is known, and use a t-test when the population standard deviation is unknown and needs to be estimated from the data.
In practice, the population standard deviation is usually unknown because it's difficult to get complete data on large populations. So data professionals use the t-test for practical applications. While the test statistic for a z-test is a Z-score, the test statistic for a t-test is a T-score. And while Z-scores are based on the standard normal distribution, T-scores are based on the t-distribution. The graph of the t-distribution has a bell shape that is similar to the standard normal distribution, but the t-distribution has bigger tails than the standard normal distribution does.
The bigger tails indicate the higher frequency of outliers that come with small dataset. As the sample size increases, the t-distribution approaches the normal distribution. For a t-test, the test statistic follows a t-distribution under the null hypothesis.
Lets explore how to conduct a two-sample t-test.
Imagine you are a data professional who works for a cosmetics company. The company is researching the amount of time customers spent on its website. Your team leader asks you to conduct an A/B test to determine if changing the background color of the landing page from gray to green has any effect on the average time spent on the page. You randomly select two groups of users. The first group visits the gray landing page named version A. The second group visits the green landing page named version B.
You collect the following data from the A/B test. 40 users visit version A. They spend a mean time of 300 seconds with a standard deviation of 18.5 seconds. 38 users visit version B. They spend a mean time of 305 seconds, with a standard deviation of 16.7 seconds.
There is an observed difference of 5 seconds between the mean time spent on version B and version A. You decide to conduct a two-sample t-test to analyze the data.
Let's review the steps for conducting a hypothesis test:
1. State the null hypothesis and the alternative hypothesis.
In a two-sample t-test, the null hypothesis states that there is no difference between the two population means. This is assumed to be true unless there is convincing evidence to the contrary. Null: There is no difference in the mean time spent on version A and version B.
Your alternative hypothesis will state the contrary claim. Alternative: there is a difference in the mean time spent on version A and version B.
2. Choose a significance level.
Set the threshold at which you will consider a result statistically significant. This is the probability of rejecting the null hypothesis when it is true. You choose a significant level of 5% which is the company's standard for A/B testing.  
3. Find the p-value.
Your p-value is the probability of observing a difference in your sample means as or more extreme than the difference observed when the null hypothesis is true.
Based on your sample data, the difference between the mean time spent on version A and version B is 5 seconds. Your null hypothesis claims that this difference is due to chance. Your p-value is the probability of observing an absolute difference in sample means that is 5 seconds or greater, if the null hypothesis is true. If the probability of this outcome is very unlikely. In particular, if your p-value is less than your significance level of 5%, then you will reject the null hypothesis.
As a data professional, you'll almost always calculate p-value on your computer using a programming language like python or other statistical software.
To find your p-value, first calculate your test statistic. Since you're conducting a t-test, you're working with a T-score. Using a formula, you'll get a test statistic of -1.2508. For a t-test, the test statistic follows a t-distribution under the null hypothesis. Recall that your alternative hypothesis states that there is a difference in the mean time spent on version A and version B. The observed difference is five seconds. So if you find a statistically significant difference between the means, either less than or greater than the observed difference of five seconds, you will reject the null hypothesis.
Because you are interested in values in both directions, either less than or greater than your test statistic, your p-value is the probability of getting a value less than the T-score -1.2508 or greater than the T-score 1.2508. Your p-value corresponds to the area under the curve on both the left tail and the right tail of the distribution. This is called a two-tailed test. If you calculate the p-value you'll find that it's 0.2148 or 21.48%. This means that there is a 21.48% probability that the difference between this mean time spent on version A and version B would be five seconds or greater, if the null hypothesis is true.
4. Reject or fail to reject the null hypothesis.
 To draw a conclusion, compare your p-value with the significance level. 
- If your p-value is less than the significance level, you conclude that there is a statistically significant difference in the means between the two versions. In other words, you reject the null hypothesis that there is no difference in the mean time spent on version A and version B.
- If your p-value is greater than the significance level, you conclude that there is not a statistically significant difference between the two versions. In other words, you fail to reject the null hypothesis that there is no difference in the mean time spent on version A and version B. 
Your p-value of 0.2148 or 21.48 %, is greater than the significance level of 0.05 or 5%. So, you fail to  conclude that there is not a statistically significant difference between the mean time spent on version A and version B. In other words, the observed difference in mean time spent is likely due to chance. Your analysis will help the company decide how to redesign their website. 
Since there is not a statistically significant difference in mean time spent based on the background colors of gray and green, you can recommend the company either test for a different color, such as blue or yellow or test for a different design feature such as text size or button shape. Perhaps a different design change will have an impact on the time spent by customers on landing page.
#### One-tailed and two-tailed tests
A hypothesis test can be either one-tailed or two-tailed. A tail in hypothesis testing refers to the tail at either end of a distribution curve. Here, we'll go over the main differences between one-tailed and two-tailed tests, and discuss the procedure of conducting each test.
##### The differences between one-tailed and two-tailed tests
A one-tailed test results when the alternative hypothesis states that the actual value of a population parameter is either less than or greater than the value in the null hypothesis. A one-tailed test may either be left-tailed or right-tailed. A left-tailed test results when the alternative hypothesis states that the actual value of the parameter is less than the value of the null hypothesis. A right-tailed test results when the alternative hypothesis states that the actual value of the parameter is greater than the value in the null hypothesis.
A two-tailed test results when the alternative hypothesis states that the actual value of the parameter does not equal the value in the null hypothesis.
For example, imagine a test in which the null hypothesis states that the mean weight of a penguin population equals 30 lbs. (pounds). 
- In a left-tailed test, the alternative hypothesis might state that the mean weight of the penguin population is less than ("<") 30 lbs.
- In a right-tailed test, the alternative hypothesis might state that the mean weight of the penguin population is greater than (">") 30 lbs.
- In a two-tailed test, the alternative hypothesis might state that the mean weight of the penguin population is not equal to 30 lbs.
Let's explore a more detailed example to get a better understanding of the difference between one-tailed and two-tailed tests.
###### Example: One-tailed tests
Imagine you are a data professional working for an online retail company. The company claims that at least 80% of its customers are satisfied with their shopping experience. You survey a random sample of 100 customers. According to the survey, 73% of the customers say they are satisfied. Based on your sample data, you conduct a z-test to evaluate the claim that at least 80% of customers are satisfied. 
Let's review the steps for conducting a hypothesis test:
1. State the null hypothesis and the alternative hypothesis.
- H0: P >= 0.80 (the proportion of satisfied customers is greater than or equal to 80%)
- Ha: P < 0.80 (the proportion of satisfied customers is less than 80%)
Note: This is a one-tailed test as the alternative hypothesis contains the less than sign ("<").
1. Choose a significance level.
Next, you choose a significant level of 0.05, or 5%. 
2. Find the p-value.
Then, you calculate your p-value based on your test statistic. Recall that p-value is the probability of observing results as or more extreme than those observed when the null hypothesis is true. In the context of hypothesis testing, "extreme" means extreme in the direction(s) of the alternative hypothesis.
Your test statistic is a z-score of 1.75 and your p-value is 0.04.
Since this is a left tailed test, the p-value is the probability that the z-score is less than 1.75 standard units away from the mean to the left. In other words, it's the probability that the z-score is less than -1.75. The probability of getting a value less than your z-score of -1.75 is calculated by taking the area under the distribution curve to the left of the z-score. This is called a left-tailed test because your p-value is located to the left tail of the distribution. The area under this part of the curve is the same as your p-value: 0.04.
3. Reject or fail to reject the null hypothesis.
Finally, you draw a conclusion. Since your p-value of 0.04 is less than your significance level of 0.05, you reject the null hypothesis.
Note: In a different testing scenario, your test statistic might be positive 1.75, and you might be interested in values as great or greater than the z-score 1.75. In that case, your p-value would be located on the right tail of the distribution, and you'd be conducting a right-tailed test.
###### Example: Two-tailed tests
Now, imagine our previous example has a slightly different set up. Suppose the company claims that 80% of its customers are satisfied with their shopping experience. To test this claim, you survey a random sample of 100 customers. According to the survey, 73% of the customers say they are satisfied. Based on the survey data, you conduct a z-test to evaluate the claim that 80% of customers are satisfied.
First, you state the null and alternative hypothesis:
- H0: P = 0.80 (the proportion of satisfied customers equals to 80%)
- Ha: P != 0.80 (the proportion of satisfied customers does not equal 80%)
Note: This is a two-tailed test as the alternative hypothesis contains the not equals sign ("!=")
Next, you choose a significant level of 0.05, or 5%. 
Then, you calculate your p-value based on your test statistic. Your test statistic is a z-score of 1.75. Since this is a two-tailed test, the p-value is the probability that the z-score is less than -1.75 or greater than 1.75. Note that the p-value for a two-tailed test is always two times the p-value for a one-tailed test. So, in this case, your p-value = 0.04 + 0.04 = 0.08. In a two-tailed test, your p-value corresponds to the area under the curve on both the left and right tail of the distribution.
Finally, you draw a conclusion. Since your p-value of 0.08 is greater than your significance level of 0.05, you fail to reject the null hypothesis.
###### One-tailed versus two-tailed
You can use one-tailed and two-tailed tests to examine different effects.
In general, a one-tailed test may provide more power to detect an effect in a single direction. However, before conducting a one-tailed test, you should consider the consequences of missing an effect in the other direction. For example, imagine a pharmaceutical company develops a new medication they believe is more effective than an existing medication. As a data professional analyzing the results of the clinical trial, you my wish to choose a one-tailed test to maximize your ability to detect the improvement. In doing so, you fail to test for the possibility that the new medication is less effective than the existing medication. And, of course, the company doesn't want to release a less effective medication to the public.
A one-tailed test may be appropriate if the negative consequences of missing an effect in the untested direction are minimal. For example, imagine that the company develops a new, less expensive medication that they believe is at least as effective as the existing medication. The lower price gives the new medication an advantage in the market. So, they just want to make sure the new medication an advantage is not less effective than the existing medication. Testing whether it's more effective is not a priority. In this case, a one-tailed test may be appropriate.
Key Takeaways
Understanding the differences between a one-tailed and a two-tailed test is an important part of conducting a hypothesis test. Depending on the context of your analysis, you may want to use a one-tailed test to examine effects in a single direction, or a two-tailed test to examine effects in both directions.
#### Two-sample tests: Proportions
Here, you will conduct a two-sample z-test to compare two population proportions. We call that for technical reasons, t-tests do not apply to proportions. A data professional might use a two-sample z-test to compare the proportion of:
- Defects among manufacturing products on two assembly lines.
- Side effects to a new medicine for two trial groups.
- Support for a new law among voters in two districts.
Let's explore an example. Imagine you are a data professional working for an international construction company. The company has offices in London and Beijing. The human resources team would like to determine whether there is a difference in the level of employee satisfaction between the Beijing office and the London office. The team surveys a random sample of 50 employees in each office to discover if they are satisfied with their current job. They ask you to find out if there is a statistically significant difference in the proportion of satisfied employees in London and in Beijing. If so the HR team will devote resources to investigating why employees at one office are more satisfied at work.
According to the survey results, 67% of the employees in the London office report being satisfied with their job, and 57% of the employees in the Beijing office report being satisfied with their job. There's an observed difference of 10 percentage points or 67 - 57, between the proportion of satisfied employees in London and Beijing. 
You decide to conduct a two-sample z-test to analyze the data. Let's review the steps for conducting a hypothesis test:
1. State the null hypothesis and the alternative hypothesis.
In a two-sample z-test, the null hypothesis states that there is no difference between the proportions of your two groups. This is assumed to be true unless there is convincing evidence to the contrary. 
- For your null hypothesis, you say there is no difference in the proportion of satisfied employees in London and Beijing.
Your alternative hypothesis will state the opposite claim. 
- For your alternative hypothesis, you say there is a difference in the proportion of satisfied employees in London and in Beijing.
2. Choose a significance level.
Next, set the significance level, or the threshold at which you will consider a result statistically significant. This is the probability of rejecting the null hypothesis when it is true. 
You choose a significant level of 5%, which is the company's standard for employee surveys.
3. Find the p-value.
In this case, your p-value is the probability of observing a difference in your sample proportions as or more extreme than the difference observed when the null hypothesis is true. 
Based on your sample data, the difference between the proportion of satisfied employees in London and Beijing is 10 percentage points. Your null hypothesis claims that this difference is due to a chance. Your p-value is the probability of observing an absolute difference in sample proportions, that is 10 percentage points or greater if the null hypothesis is true. If the probability of the outcome is very unlikely. 
In particular, if your p-value is less than your significance level of 5%, then you will reject the null hypothesis. As a data professional, you'll almost calculate p-value on your computer using a programming language like Python or other statistical software.
To find your p-value, first, calculate your test statistic Z using a formula. If you enter the numbers in the formula and do the calculation, you get a z-score of 1.03.
4. Reject or fail to reject the null hypothesis.
For a z-test, the test statistic follows a normal distribution under the null hypothesis. Recall that your alternative hypothesis states that there is a difference in the proportion of satisfied employees in London and Beijing. The observed difference is 10 percentage points. 
If you find a statistically significant difference between the two proportions, either less than or greater than the observed difference of 10 percentage points, you will reject the null hypothesis. Because you are interested in values in both directions, either less than or greater than your test statistic, your p-value is the probability of getting a value less than the z-score of - 1.03 or greater than the z-score 1.03. Your p-value corresponds to the area under the curve on both the left tail and the right tail of the distribution. This is a two tailed test. If you calculate the p-value, you'll find that it's 0.3030 or 30 percent.
This means that there's a 30.3 percent probability that the absolute difference between the proportion of satisfied employees in London and Beijing would be 10 percent or greater if the null hypothesis is true. To draw a conclusion, compare your p-value with the significance level.
- If your p-value is less than the significance level, you conclude that there is a statistically significant difference in the proportions between the two groups. In other words, you reject the null hypothesis.
- If your p-value is greater than the significance level, you conclude that there is not a statistically significant difference in the proportions between the two groups. In other words, you fail to reject the null hypothesis.
Your p-value of 0.3030, or 30.3 percent is greater than the significance level of 0.05, or 5%. So you fail to reject the null hypothesis and conclude that there is not a statistically significant difference between the proportion of satisfied employees in London office and the Beijing office. In other words, the observed difference in proportions is likely due to chance.
Your analysis will help the human resources team save a lot of time and money, since there's no difference in employee satisfaction between the two offices, the HR team will not have to devote resources to investigating with the reasons behind the difference. However, they may want to find out how to make the employee satisfaction level a bit higher.
#### A/B testing
A/B testing is a way to compare two versions of something to find out which version performs better. For example, a data professional might use A/B testing to compare two versions of an online ad. You also learned that A/B testing utilizes statistical methods such as sampling and hypothesis testing.
In this reading, you'll learn more about the general purpose and design of an A/B test and how A/B testing uses statistical methods to analyze data.
###### Business context
Data professionals often use A/B testing to help stakeholders choose the best design for a website or app to optimize marketing, increase revenue, or enhance customer experience. In practice, A/B testing involves randomly selecting a sample of users and dividing them into two groups (A and B). The two groups visit different versions of a company's website. The two versions are identical except for a single design feature. For instance, the 'Purchase' button on Group A's version might have a different size, shape, or color than the 'Purchase' button on Group B's version. An A/B test uses statistical analysis to determine whether the change in the feature (e.g., a larger button) affects user behavior for a specific metric. A data professional might use an A/B test to analyze one of the following metrics:
- Average revenue per user: How much revenue does a user generate for a website?
- Average session duration: How long does a user remain on a website?
- Click rate: If a user is shown an ad, does the user click on it?
- Conversion rate: If a user is shown an ad, will that user convert into a customer?
Let's explore an example to get a better understanding of how A/B testing work.
###### Example: Average revenue per user
Imagine you're a data professional who works for an online footwear retailer. The company is trying to grow its business and is researching the average revenue per user on its website. Your team leader asks you to conduct an A/B test to determine whether increasing the size of the 'Purchase' button has any effect on average revenue. You randomly select a sample of users and divide them into two groups, A and B. Group A visits the standard version of the company website. Group B visits a version of the website that is identical to the standard version except for the larger 'Purchase' button. You run the test online and collect your sample data. The results indicate that average revenue per user is higher for Group B. Finally, you conduct a two-sample hypothesis test to determine whether the observed difference in average revenue is statistically significant or due to chance.
A typical A/B test has at least three main features:
- Test design
- Sampling
- Hypothesis testing
Let's examine each feature in more detail using our example.
#### Test design
First, let's design the fundamental design of an A/B test.
###### Randomized controlled experiment
An A/B test is a basic version of what's known as a randomized controlled experiment. In a randomized controlled experiment, test subjects are randomly assigned to a control group and a treatment group. The treatment is the new change being tested in the experiment. The control group is not exposed to the treatment. The treatment group is exposed to the treatment. The difference in treatment values between the two groups measures the treatment's effect on the test subjects.
Note: Ideally, exposure to the treatment is the only significant difference between the two groups. This test design allows researchers to control for other factors that might influence the test results and draw casual conclusions about the effect of the treatment.
In our example, group A is the control group, group B is the treatment group, and the treatment is displaying a larger 'Purchase' button. Users in the treatment group (B) visit an alternative version with a larger 'Purchase' button (i.e., are exposed to the treatment). By making the website versions for A and B identical, except for the size of the 'Purchase' button, you minimize the chance that any observed difference in average revenue is due to other features such as page layout or background. This allows you to measure the effect of the larger button by comparing the difference in average revenue per user for group A and group B.
Randomization, or randomly assigning test subjects to the control group or treatment group, also helps control the potential effect of the experiment. In practice, many different factors might influence whether a user clicks the 'Purchase' button or not. For example, perhaps super wealthy users are much more likely to make large purchases in general, regardless of button size. If your treatment group consists only of super wealthy users, you won't get valid test results. Any observed increase in average revenue might be due to wealth, not to the larger size of the 'Purchase' button (the factor you are interested in testing). Randomization helps minimize the chance that other factors, such as wealth, will significantly influence your results on average.
###### Sampling
Random selection helps you create a representative sample that reflects the characteristics of the overall user population. In our example, this is the population of online customers of the company you work for. Using a representative sample for your A/B test will give you valid results that are generalized, or applicable to the overall population.
You'll also need to choose a sample size that is appropriate for your A/B test. The larger the sample size, the more precise the results, and the more likely you'll get results that are statistically significant when there is a difference between group A and group B. However, working with large samples can be expensive and time-consuming. Data professionals determine sample size based on both the goal of the analysis and their available budget.
###### Hypothesis testing
For the purpose of our example, let's say you run the online test, collect your data, and discover that group B has higher average revenue per user than group A. Recall that group B is the treatment group (larger 'Purchase' button), and group A is the control group. The next step is to determine whether the observed difference in your data is statistically significant or due to chance. A/B tests use two-sample hypothesis tests to draw conclusions about statistical significance. To determine whether the observed difference in average revenue per user is statistically significant, you conduct a two-sample t-test. You formulate your hypotheses as follows:
- H0: There is no difference in average revenue per user between A and B.
- Ha: There is a difference in average revenue per user between A and B.
###### Results
Based on the results of your t-test, you reject the null hypothesis and conclude that the observed increase in average revenue per user is statistically significant.
The results of your A/B test helps you decide whether or not to recommend a design change for your company's website. In this case, when you present your results to company stakeholders, you suggest implementing the larger 'Purchase' button to increase average revenue per user going forward.
Key takeaways
A/B testing is one of the most popular applications of statistics for business purposes. Data professionals use A/B testing to help business leaders to optimize product performance, improve customer experience, and grow their online business. Understanding the general purpose and design of an A/B test will be useful in your future career as a data professional.
#### Experimental Design
Data professionals often work with experimental data previously collected by other researchers. However, the right data for a specific project might not always be available or accessible. In this case, data professionals can design their own experiments and collect their own data.
In this reading, we'll discuss how data professionals design experiments to collect data, test hypotheses, and discover relationships between variables. You will also learn more about the basic concepts and procedures of experimental design.
###### Context: Experimental design
Experimental design refers to planning an experiment in order to collect data to answer your research question.
Researchers conduct experiments in many fields: medicine, physics, psychology, manufacturing, marketing, and more. The typical purpose of an experiment is to discover a cause-and-effect relationship between variables. For example, a data professional might design an experiment to discover whether:
- A new medicine leads to faster recovery time
- A new website design increases product sales
- A new fertilizer increases crop growth
- A new training program improves athletic performance
It's important to understand experimental design because it affects the quality of your data, and the validity of any conclusions you draw based on your results. A poor design might lead to invalid results, which can be costly for companies and consumers. Based on the results of a flawed experiment, a company can spend years developing a medicine that is ineffective or invest heavily in a manufacturing process that is inefficient. A well-designed experiment will give you reliable data that helps answer your research question.
You can explore an example to get a better understanding of experimental design.
###### Example: Clinical trial
Imagine you are a data professional who works for a pharmaceutical company. The company invents a new medicine to treat the common cold. Your team leader asks you to design an experiment to test the effectiveness of the medicine. You want to find out whether taking the medicine leads to faster recovery time.
There are at least three key steps in designing an experiment:
- Define your variables
- Formulate your hypothesis
- Assign test subjects to treatment and control groups
Note: These are basic steps that apply to controlled experiments. Experimental design is a complex topic, and this is a simplified explanation.
Next, examine each step in more detail using our example.
###### Step 1: Define your variables
Data professionals often begin by defining the independent and dependent variables in their experiment. This helps clarify the relationship between the variables.
- The independent variable refers to the cause you are interested in investigating. A researcher changes or controls the independent variable to determine how it affects the dependent variable. 'Independent' means it's not influenced by other variables in the experiment.
- The dependent variable refers to the effect you're interested in measuring. 'Dependent' means its value is influenced by the independent variable.
In the clinical trial, you want to find out how the medicine affects recovery time. Therefore:
- Your independent variable is the medicine-- the cause you want to investigate.
- Your dependent variable is recovery time-- the effect you want to measure.
In a more complex experiment, you might test the effect of different medicines on recovery time, or different doses of the same medicine. In each case, you manipulate your independent variable (medicine) to measure its effects on your dependent variable (recovery time).
###### Step 2: Formulate your hypothesis
Your next step is to formulate a hypothesis. Your hypothesis states the relationship between your independent and dependent variables and predicts the outcome of your experiment. Data professionals formulate both null and alternative hypotheses when they conduct research that involves statistical testing. Recall that the null hypothesis typically assumes that there is no effect on the population, and the alternative hypothesis assumes the opposite. For your clinical trial:
- Your null hypothesis (H0) is that the medicine has no effect.
- Your alternative hypothesis (Ha) is that the medicine is effective.
###### Step 3: Assign test subjects to treatment and control groups
Treatment and control groups
Experiments such as clinical trials and A/B tests are controlled experiments. In a controlled experiment, test subjects are assigned to a treatment group and a control group. The treatment is the new change being tested in the experiment. The treatment group is exposed to the treatment. The control group is not exposed to the treatment. The difference in metric values between the two groups measures the treatment's effect on the test subjects.
In your clinical trial, the treatment is the medicine that the subjects in the treatment group are given. The subjects in the control group are not given the medicine. Imagine that your results show that the mean recovery time is lower in the treatment group (6.2 days) than the control group (7.5 days). The difference between the two groups, 7.5-6.2=1.3 days, measures the treatment's impact. In other words, the medicine decreases mean recovery time by 1.3 days.
Note: After a data professional designs and runs their experiment, they use statistical testing to analyze the results. As a next step, you might conduct a two-sample t-test to determine whether the observed difference in recovery time is statistically significant or due to chance.
Ideally, exposure to the treatment is the only significant difference between the two groups. The design allows researchers to control for other factors that might influence the test results and draw causal conclusions about the effect of the treatment.
For example, imagine the subjects in your treatment group have a much healthier diet than the subjects in your control group. Any observed decrease in recovery time for the treatment group might be due to healthier diet--and not the medicine. In this case, you cannot say with confidence that the medicine alone is the cause of the faster recovery time.
###### Randomization
Typically, data professionals randomly assign test subjects to treatment and control groups. Randomization helps control the effect of other factors on the outcome of an experiment. Two common methods for assigning subjects to treatment and control groups are completely randomized design and randomized block design.
In a completely randomized design, test subjects are assigned to treatment and control groups using a random process. For example, in a clinical trial, you might use a computer program to label each subject with a number and then randomly select numbers for each group.
Sometimes, however, a completely randomized design might not be the most effective approach. When designing an experiment, data professionals must account for nuisance factors. These are factors that can affect the result of an experiment, but are not of primary interest to the researcher.
Researchers can use a randomized block design to minimize the impact of known nuisance factors. Blocking is the arranging of test subjects in groups, or blocks, that are similar to one another. In block design, you first divide subjects into blocks, and then you randomly assign the subjects within each block to treatment and control groups.
For example, suppose you know that age is a significant factor in recovery time from the common cold. In particular, you know that people under the age of 35 years tend to recover faster than older people. In this scenario, age is a nuisance factor because it might affect the results of your experiment. For example, in a clinical trial with a completely randomized design and a smaller sample size, you might randomly get large proportion of young people in the treatment group. This will make it more difficult to determine whether any observed decrease in recovery time is due to the treatment (medicine) or to the nuisance factor (age).
In this case, blocking for the age factor is a more effective way to design your experiment. 
- First, you divide the test subjects into blocks based on age, such as 21-35, 36-50, and 51-65. 
- Next, you randomly assign the subjects within each block to treatment and control groups.
This way, if there is a significant difference in recovery time within a specific block, you can be more confident that this result is due to treatment (medicine) and not to the nuisance factor (age).
Key takeaways
Data professionals use experimental design to plan experiments and collect data that helps answer their research questions. The design of an experiment affects the quality of your data and the validity of your conclusions. Whether you're designing your own experiment, or using data collected by others, its important to understand the basic principles of experimental design. This knowledge will help you analyze data from experiments such as clinical trials, A/B tests, and more.
### Case study: 
##### Ipsos: How a market research company used A/B testing to help advertisers create more effective ads.
A/B testing is a way to compare two versions of something to find out which version performs better. For example, a data professional might use A/B testing to compare two versions of a web page or two versions of an online ad. A/B testing utilizes statistical methods such as sampling and hypothesis testing. 
This case study describes how Ipsos used A/B testing to compare two different online ad formats: ads presented in a sequenced narrative vs. a traditional 30-second ad presented multiple times. You'll also learn how data-driven market research reveals important insights about the impact of different ad formats on the effectiveness of a digital ad campaign.
###### Company background
Ipsos is a full service market research company. Founded in France in 1975, Ipsos is now a global company with 18,000 staff operating in 90 countries. Ipsos provides research services across numerous private and public sector domains. These services includes brand building; advertising effectiveness; product development; reputation; customer and user experience; and public opinion, election and crisis management. Ipsos uses a combination of data sources for their research, from primary data collection to social listening, mobility, and satellite imagery.
###### Project background
Ipsos' client for this project was an online media company that allows users to post their own video content. The media company wanted to help its own clients - the advertisers on their platform - to create the most affective ads. In particular, they wanted to find out whether investing in sequenced ads would increase the likelihood of viewers recalling an ad and purchasing a product. Video ad sequencing lets advertisers show ads in an order based on the most engaging and memorable story structures. The media company commissioned Ipsos to conduct research to measure the impact of five different sequence structures on brand fit.
Note: For the purpose of this case study, we will focus on only one sequence structure. Tease, Amplify, Echo. This sequence starts with a short ad to spark viewer curiosity (Tease); then, it moves on to a longer ad with more information to secure viewer engagement (Amplify); finally, it ends with a shorter ad that recaps the story and spurs viewers to action (Echo).
###### Project framework
Ipsos developed the following research question to guide their project: Does an ad sequence with the Tease, Amplify, Echo structure increase ad recall and purchase intent compared to repeated viewings of a traditional 30-second ad?
Ipsos' initial hypothesis was that ads presented in a sequenced narrative would be more effective than repeated viewings of a traditional ad. To test this hypothesis for these two advertising approaches, Ipsos conducted an A/B test. The A/B test set up an experiment for two groups of users : one group was shown the Tease, Amplify, Echo ads and the other group was shown a traditional ad multiple times. In each case, the different ad formats were based on the same brand content. You'll learn more about the details of the testing process in what follows.
###### The challenge
At the outset of the project, Ipsos identified two main challenges. The first challenge involved properly designing the A/B test. The second challenge involved creating the test ads in the appropriate test environment.
###### Test design
Ipsos's primary concern was the results of the A/B test to be generalizable, or applicable to the overall population of the media company's users. In other words, Ipsos wanted to make valid inferences about the larger user population based on the smaller sample of test participants. To obtain valid test results, Ipsos needed to do the following;
- Create a representative sample of test participants that mirrored the overall population of the media company's users.
- Create an online test environment that mirrored the media company's online environment. This also implied creating test ads from multiple brands to reproduce the diversity of ads featured on the media company's platform.
###### The approach
Despite these challenges, Ipsos conducted the A/B test and achieved their research goals. Ipsos' successful approach to their project included the following elements:
- Team
To build an effective team, Ipsos created a cross-functional operation, including video production to create test ads based on the Tease, Amplify, Echo structure, and technology to build a realistic online test environment.
To facilitate collaboration among project participants, Ipsos outlined a clear set of editing rules and organized a shared site to house links to videos and edit notes. This allowed rapid feedback and adjustment throughout the development process. Finally, Ipsos has senior client service project managers own and monitor the project design and workflow from beginning to end.  
- Sampling
Ipsos used random selection from consumer panels to generate a representative sample that accurately reflected the characteristics of the overall user population. Ipsos also made sure that each test group included the same proportion of respondents for key categories such as age and gender. Further, Ipsos used a relatively large sample size of 7,500 respondents to obtain more precise results.
- Testing process
To get valid test results, Ipsos conducted A/B testing in an online environment where respondents used the media company's platform as they typically would in their everyday lives. To mirror the diversity of ads on the platform, Ipsos developed test ads across 30 brand categories, from airline tickets to fast food laundry detergent.
The testing process was organized in the following way:
Surveys were administered online via respondent's smartphones in November and December 2018. After initial screening, respondents were taken to a browser-based version of the platform where they were free to search for and watch videos as they normally would. Ipsos dynamically inserted test ads at the beginning of the videos selected by respondents in the live test environment. After the browsing session, respondents completed a survey to measure brand lift for ad recall and product intent.
- Hypothesis testing
The survey data indicated that the Tease, Amplify, Echo ad sequence led to higher levels of ad recall and purchase intent among respondents than repeated viewings of a traditional ad. To determine whether their observed results were statistically significant, Ipsos conducted a two-sample t-test for each category: one for ad recall and one for purchase intent. They formulated their hypotheses as follows:
- H0: There is no difference in ad recall between sequenced ads and a repeated traditional ad.
- Ha: There is a difference in ad recall between sequenced ads and a repeated traditional ad.
- H0: There is no difference in purchase intent between sequenced ads and a repeated traditional ad.
- H0: There is a difference in purchase intent between sequenced ads and a repeated traditional ad.
For both tests, Ipsos rejected the null hypothesis. They concluded that there are statistically significant and substantively meaningful differences in ad recall and purchase intent between sequenced ads and a repeated traditional ad.
##### The results
The results of the A/B test indicated that ad sequencing works! The Tease, Amplify, Echo ad sequence had a significantly greater impact on ad recall and purchase intent than repeated viewings of a traditional ad. For example, across all product categories, 54% of viewers recalled the ad after being exposed to the "Tease, Amplify, Echo" sequence as compared to 42% with a repeated traditional ad. Further, 30% of viewers expressed intent to purchase after being exposed to the "Tease, Amplify, Echo" sequence as compared to 25% with a repeated traditional ad.
Overall, the results suggested that advertisers should invest in ad sequencing for their digital campaigns to increase brand fit.
###### Conclusion
This case study on Ipsos' A/B testing demonstrates the power of data-driven research to generate key business insights. The results of the A/B test clearly show how ad sequencing increases ad recall and purchase intent compared to repeated viewings of a traditional ad. Ipsos's research on the benefits of ad sequencing helped the media company improve the experience and performance of the advertisers on their platform and added value to the media company's brand.